# Copyright (c) Meta Platforms, Inc. and affiliates.
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
distrib.py:def rank():
distrib.py:    if torch.distributed.is_initialized():
distrib.py:        return torch.distributed.get_rank()
distrib.py:    else:
distrib.py:        return 0
distrib.py:def world_size():
distrib.py:    if torch.distributed.is_initialized():
distrib.py:        return torch.distributed.get_world_size()
distrib.py:    else:
distrib.py:        return 1
distrib.py:def is_distributed():
distrib.py:    return world_size() > 1
distrib.py:def all_reduce(tensor: torch.Tensor, op=torch.distributed.ReduceOp.SUM):
distrib.py:    if is_distributed():
distrib.py:        return torch.distributed.all_reduce(tensor, op)
distrib.py:def _is_complex_or_float(tensor):
distrib.py:    return torch.is_floating_point(tensor) or torch.is_complex(tensor)
distrib.py:def _check_number_of_params(params: tp.List[torch.Tensor]):
distrib.py:    # utility function to check that the number of params in all workers is the same,
distrib.py:    # and thus avoid a deadlock with distributed all reduce.
distrib.py:    if not is_distributed() or not params:
distrib.py:        return
distrib.py:    tensor = torch.tensor([len(params)], device=params[0].device, dtype=torch.long)
distrib.py:    all_reduce(tensor)
distrib.py:    if tensor.item() != len(params) * world_size():
distrib.py:        # If not all the workers have the same number, for at least one of them,
distrib.py:        # this inequality will be verified.
distrib.py:        raise RuntimeError(f"Mismatch in number of params: ours is {len(params)}, "
distrib.py:                           "at least one worker has a different one.")
distrib.py:def broadcast_tensors(tensors: tp.Iterable[torch.Tensor], src: int = 0):
distrib.py:    """Broadcast the tensors from the given parameters to all workers.
distrib.py:    This can be used to ensure that all workers have the same model to start with.
distrib.py:    """
distrib.py:    if not is_distributed():
distrib.py:        return
distrib.py:    tensors = [tensor for tensor in tensors if _is_complex_or_float(tensor)]
distrib.py:    _check_number_of_params(tensors)
distrib.py:    handles = []
distrib.py:    for tensor in tensors:
distrib.py:        handle = torch.distributed.broadcast(tensor.data, src=src, async_op=True)
distrib.py:        handles.append(handle)
distrib.py:    for handle in handles:
distrib.py:        handle.wait()
distrib.py:def sync_buffer(buffers, average=True):
distrib.py:    """
distrib.py:    Sync grad for buffers. If average is False, broadcast instead of averaging.
distrib.py:    """
distrib.py:    if not is_distributed():
distrib.py:        return
distrib.py:    handles = []
distrib.py:    for buffer in buffers:
distrib.py:        if torch.is_floating_point(buffer.data):
distrib.py:            if average:
distrib.py:                handle = torch.distributed.all_reduce(
distrib.py:                    buffer.data, op=torch.distributed.ReduceOp.SUM, async_op=True)
distrib.py:            else:
distrib.py:                handle = torch.distributed.broadcast(
distrib.py:                    buffer.data, src=0, async_op=True)
distrib.py:            handles.append((buffer, handle))
distrib.py:    for buffer, handle in handles:
distrib.py:        handle.wait()
distrib.py:        if average:
distrib.py:            buffer.data /= world_size
distrib.py:def sync_grad(params):
distrib.py:    """
distrib.py:    Simpler alternative to DistributedDataParallel, that doesn't rely
distrib.py:    on any black magic. For simple models it can also be as fast.
distrib.py:    Just call this on your model parameters after the call to backward!
distrib.py:    """
distrib.py:    if not is_distributed():
distrib.py:        return
distrib.py:    handles = []
distrib.py:    for p in params:
distrib.py:        if p.grad is not None:
distrib.py:            handle = torch.distributed.all_reduce(
distrib.py:                p.grad.data, op=torch.distributed.ReduceOp.SUM, async_op=True)
distrib.py:            handles.append((p, handle))
distrib.py:    for p, handle in handles:
distrib.py:        handle.wait()
distrib.py:        p.grad.data /= world_size()
distrib.py:def average_metrics(metrics: tp.Dict[str, float], count=1.):
distrib.py:    """Average a dictionary of metrics across all workers, using the optional
distrib.py:    `count` as unnormalized weight.
distrib.py:    """
distrib.py:    if not is_distributed():
distrib.py:        return metrics
distrib.py:    keys, values = zip(*metrics.items())
distrib.py:    device = 'cuda' if torch.cuda.is_available() else 'cpu'
distrib.py:    tensor = torch.tensor(list(values) + [1], device=device, dtype=torch.float32)
distrib.py:    tensor *= count
distrib.py:    all_reduce(tensor)
distrib.py:    averaged = (tensor[:-1] / tensor[-1]).cpu().tolist()
distrib.py:    return dict(zip(keys, averaged))
balancer.py:from collections import defaultdict
balancer.py:import typing as tp
balancer.py:import torch
balancer.py:from torch import autograd
balancer.py:from .distrib import average_metrics
balancer.py:def averager(beta: float = 1):
balancer.py:    """
balancer.py:    Exponential Moving Average callback.
balancer.py:    Returns a single function that can be called to repeatidly update the EMA
balancer.py:    with a dict of metrics. The callback will return
balancer.py:    the new averaged dict of metrics.
balancer.py:    Note that for `beta=1`, this is just plain averaging.
balancer.py:    """
balancer.py:    fix: tp.Dict[str, float] = defaultdict(float)
balancer.py:    total: tp.Dict[str, float] = defaultdict(float)
balancer.py:    def _update(metrics: tp.Dict[str, tp.Any], weight: float = 1) -> tp.Dict[str, float]:
balancer.py:        nonlocal total, fix
balancer.py:        for key, value in metrics.items():
balancer.py:            total[key] = total[key] * beta + weight * float(value)
balancer.py:            fix[key] = fix[key] * beta + weight
balancer.py:        return {key: tot / fix[key] for key, tot in total.items()}
balancer.py:    return _update
balancer.py:class Balancer:
balancer.py:    """Loss balancer.
balancer.py:    The loss balancer combines losses together to compute gradients for the backward.
balancer.py:    A call to the balancer will weight the losses according the specified weight coefficients.
balancer.py:    A call to the backward method of the balancer will compute the gradients, combining all the losses and
balancer.py:    potentially rescaling the gradients, which can help stabilize the training and reasonate
balancer.py:    about multiple losses with varying scales.
balancer.py:    Expected usage:
balancer.py:        weights = {'loss_a': 1, 'loss_b': 4}
balancer.py:        balancer = Balancer(weights, ...)
balancer.py:        losses: dict = {}
balancer.py:        losses['loss_a'] = compute_loss_a(x, y)
balancer.py:        losses['loss_b'] = compute_loss_b(x, y)
balancer.py:        if model.training():
balancer.py:            balancer.backward(losses, x)
balancer.py:    ..Warning:: It is unclear how this will interact with DistributedDataParallel,
balancer.py:        in particular if you have some losses not handled by the balancer. In that case
balancer.py:        you can use `encodec.distrib.sync_grad(model.parameters())` and
balancer.py:        `encodec.distrib.sync_buffwers(model.buffers())` as a safe alternative.
balancer.py:    Args:
balancer.py:        weights (Dict[str, float]): Weight coefficient for each loss. The balancer expect the losses keys
balancer.py:            from the backward method to match the weights keys to assign weight to each of the provided loss.
balancer.py:        rescale_grads (bool): Whether to rescale gradients or not, without. If False, this is just
balancer.py:            a regular weighted sum of losses.
balancer.py:        total_norm (float): Reference norm when rescaling gradients, ignored otherwise.
balancer.py:        emay_decay (float): EMA decay for averaging the norms when `rescale_grads` is True.
balancer.py:        per_batch_item (bool): Whether to compute the averaged norm per batch item or not. This only holds
balancer.py:            when rescaling the gradients.
balancer.py:        epsilon (float): Epsilon value for numerical stability.
balancer.py:        monitor (bool): Whether to store additional ratio for each loss key in metrics.
balancer.py:    """
balancer.py:    def __init__(self, weights: tp.Dict[str, float], rescale_grads: bool = True, total_norm: float = 1.,
balancer.py:                 ema_decay: float = 0.999, per_batch_item: bool = True, epsilon: float = 1e-12,
balancer.py:                 monitor: bool = False):
balancer.py:        self.weights = weights
balancer.py:        self.per_batch_item = per_batch_item
balancer.py:        self.total_norm = total_norm
balancer.py:        self.averager = averager(ema_decay)
balancer.py:        self.epsilon = epsilon
balancer.py:        self.monitor = monitor
balancer.py:        self.rescale_grads = rescale_grads
balancer.py:        self._metrics: tp.Dict[str, tp.Any] = {}
balancer.py:    @property
balancer.py:    def metrics(self):
balancer.py:        return self._metrics
balancer.py:    def backward(self, losses: tp.Dict[str, torch.Tensor], input: torch.Tensor):
balancer.py:        norms = {}
balancer.py:        grads = {}
balancer.py:        for name, loss in losses.items():
balancer.py:            grad, = autograd.grad(loss, [input], retain_graph=True)
balancer.py:            if self.per_batch_item:
balancer.py:                dims = tuple(range(1, grad.dim()))
balancer.py:                norm = grad.norm(dim=dims).mean()
balancer.py:            else:
balancer.py:                norm = grad.norm()
balancer.py:            norms[name] = norm
balancer.py:            grads[name] = grad
balancer.py:        count = 1
balancer.py:        if self.per_batch_item:
balancer.py:            count = len(grad)
balancer.py:        avg_norms = average_metrics(self.averager(norms), count)
balancer.py:        total = sum(avg_norms.values())
balancer.py:        self._metrics = {}
balancer.py:        if self.monitor:
balancer.py:            for k, v in avg_norms.items():
balancer.py:                self._metrics[f'ratio_{k}'] = v / total
balancer.py:        total_weights = sum([self.weights[k] for k in avg_norms])
balancer.py:        ratios = {k: w / total_weights for k, w in self.weights.items()}
balancer.py:        out_grad: tp.Any = 0
balancer.py:        for name, avg_norm in avg_norms.items():
balancer.py:            if self.rescale_grads:
balancer.py:                scale = ratios[name] * self.total_norm / (self.epsilon + avg_norm)
balancer.py:                grad = grads[name] * scale
balancer.py:            else:
balancer.py:                grad = self.weights[name] * grads[name]
balancer.py:            out_grad += grad
balancer.py:        input.backward(out_grad)
balancer.py:def test():
balancer.py:    from torch.nn import functional as F
balancer.py:    x = torch.zeros(1, requires_grad=True)
balancer.py:    one = torch.ones_like(x)
balancer.py:    loss_1 = F.l1_loss(x, one)
balancer.py:    loss_2 = 100 * F.l1_loss(x, -one)
balancer.py:    losses = {'1': loss_1, '2': loss_2}
balancer.py:    balancer = Balancer(weights={'1': 1, '2': 1}, rescale_grads=False)
balancer.py:    balancer.backward(losses, x)
balancer.py:    assert torch.allclose(x.grad, torch.tensor(99.)), x.grad
balancer.py:    loss_1 = F.l1_loss(x, one)
balancer.py:    loss_2 = 100 * F.l1_loss(x, -one)
balancer.py:    losses = {'1': loss_1, '2': loss_2}
balancer.py:    x.grad = None
balancer.py:    balancer = Balancer(weights={'1': 1, '2': 1}, rescale_grads=True)
balancer.py:    balancer.backward({'1': loss_1, '2': loss_2}, x)
balancer.py:    assert torch.allclose(x.grad, torch.tensor(0.)), x.grad
model.py:"""EnCodec model implementation."""
model.py:import math
model.py:from pathlib import Path
model.py:import typing as tp
model.py:import numpy as np
model.py:import torch
model.py:from torch import nn
model.py:from . import quantization as qt
model.py:from . import modules as m
model.py:from .utils import _check_checksum, _linear_overlap_add, _get_checkpoint_url
model.py:ROOT_URL = 'https://dl.fbaipublicfiles.com/encodec/v0/'
model.py:EncodedFrame = tp.Tuple[torch.Tensor, tp.Optional[torch.Tensor]]
model.py:class LMModel(nn.Module):
model.py:    """Language Model to estimate probabilities of each codebook entry.
model.py:    We predict all codebooks in parallel for a given time step.
model.py:    Args:
model.py:        n_q (int): number of codebooks.
model.py:        card (int): codebook cardinality.
model.py:        dim (int): transformer dimension.
model.py:        **kwargs: passed to `encodec.modules.transformer.StreamingTransformerEncoder`.
model.py:    """
model.py:    def __init__(self, n_q: int = 32, card: int = 1024, dim: int = 200, **kwargs):
model.py:        super().__init__()
model.py:        self.card = card
model.py:        self.n_q = n_q
model.py:        self.dim = dim
model.py:        self.transformer = m.StreamingTransformerEncoder(dim=dim, **kwargs)
model.py:        self.emb = nn.ModuleList([nn.Embedding(card + 1, dim) for _ in range(n_q)])
model.py:        self.linears = nn.ModuleList([nn.Linear(dim, card) for _ in range(n_q)])
model.py:    def forward(self, indices: torch.Tensor,
model.py:                states: tp.Optional[tp.List[torch.Tensor]] = None, offset: int = 0):
model.py:        """
model.py:        Args:
model.py:            indices (torch.Tensor): indices from the previous time step. Indices
model.py:                should be 1 + actual index in the codebook. The value 0 is reserved for
model.py:                when the index is missing (i.e. first time step). Shape should be
model.py:                `[B, n_q, T]`.
model.py:            states: state for the streaming decoding.
model.py:            offset: offset of the current time step.
model.py:        Returns a 3-tuple `(probabilities, new_states, new_offset)` with probabilities
model.py:        with a shape `[B, card, n_q, T]`.
model.py:        """
model.py:        B, K, T = indices.shape
model.py:        input_ = sum([self.emb[k](indices[:, k]) for k in range(K)])
model.py:        out, states, offset = self.transformer(input_, states, offset)
model.py:        logits = torch.stack([self.linears[k](out) for k in range(K)], dim=1).permute(0, 3, 1, 2)
model.py:        return torch.softmax(logits, dim=1), states, offset
model.py:class EncodecModel(nn.Module):
model.py:    """EnCodec model operating on the raw waveform.
model.py:    Args:
model.py:        target_bandwidths (list of float): Target bandwidths.
model.py:        encoder (nn.Module): Encoder network.
model.py:        decoder (nn.Module): Decoder network.
model.py:        sample_rate (int): Audio sample rate.
model.py:        channels (int): Number of audio channels.
model.py:        normalize (bool): Whether to apply audio normalization.
model.py:        segment (float or None): segment duration in sec. when doing overlap-add.
model.py:        overlap (float): overlap between segment, given as a fraction of the segment duration.
model.py:        name (str): name of the model, used as metadata when compressing audio.
model.py:    """
model.py:    def __init__(self,
model.py:                 encoder: m.SEANetEncoder,
model.py:                 decoder: m.SEANetDecoder,
model.py:                 quantizer: qt.ResidualVectorQuantizer,
model.py:                 target_bandwidths: tp.List[float],
model.py:                 sample_rate: int,
model.py:                 channels: int,
model.py:                 normalize: bool = False,
model.py:                 segment: tp.Optional[float] = None,
model.py:                 overlap: float = 0.01,
model.py:                 name: str = 'unset'):
model.py:        super().__init__()
model.py:        self.bandwidth: tp.Optional[float] = None
model.py:        self.target_bandwidths = target_bandwidths
model.py:        self.encoder = encoder
model.py:        self.quantizer = quantizer
model.py:        self.decoder = decoder
model.py:        self.sample_rate = sample_rate
model.py:        self.channels = channels
model.py:        self.normalize = normalize
model.py:        self.segment = segment
model.py:        self.overlap = overlap
model.py:        self.frame_rate = math.ceil(self.sample_rate / np.prod(self.encoder.ratios))
model.py:        self.name = name
model.py:        self.bits_per_codebook = int(math.log2(self.quantizer.bins))
model.py:        assert 2 ** self.bits_per_codebook == self.quantizer.bins, \
model.py:            "quantizer bins must be a power of 2."
model.py:    @property
model.py:    def segment_length(self) -> tp.Optional[int]:
model.py:        if self.segment is None:
model.py:            return None
model.py:        return int(self.segment * self.sample_rate)
model.py:    @property
model.py:    def segment_stride(self) -> tp.Optional[int]:
model.py:        segment_length = self.segment_length
model.py:        if segment_length is None:
model.py:            return None
model.py:        return max(1, int((1 - self.overlap) * segment_length))
model.py:    def encode(self, x: torch.Tensor) -> tp.List[EncodedFrame]:
model.py:        """Given a tensor `x`, returns a list of frames containing
model.py:        the discrete encoded codes for `x`, along with rescaling factors
model.py:        for each segment, when `self.normalize` is True.
model.py:        Each frames is a tuple `(codebook, scale)`, with `codebook` of
model.py:        shape `[B, K, T]`, with `K` the number of codebooks.
model.py:        """
model.py:        assert x.dim() == 3
model.py:        _, channels, length = x.shape
model.py:        assert channels > 0 and channels <= 2
model.py:        segment_length = self.segment_length
model.py:        if segment_length is None:
model.py:            segment_length = length
model.py:            stride = length
model.py:        else:
model.py:            stride = self.segment_stride  # type: ignore
model.py:            assert stride is not None
model.py:        encoded_frames: tp.List[EncodedFrame] = []
model.py:        for offset in range(0, length, stride):
model.py:            frame = x[:, :, offset: offset + segment_length]
model.py:            encoded_frames.append(self._encode_frame(frame))
model.py:        return encoded_frames
model.py:    def _encode_frame(self, x: torch.Tensor) -> EncodedFrame:
model.py:        length = x.shape[-1]
model.py:        duration = length / self.sample_rate
model.py:        assert self.segment is None or duration <= 1e-5 + self.segment
model.py:        if self.normalize:
model.py:            mono = x.mean(dim=1, keepdim=True)
model.py:            volume = mono.pow(2).mean(dim=2, keepdim=True).sqrt()
model.py:            scale = 1e-8 + volume
model.py:            x = x / scale
model.py:            scale = scale.view(-1, 1)
model.py:        else:
model.py:            scale = None
model.py:        emb = self.encoder(x)
model.py:        codes = self.quantizer.encode(emb, self.frame_rate, self.bandwidth)
model.py:        codes = codes.transpose(0, 1)
model.py:        # codes is [B, K, T], with T frames, K nb of codebooks.
model.py:        return codes, scale
model.py:    def decode(self, encoded_frames: tp.List[EncodedFrame]) -> torch.Tensor:
model.py:        """Decode the given frames into a waveform.
model.py:        Note that the output might be a bit bigger than the input. In that case,
model.py:        any extra steps at the end can be trimmed.
model.py:        """
model.py:        segment_length = self.segment_length
model.py:        if segment_length is None:
model.py:            assert len(encoded_frames) == 1
model.py:            return self._decode_frame(encoded_frames[0])
model.py:        frames = [self._decode_frame(frame) for frame in encoded_frames]
model.py:        return _linear_overlap_add(frames, self.segment_stride or 1)
model.py:    def _decode_frame(self, encoded_frame: EncodedFrame) -> torch.Tensor:
model.py:        codes, scale = encoded_frame
model.py:        codes = codes.transpose(0, 1)
model.py:        emb = self.quantizer.decode(codes)
model.py:        out = self.decoder(emb)
model.py:        if scale is not None:
model.py:            out = out * scale.view(-1, 1, 1)
model.py:        return out
model.py:    def forward(self, x: torch.Tensor) -> torch.Tensor:
model.py:        frames = self.encode(x)
model.py:        return self.decode(frames)[:, :, :x.shape[-1]]
model.py:    def set_target_bandwidth(self, bandwidth: float):
model.py:        if bandwidth not in self.target_bandwidths:
model.py:            raise ValueError(f"This model doesn't support the bandwidth {bandwidth}. "
model.py:                             f"Select one of {self.target_bandwidths}.")
model.py:        self.bandwidth = bandwidth
model.py:    def get_lm_model(self) -> LMModel:
model.py:        """Return the associated LM model to improve the compression rate.
model.py:        """
model.py:        device = next(self.parameters()).device
model.py:        lm = LMModel(self.quantizer.n_q, self.quantizer.bins, num_layers=5, dim=200,
model.py:                     past_context=int(3.5 * self.frame_rate)).to(device)
model.py:        checkpoints = {
model.py:            'encodec_24khz': 'encodec_lm_24khz-1608e3c0.th',
model.py:            'encodec_48khz': 'encodec_lm_48khz-7add9fc3.th',
model.py:        }
model.py:        try:
model.py:            checkpoint_name = checkpoints[self.name]
model.py:        except KeyError:
model.py:            raise RuntimeError("No LM pre-trained for the current Encodec model.")
model.py:        url = _get_checkpoint_url(ROOT_URL, checkpoint_name)
model.py:        state = torch.hub.load_state_dict_from_url(
model.py:            url, map_location='cpu', check_hash=True)  # type: ignore
model.py:        lm.load_state_dict(state)
model.py:        lm.eval()
model.py:        return lm
model.py:    @staticmethod
model.py:    def _get_model(target_bandwidths: tp.List[float],
model.py:                   sample_rate: int = 24_000,
model.py:                   channels: int = 1,
model.py:                   causal: bool = True,
model.py:                   model_norm: str = 'weight_norm',
model.py:                   audio_normalize: bool = False,
model.py:                   segment: tp.Optional[float] = None,
model.py:                   name: str = 'unset'):
model.py:        encoder = m.SEANetEncoder(channels=channels, norm=model_norm, causal=causal)
model.py:        decoder = m.SEANetDecoder(channels=channels, norm=model_norm, causal=causal)
model.py:        n_q = int(1000 * target_bandwidths[-1] // (math.ceil(sample_rate / encoder.hop_length) * 10))
model.py:        quantizer = qt.ResidualVectorQuantizer(
model.py:            dimension=encoder.dimension,
model.py:            n_q=n_q,
model.py:            bins=1024,
model.py:        )
model.py:        model = EncodecModel(
model.py:            encoder,
model.py:            decoder,
model.py:            quantizer,
model.py:            target_bandwidths,
model.py:            sample_rate,
model.py:            channels,
model.py:            normalize=audio_normalize,
model.py:            segment=segment,
model.py:            name=name,
model.py:        )
model.py:        return model
model.py:    @staticmethod
model.py:    def _get_pretrained(checkpoint_name: str, repository: tp.Optional[Path] = None):
model.py:        if repository is not None:
model.py:            if not repository.is_dir():
model.py:                raise ValueError(f"{repository} must exist and be a directory.")
model.py:            file = repository / checkpoint_name
model.py:            checksum = file.stem.split('-')[1]
model.py:            _check_checksum(file, checksum)
model.py:            return torch.load(file)
model.py:        else:
model.py:            url = _get_checkpoint_url(ROOT_URL, checkpoint_name)
model.py:            return torch.hub.load_state_dict_from_url(url, map_location='cpu', check_hash=True)  # type:ignore
model.py:    @staticmethod
model.py:    def encodec_model_24khz(pretrained: bool = True, repository: tp.Optional[Path] = None):
model.py:        """Return the pretrained causal 24khz model.
model.py:        """
model.py:        if repository:
model.py:            assert pretrained
model.py:        target_bandwidths = [1.5, 3., 6, 12., 24.]
model.py:        checkpoint_name = 'encodec_24khz-d7cc33bc.th'
model.py:        sample_rate = 24_000
model.py:        channels = 1
model.py:        model = EncodecModel._get_model(
model.py:            target_bandwidths, sample_rate, channels,
model.py:            causal=True, model_norm='weight_norm', audio_normalize=False,
model.py:            name='encodec_24khz' if pretrained else 'unset')
model.py:        if pretrained:
model.py:            state_dict = EncodecModel._get_pretrained(checkpoint_name, repository)
model.py:            model.load_state_dict(state_dict)
model.py:        model.eval()
model.py:        return model
model.py:    @staticmethod
model.py:    def encodec_model_48khz(pretrained: bool = True, repository: tp.Optional[Path] = None):
model.py:        """Return the pretrained 48khz model.
model.py:        """
model.py:        if repository:
model.py:            assert pretrained
model.py:        target_bandwidths = [3., 6., 12., 24.]
model.py:        checkpoint_name = 'encodec_48khz-7e698e3e.th'
model.py:        sample_rate = 48_000
model.py:        channels = 2
model.py:        model = EncodecModel._get_model(
model.py:            target_bandwidths, sample_rate, channels,
model.py:            causal=False, model_norm='time_group_norm', audio_normalize=True,
model.py:            segment=1., name='encodec_48khz' if pretrained else 'unset')
model.py:        if pretrained:
model.py:            state_dict = EncodecModel._get_pretrained(checkpoint_name, repository)
model.py:            model.load_state_dict(state_dict)
model.py:        model.eval()
model.py:        return model
model.py:def test():
model.py:    from itertools import product
model.py:    import torchaudio
model.py:    bandwidths = [3, 6, 12, 24]
model.py:    models = {
model.py:        'encodec_24khz': EncodecModel.encodec_model_24khz,
model.py:        'encodec_48khz': EncodecModel.encodec_model_48khz
model.py:    }
model.py:    for model_name, bw in product(models.keys(), bandwidths):
model.py:        model = models[model_name]()
model.py:        model.set_target_bandwidth(bw)
model.py:        audio_suffix = model_name.split('_')[1][:3]
model.py:        wav, sr = torchaudio.load(f"test_{audio_suffix}.wav")
model.py:        wav = wav[:, :model.sample_rate * 2]
model.py:        wav_in = wav.unsqueeze(0)
model.py:        wav_dec = model(wav_in)[0]
model.py:        assert wav.shape == wav_dec.shape, (wav.shape, wav_dec.shape)
model.py:if __name__ == '__main__':
model.py:    test()
quantization/core_vq.py:"""Core vector quantization implementation."""
quantization/core_vq.py:import typing as tp
quantization/core_vq.py:import warnings
quantization/core_vq.py:from einops import rearrange, repeat
quantization/core_vq.py:import torch
quantization/core_vq.py:from torch import nn
quantization/core_vq.py:import torch.nn.functional as F
quantization/core_vq.py:from .. import distrib
quantization/core_vq.py:def default(val: tp.Any, d: tp.Any) -> tp.Any:
quantization/core_vq.py:    return val if val is not None else d
quantization/core_vq.py:def ema_inplace(moving_avg, new, decay: float):
quantization/core_vq.py:    moving_avg.data.mul_(decay).add_(new, alpha=(1 - decay))
quantization/core_vq.py:def laplace_smoothing(x, n_categories: int, epsilon: float = 1e-5):
quantization/core_vq.py:    return (x + epsilon) / (x.sum() + n_categories * epsilon)
quantization/core_vq.py:def uniform_init(*shape: int):
quantization/core_vq.py:    t = torch.empty(shape)
quantization/core_vq.py:    nn.init.kaiming_uniform_(t)
quantization/core_vq.py:    return t
quantization/core_vq.py:def sample_vectors(samples, num: int):
quantization/core_vq.py:    num_samples, device = samples.shape[0], samples.device
quantization/core_vq.py:    if num_samples >= num:
quantization/core_vq.py:        indices = torch.randperm(num_samples, device=device)[:num]
quantization/core_vq.py:    else:
quantization/core_vq.py:        indices = torch.randint(0, num_samples, (num,), device=device)
quantization/core_vq.py:    return samples[indices]
quantization/core_vq.py:def kmeans(samples, num_clusters: int, num_iters: int = 10):
quantization/core_vq.py:    dim, dtype = samples.shape[-1], samples.dtype
quantization/core_vq.py:    means = sample_vectors(samples, num_clusters)
quantization/core_vq.py:    for _ in range(num_iters):
quantization/core_vq.py:        diffs = rearrange(samples, "n d -> n () d") - rearrange(
quantization/core_vq.py:            means, "c d -> () c d"
quantization/core_vq.py:        )
quantization/core_vq.py:        dists = -(diffs ** 2).sum(dim=-1)
quantization/core_vq.py:        buckets = dists.max(dim=-1).indices
quantization/core_vq.py:        bins = torch.bincount(buckets, minlength=num_clusters)
quantization/core_vq.py:        zero_mask = bins == 0
quantization/core_vq.py:        bins_min_clamped = bins.masked_fill(zero_mask, 1)
quantization/core_vq.py:        new_means = buckets.new_zeros(num_clusters, dim, dtype=dtype)
quantization/core_vq.py:        new_means.scatter_add_(0, repeat(buckets, "n -> n d", d=dim), samples)
quantization/core_vq.py:        new_means = new_means / bins_min_clamped[..., None]
quantization/core_vq.py:        means = torch.where(zero_mask[..., None], means, new_means)
quantization/core_vq.py:    return means, bins
quantization/core_vq.py:class EuclideanCodebook(nn.Module):
quantization/core_vq.py:    """Codebook with Euclidean distance.
quantization/core_vq.py:    Args:
quantization/core_vq.py:        dim (int): Dimension.
quantization/core_vq.py:        codebook_size (int): Codebook size.
quantization/core_vq.py:        kmeans_init (bool): Whether to use k-means to initialize the codebooks.
quantization/core_vq.py:            If set to true, run the k-means algorithm on the first training batch and use
quantization/core_vq.py:            the learned centroids as initialization.
quantization/core_vq.py:        kmeans_iters (int): Number of iterations used for k-means algorithm at initialization.
quantization/core_vq.py:        decay (float): Decay for exponential moving average over the codebooks.
quantization/core_vq.py:        epsilon (float): Epsilon value for numerical stability.
quantization/core_vq.py:        threshold_ema_dead_code (int): Threshold for dead code expiration. Replace any codes
quantization/core_vq.py:            that have an exponential moving average cluster size less than the specified threshold with
quantization/core_vq.py:            randomly selected vector from the current batch.
quantization/core_vq.py:    """
quantization/core_vq.py:    def __init__(
quantization/core_vq.py:        self,
quantization/core_vq.py:        dim: int,
quantization/core_vq.py:        codebook_size: int,
quantization/core_vq.py:        kmeans_init: int = False,
quantization/core_vq.py:        kmeans_iters: int = 10,
quantization/core_vq.py:        decay: float = 0.99,
quantization/core_vq.py:        epsilon: float = 1e-5,
quantization/core_vq.py:        threshold_ema_dead_code: int = 2,
quantization/core_vq.py:    ):
quantization/core_vq.py:        super().__init__()
quantization/core_vq.py:        self.decay = decay
quantization/core_vq.py:        init_fn: tp.Union[tp.Callable[..., torch.Tensor], tp.Any] = uniform_init if not kmeans_init else torch.zeros
quantization/core_vq.py:        embed = init_fn(codebook_size, dim)
quantization/core_vq.py:        self.codebook_size = codebook_size
quantization/core_vq.py:        self.kmeans_iters = kmeans_iters
quantization/core_vq.py:        self.epsilon = epsilon
quantization/core_vq.py:        self.threshold_ema_dead_code = threshold_ema_dead_code
quantization/core_vq.py:        self.register_buffer("inited", torch.Tensor([not kmeans_init]))
quantization/core_vq.py:        self.register_buffer("cluster_size", torch.zeros(codebook_size))
quantization/core_vq.py:        self.register_buffer("embed", embed)
quantization/core_vq.py:        self.register_buffer("embed_avg", embed.clone())
quantization/core_vq.py:    @torch.jit.ignore
quantization/core_vq.py:    def init_embed_(self, data):
quantization/core_vq.py:        if self.inited:
quantization/core_vq.py:            return
quantization/core_vq.py:        embed, cluster_size = kmeans(data, self.codebook_size, self.kmeans_iters)
quantization/core_vq.py:        self.embed.data.copy_(embed)
quantization/core_vq.py:        self.embed_avg.data.copy_(embed.clone())
quantization/core_vq.py:        self.cluster_size.data.copy_(cluster_size)
quantization/core_vq.py:        self.inited.data.copy_(torch.Tensor([True]))
quantization/core_vq.py:        # Make sure all buffers across workers are in sync after initialization
quantization/core_vq.py:        distrib.broadcast_tensors(self.buffers())
quantization/core_vq.py:    def replace_(self, samples, mask):
quantization/core_vq.py:        modified_codebook = torch.where(
quantization/core_vq.py:            mask[..., None], sample_vectors(samples, self.codebook_size), self.embed
quantization/core_vq.py:        )
quantization/core_vq.py:        self.embed.data.copy_(modified_codebook)
quantization/core_vq.py:    def expire_codes_(self, batch_samples):
quantization/core_vq.py:        if self.threshold_ema_dead_code == 0:
quantization/core_vq.py:            return
quantization/core_vq.py:        expired_codes = self.cluster_size < self.threshold_ema_dead_code
quantization/core_vq.py:        if not torch.any(expired_codes):
quantization/core_vq.py:            return
quantization/core_vq.py:        batch_samples = rearrange(batch_samples, "... d -> (...) d")
quantization/core_vq.py:        self.replace_(batch_samples, mask=expired_codes)
quantization/core_vq.py:        distrib.broadcast_tensors(self.buffers())
quantization/core_vq.py:    def preprocess(self, x):
quantization/core_vq.py:        x = rearrange(x, "... d -> (...) d")
quantization/core_vq.py:        return x
quantization/core_vq.py:    def quantize(self, x):
quantization/core_vq.py:        embed = self.embed.t()
quantization/core_vq.py:        dist = -(
quantization/core_vq.py:            x.pow(2).sum(1, keepdim=True)
quantization/core_vq.py:            - 2 * x @ embed
quantization/core_vq.py:            + embed.pow(2).sum(0, keepdim=True)
quantization/core_vq.py:        )
quantization/core_vq.py:        embed_ind = dist.max(dim=-1).indices
quantization/core_vq.py:        return embed_ind
quantization/core_vq.py:    def postprocess_emb(self, embed_ind, shape):
quantization/core_vq.py:        return embed_ind.view(*shape[:-1])
quantization/core_vq.py:    def dequantize(self, embed_ind):
quantization/core_vq.py:        quantize = F.embedding(embed_ind, self.embed)
quantization/core_vq.py:        return quantize
quantization/core_vq.py:    def encode(self, x):
quantization/core_vq.py:        shape = x.shape
quantization/core_vq.py:        # pre-process
quantization/core_vq.py:        x = self.preprocess(x)
quantization/core_vq.py:        # quantize
quantization/core_vq.py:        embed_ind = self.quantize(x)
quantization/core_vq.py:        # post-process
quantization/core_vq.py:        embed_ind = self.postprocess_emb(embed_ind, shape)
quantization/core_vq.py:        return embed_ind
quantization/core_vq.py:    def decode(self, embed_ind):
quantization/core_vq.py:        quantize = self.dequantize(embed_ind)
quantization/core_vq.py:        return quantize
quantization/core_vq.py:    def forward(self, x):
quantization/core_vq.py:        shape, dtype = x.shape, x.dtype
quantization/core_vq.py:        x = self.preprocess(x)
quantization/core_vq.py:        self.init_embed_(x)
quantization/core_vq.py:        embed_ind = self.quantize(x)
quantization/core_vq.py:        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)
quantization/core_vq.py:        embed_ind = self.postprocess_emb(embed_ind, shape)
quantization/core_vq.py:        quantize = self.dequantize(embed_ind)
quantization/core_vq.py:        if self.training:
quantization/core_vq.py:            # We do the expiry of code at that point as buffers are in sync
quantization/core_vq.py:            # and all the workers will take the same decision.
quantization/core_vq.py:            self.expire_codes_(x)
quantization/core_vq.py:            ema_inplace(self.cluster_size, embed_onehot.sum(0), self.decay)
quantization/core_vq.py:            embed_sum = x.t() @ embed_onehot
quantization/core_vq.py:            ema_inplace(self.embed_avg, embed_sum.t(), self.decay)
quantization/core_vq.py:            cluster_size = (
quantization/core_vq.py:                laplace_smoothing(self.cluster_size, self.codebook_size, self.epsilon)
quantization/core_vq.py:                * self.cluster_size.sum()
quantization/core_vq.py:            )
quantization/core_vq.py:            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)
quantization/core_vq.py:            self.embed.data.copy_(embed_normalized)
quantization/core_vq.py:        return quantize, embed_ind
quantization/core_vq.py:class VectorQuantization(nn.Module):
quantization/core_vq.py:    """Vector quantization implementation.
quantization/core_vq.py:    Currently supports only euclidean distance.
quantization/core_vq.py:    Args:
quantization/core_vq.py:        dim (int): Dimension
quantization/core_vq.py:        codebook_size (int): Codebook size
quantization/core_vq.py:        codebook_dim (int): Codebook dimension. If not defined, uses the specified dimension in dim.
quantization/core_vq.py:        decay (float): Decay for exponential moving average over the codebooks.
quantization/core_vq.py:        epsilon (float): Epsilon value for numerical stability.
quantization/core_vq.py:        kmeans_init (bool): Whether to use kmeans to initialize the codebooks.
quantization/core_vq.py:        kmeans_iters (int): Number of iterations used for kmeans initialization.
quantization/core_vq.py:        threshold_ema_dead_code (int): Threshold for dead code expiration. Replace any codes
quantization/core_vq.py:            that have an exponential moving average cluster size less than the specified threshold with
quantization/core_vq.py:            randomly selected vector from the current batch.
quantization/core_vq.py:        commitment_weight (float): Weight for commitment loss.
quantization/core_vq.py:    """
quantization/core_vq.py:    def __init__(
quantization/core_vq.py:        self,
quantization/core_vq.py:        dim: int,
quantization/core_vq.py:        codebook_size: int,
quantization/core_vq.py:        codebook_dim: tp.Optional[int] = None,
quantization/core_vq.py:        decay: float = 0.99,
quantization/core_vq.py:        epsilon: float = 1e-5,
quantization/core_vq.py:        kmeans_init: bool = True,
quantization/core_vq.py:        kmeans_iters: int = 50,
quantization/core_vq.py:        threshold_ema_dead_code: int = 2,
quantization/core_vq.py:        commitment_weight: float = 1.,
quantization/core_vq.py:    ):
quantization/core_vq.py:        super().__init__()
quantization/core_vq.py:        _codebook_dim: int = default(codebook_dim, dim)
quantization/core_vq.py:        requires_projection = _codebook_dim != dim
quantization/core_vq.py:        self.project_in = (nn.Linear(dim, _codebook_dim) if requires_projection else nn.Identity())
quantization/core_vq.py:        self.project_out = (nn.Linear(_codebook_dim, dim) if requires_projection else nn.Identity())
quantization/core_vq.py:        self.epsilon = epsilon
quantization/core_vq.py:        self.commitment_weight = commitment_weight
quantization/core_vq.py:        self._codebook = EuclideanCodebook(dim=_codebook_dim, codebook_size=codebook_size,
quantization/core_vq.py:                                           kmeans_init=kmeans_init, kmeans_iters=kmeans_iters,
quantization/core_vq.py:                                           decay=decay, epsilon=epsilon,
quantization/core_vq.py:                                           threshold_ema_dead_code=threshold_ema_dead_code)
quantization/core_vq.py:        self.codebook_size = codebook_size
quantization/core_vq.py:    @property
quantization/core_vq.py:    def codebook(self):
quantization/core_vq.py:        return self._codebook.embed
quantization/core_vq.py:    def encode(self, x):
quantization/core_vq.py:        x = rearrange(x, "b d n -> b n d")
quantization/core_vq.py:        x = self.project_in(x)
quantization/core_vq.py:        embed_in = self._codebook.encode(x)
quantization/core_vq.py:        return embed_in
quantization/core_vq.py:    def decode(self, embed_ind):
quantization/core_vq.py:        quantize = self._codebook.decode(embed_ind)
quantization/core_vq.py:        quantize = self.project_out(quantize)
quantization/core_vq.py:        quantize = rearrange(quantize, "b n d -> b d n")
quantization/core_vq.py:        return quantize
quantization/core_vq.py:    def forward(self, x):
quantization/core_vq.py:        device = x.device
quantization/core_vq.py:        x = rearrange(x, "b d n -> b n d")
quantization/core_vq.py:        x = self.project_in(x)
quantization/core_vq.py:        quantize, embed_ind = self._codebook(x)
quantization/core_vq.py:        if self.training:
quantization/core_vq.py:            quantize = x + (quantize - x).detach()
quantization/core_vq.py:        loss = torch.tensor([0.0], device=device, requires_grad=self.training)
quantization/core_vq.py:        if self.training:
quantization/core_vq.py:            warnings.warn('When using RVQ in training model, first check '
quantization/core_vq.py:                          'https://github.com/facebookresearch/encodec/issues/25 . '
quantization/core_vq.py:                          'The bug wasn\'t fixed here for reproducibility.')
quantization/core_vq.py:            if self.commitment_weight > 0:
quantization/core_vq.py:                commit_loss = F.mse_loss(quantize.detach(), x)
quantization/core_vq.py:                loss = loss + commit_loss * self.commitment_weight
quantization/core_vq.py:        quantize = self.project_out(quantize)
quantization/core_vq.py:        quantize = rearrange(quantize, "b n d -> b d n")
quantization/core_vq.py:        return quantize, embed_ind, loss
quantization/core_vq.py:class ResidualVectorQuantization(nn.Module):
quantization/core_vq.py:    """Residual vector quantization implementation.
quantization/core_vq.py:    Follows Algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf
quantization/core_vq.py:    """
quantization/core_vq.py:    def __init__(self, *, num_quantizers, **kwargs):
quantization/core_vq.py:        super().__init__()
quantization/core_vq.py:        self.layers = nn.ModuleList(
quantization/core_vq.py:            [VectorQuantization(**kwargs) for _ in range(num_quantizers)]
quantization/core_vq.py:        )
quantization/core_vq.py:    def forward(self, x, n_q: tp.Optional[int] = None):
quantization/core_vq.py:        quantized_out = 0.0
quantization/core_vq.py:        residual = x
quantization/core_vq.py:        all_losses = []
quantization/core_vq.py:        all_indices = []
quantization/core_vq.py:        n_q = n_q or len(self.layers)
quantization/core_vq.py:        for layer in self.layers[:n_q]:
quantization/core_vq.py:            quantized, indices, loss = layer(residual)
quantization/core_vq.py:            residual = residual - quantized
quantization/core_vq.py:            quantized_out = quantized_out + quantized
quantization/core_vq.py:            all_indices.append(indices)
quantization/core_vq.py:            all_losses.append(loss)
quantization/core_vq.py:        out_losses, out_indices = map(torch.stack, (all_losses, all_indices))
quantization/core_vq.py:        return quantized_out, out_indices, out_losses
quantization/core_vq.py:    def encode(self, x: torch.Tensor, n_q: tp.Optional[int] = None) -> torch.Tensor:
quantization/core_vq.py:        residual = x
quantization/core_vq.py:        all_indices = []
quantization/core_vq.py:        n_q = n_q or len(self.layers)
quantization/core_vq.py:        for layer in self.layers[:n_q]:
quantization/core_vq.py:            indices = layer.encode(residual)
quantization/core_vq.py:            quantized = layer.decode(indices)
quantization/core_vq.py:            residual = residual - quantized
quantization/core_vq.py:            all_indices.append(indices)
quantization/core_vq.py:        out_indices = torch.stack(all_indices)
quantization/core_vq.py:        return out_indices
quantization/core_vq.py:    def decode(self, q_indices: torch.Tensor) -> torch.Tensor:
quantization/core_vq.py:        quantized_out = torch.tensor(0.0, device=q_indices.device)
quantization/core_vq.py:        for i, indices in enumerate(q_indices):
quantization/core_vq.py:            layer = self.layers[i]
quantization/core_vq.py:            quantized = layer.decode(indices)
quantization/core_vq.py:            quantized_out = quantized_out + quantized
quantization/core_vq.py:        return quantized_out
quantization/vq.py:"""Residual vector quantizer implementation."""
quantization/vq.py:from dataclasses import dataclass, field
quantization/vq.py:import math
quantization/vq.py:import typing as tp
quantization/vq.py:import torch
quantization/vq.py:from torch import nn
quantization/vq.py:from .core_vq import ResidualVectorQuantization
quantization/vq.py:@dataclass
quantization/vq.py:class QuantizedResult:
quantization/vq.py:    quantized: torch.Tensor
quantization/vq.py:    codes: torch.Tensor
quantization/vq.py:    bandwidth: torch.Tensor  # bandwidth in kb/s used, per batch item.
quantization/vq.py:    penalty: tp.Optional[torch.Tensor] = None
quantization/vq.py:    metrics: dict = field(default_factory=dict)
quantization/vq.py:class ResidualVectorQuantizer(nn.Module):
quantization/vq.py:    """Residual Vector Quantizer.
quantization/vq.py:    Args:
quantization/vq.py:        dimension (int): Dimension of the codebooks.
quantization/vq.py:        n_q (int): Number of residual vector quantizers used.
quantization/vq.py:        bins (int): Codebook size.
quantization/vq.py:        decay (float): Decay for exponential moving average over the codebooks.
quantization/vq.py:        kmeans_init (bool): Whether to use kmeans to initialize the codebooks.
quantization/vq.py:        kmeans_iters (int): Number of iterations used for kmeans initialization.
quantization/vq.py:        threshold_ema_dead_code (int): Threshold for dead code expiration. Replace any codes
quantization/vq.py:            that have an exponential moving average cluster size less than the specified threshold with
quantization/vq.py:            randomly selected vector from the current batch.
quantization/vq.py:    """
quantization/vq.py:    def __init__(
quantization/vq.py:        self,
quantization/vq.py:        dimension: int = 256,
quantization/vq.py:        n_q: int = 8,
quantization/vq.py:        bins: int = 1024,
quantization/vq.py:        decay: float = 0.99,
quantization/vq.py:        kmeans_init: bool = True,
quantization/vq.py:        kmeans_iters: int = 50,
quantization/vq.py:        threshold_ema_dead_code: int = 2,
quantization/vq.py:    ):
quantization/vq.py:        super().__init__()
quantization/vq.py:        self.n_q = n_q
quantization/vq.py:        self.dimension = dimension
quantization/vq.py:        self.bins = bins
quantization/vq.py:        self.decay = decay
quantization/vq.py:        self.kmeans_init = kmeans_init
quantization/vq.py:        self.kmeans_iters = kmeans_iters
quantization/vq.py:        self.threshold_ema_dead_code = threshold_ema_dead_code
quantization/vq.py:        self.vq = ResidualVectorQuantization(
quantization/vq.py:            dim=self.dimension,
quantization/vq.py:            codebook_size=self.bins,
quantization/vq.py:            num_quantizers=self.n_q,
quantization/vq.py:            decay=self.decay,
quantization/vq.py:            kmeans_init=self.kmeans_init,
quantization/vq.py:            kmeans_iters=self.kmeans_iters,
quantization/vq.py:            threshold_ema_dead_code=self.threshold_ema_dead_code,
quantization/vq.py:        )
quantization/vq.py:    def forward(self, x: torch.Tensor, frame_rate: int, bandwidth: tp.Optional[float] = None) -> QuantizedResult:
quantization/vq.py:        """Residual vector quantization on the given input tensor.
quantization/vq.py:        Args:
quantization/vq.py:            x (torch.Tensor): Input tensor.
quantization/vq.py:            frame_rate (int): Sample rate of the input tensor.
quantization/vq.py:            bandwidth (float): Target bandwidth.
quantization/vq.py:        Returns:
quantization/vq.py:            QuantizedResult:
quantization/vq.py:                The quantized (or approximately quantized) representation with
quantization/vq.py:                the associated bandwidth and any penalty term for the loss.
quantization/vq.py:        """
quantization/vq.py:        bw_per_q = self.get_bandwidth_per_quantizer(frame_rate)
quantization/vq.py:        n_q = self.get_num_quantizers_for_bandwidth(frame_rate, bandwidth)
quantization/vq.py:        quantized, codes, commit_loss = self.vq(x, n_q=n_q)
quantization/vq.py:        bw = torch.tensor(n_q * bw_per_q).to(x)
quantization/vq.py:        return QuantizedResult(quantized, codes, bw, penalty=torch.mean(commit_loss))
quantization/vq.py:    def get_num_quantizers_for_bandwidth(self, frame_rate: int, bandwidth: tp.Optional[float] = None) -> int:
quantization/vq.py:        """Return n_q based on specified target bandwidth.
quantization/vq.py:        """
quantization/vq.py:        bw_per_q = self.get_bandwidth_per_quantizer(frame_rate)
quantization/vq.py:        n_q = self.n_q
quantization/vq.py:        if bandwidth and bandwidth > 0.:
quantization/vq.py:            # bandwidth is represented as a thousandth of what it is, e.g. 6kbps bandwidth is represented as
quantization/vq.py:            # bandwidth == 6.0
quantization/vq.py:            n_q = int(max(1, math.floor(bandwidth * 1000 / bw_per_q)))
quantization/vq.py:        return n_q
quantization/vq.py:    def get_bandwidth_per_quantizer(self, frame_rate: int):
quantization/vq.py:        """Return bandwidth per quantizer for a given input frame rate.
quantization/vq.py:        Each quantizer encodes a frame with lg(bins) bits.
quantization/vq.py:        """
quantization/vq.py:        return math.log2(self.bins) * frame_rate
quantization/vq.py:    def encode(self, x: torch.Tensor, frame_rate: int, bandwidth: tp.Optional[float] = None) -> torch.Tensor:
quantization/vq.py:        """Encode a given input tensor with the specified frame rate at the given bandwidth.
quantization/vq.py:        The RVQ encode method sets the appropriate number of quantizers to use
quantization/vq.py:        and returns indices for each quantizer.
quantization/vq.py:        """
quantization/vq.py:        n_q = self.get_num_quantizers_for_bandwidth(frame_rate, bandwidth)
quantization/vq.py:        codes = self.vq.encode(x, n_q=n_q)
quantization/vq.py:        return codes
quantization/vq.py:    def decode(self, codes: torch.Tensor) -> torch.Tensor:
quantization/vq.py:        """Decode the given codes to the quantized representation.
quantization/vq.py:        """
quantization/vq.py:        quantized = self.vq.decode(codes)
quantization/vq.py:        return quantized
quantization/ac.py:"""Arithmetic coder."""
quantization/ac.py:import io
quantization/ac.py:import math
quantization/ac.py:import random
quantization/ac.py:import typing as tp
quantization/ac.py:import torch
quantization/ac.py:from ..binary import BitPacker, BitUnpacker
quantization/ac.py:def build_stable_quantized_cdf(pdf: torch.Tensor, total_range_bits: int,
quantization/ac.py:                               roundoff: float = 1e-8, min_range: int = 2,
quantization/ac.py:                               check: bool = True) -> torch.Tensor:
quantization/ac.py:    """Turn the given PDF into a quantized CDF that splits
quantization/ac.py:    [0, 2 ** self.total_range_bits - 1] into chunks of size roughly proportional
quantization/ac.py:    to the PDF.
quantization/ac.py:    Args:
quantization/ac.py:        pdf (torch.Tensor): probability distribution, shape should be `[N]`.
quantization/ac.py:        total_range_bits (int): see `ArithmeticCoder`, the typical range we expect
quantization/ac.py:            during the coding process is `[0, 2 ** total_range_bits - 1]`.
quantization/ac.py:        roundoff (float): will round the pdf up to that level to remove difference coming
quantization/ac.py:        from e.g. evaluating the Language Model on different architectures.
quantization/ac.py:        min_range (int): minimum range width. Should always be at least 2 for numerical
quantization/ac.py:            stability. Use this to avoid pathological behavior is a value
quantization/ac.py:            that is expected to be rare actually happens in real life.
quantization/ac.py:        check (bool): if True, checks that nothing bad happened, can be deactivated for speed.
quantization/ac.py:    """
quantization/ac.py:    pdf = pdf.detach()
quantization/ac.py:    if roundoff:
quantization/ac.py:        pdf = (pdf / roundoff).floor() * roundoff
quantization/ac.py:    # interpolate with uniform distribution to achieve desired minimum probability.
quantization/ac.py:    total_range = 2 ** total_range_bits
quantization/ac.py:    cardinality = len(pdf)
quantization/ac.py:    alpha = min_range * cardinality / total_range
quantization/ac.py:    assert alpha <= 1, "you must reduce min_range"
quantization/ac.py:    ranges = (((1 - alpha) * total_range) * pdf).floor().long()
quantization/ac.py:    ranges += min_range
quantization/ac.py:    quantized_cdf = torch.cumsum(ranges, dim=-1)
quantization/ac.py:    if min_range < 2:
quantization/ac.py:        raise ValueError("min_range must be at least 2.")
quantization/ac.py:    if check:
quantization/ac.py:        assert quantized_cdf[-1] <= 2 ** total_range_bits, quantized_cdf[-1]
quantization/ac.py:        if ((quantized_cdf[1:] - quantized_cdf[:-1]) < min_range).any() or quantized_cdf[0] < min_range:
quantization/ac.py:            raise ValueError("You must increase your total_range_bits.")
quantization/ac.py:    return quantized_cdf
quantization/ac.py:class ArithmeticCoder:
quantization/ac.py:    """ArithmeticCoder,
quantization/ac.py:    Let us take a distribution `p` over `N` symbols, and assume we have a stream
quantization/ac.py:    of random variables `s_t` sampled from `p`. Let us assume that we have a budget
quantization/ac.py:    of `B` bits that we can afford to write on device. There are `2**B` possible numbers,
quantization/ac.py:    corresponding to the range `[0, 2 ** B - 1]`. We can map each of those number to a single
quantization/ac.py:    sequence `(s_t)` by doing the following:
quantization/ac.py:    1) Initialize the current range to` [0 ** 2 B - 1]`.
quantization/ac.py:    2) For each time step t, split the current range into contiguous chunks,
quantization/ac.py:        one for each possible outcome, with size roughly proportional to `p`.
quantization/ac.py:        For instance, if `p = [0.75, 0.25]`, and the range is `[0, 3]`, the chunks
quantization/ac.py:        would be `{[0, 2], [3, 3]}`.
quantization/ac.py:    3) Select the chunk corresponding to `s_t`, and replace the current range with this.
quantization/ac.py:    4) When done encoding all the values, just select any value remaining in the range.
quantization/ac.py:    You will notice that this procedure can fail: for instance if at any point in time
quantization/ac.py:    the range is smaller than `N`, then we can no longer assign a non-empty chunk to each
quantization/ac.py:    possible outcome. Intuitively, the more likely a value is, the less the range width
quantization/ac.py:    will reduce, and the longer we can go on encoding values. This makes sense: for any efficient
quantization/ac.py:    coding scheme, likely outcomes would take less bits, and more of them can be coded
quantization/ac.py:    with a fixed budget.
quantization/ac.py:    In practice, we do not know `B` ahead of time, but we have a way to inject new bits
quantization/ac.py:    when the current range decreases below a given limit (given by `total_range_bits`), without
quantization/ac.py:    having to redo all the computations. If we encode mostly likely values, we will seldom
quantization/ac.py:    need to inject new bits, but a single rare value can deplete our stock of entropy!
quantization/ac.py:    In this explanation, we assumed that the distribution `p` was constant. In fact, the present
quantization/ac.py:    code works for any sequence `(p_t)` possibly different for each timestep.
quantization/ac.py:    We also assume that `s_t ~ p_t`, but that doesn't need to be true, although the smaller
quantization/ac.py:    the KL between the true distribution and `p_t`, the most efficient the coding will be.
quantization/ac.py:    Args:
quantization/ac.py:        fo (IO[bytes]): file-like object to which the bytes will be written to.
quantization/ac.py:        total_range_bits (int): the range `M` described above is `2 ** total_range_bits.
quantization/ac.py:            Any time the current range width fall under this limit, new bits will
quantization/ac.py:            be injected to rescale the initial range.
quantization/ac.py:    """
quantization/ac.py:    def __init__(self, fo: tp.IO[bytes], total_range_bits: int = 24):
quantization/ac.py:        assert total_range_bits <= 30
quantization/ac.py:        self.total_range_bits = total_range_bits
quantization/ac.py:        self.packer = BitPacker(bits=1, fo=fo)  # we push single bits at a time.
quantization/ac.py:        self.low: int = 0
quantization/ac.py:        self.high: int = 0
quantization/ac.py:        self.max_bit: int = -1
quantization/ac.py:        self._dbg: tp.List[tp.Any] = []
quantization/ac.py:        self._dbg2: tp.List[tp.Any] = []
quantization/ac.py:    @property
quantization/ac.py:    def delta(self) -> int:
quantization/ac.py:        """Return the current range width."""
quantization/ac.py:        return self.high - self.low + 1
quantization/ac.py:    def _flush_common_prefix(self):
quantization/ac.py:        # If self.low and self.high start with the sames bits,
quantization/ac.py:        # those won't change anymore as we always just increase the range
quantization/ac.py:        # by powers of 2, and we can flush them out to the bit stream.
quantization/ac.py:        assert self.high >= self.low, (self.low, self.high)
quantization/ac.py:        assert self.high < 2 ** (self.max_bit + 1)
quantization/ac.py:        while self.max_bit >= 0:
quantization/ac.py:            b1 = self.low >> self.max_bit
quantization/ac.py:            b2 = self.high >> self.max_bit
quantization/ac.py:            if b1 == b2:
quantization/ac.py:                self.low -= (b1 << self.max_bit)
quantization/ac.py:                self.high -= (b1 << self.max_bit)
quantization/ac.py:                assert self.high >= self.low, (self.high, self.low, self.max_bit)
quantization/ac.py:                assert self.low >= 0
quantization/ac.py:                self.max_bit -= 1
quantization/ac.py:                self.packer.push(b1)
quantization/ac.py:            else:
quantization/ac.py:                break
quantization/ac.py:    def push(self, symbol: int, quantized_cdf: torch.Tensor):
quantization/ac.py:        """Push the given symbol on the stream, flushing out bits
quantization/ac.py:        if possible.
quantization/ac.py:        Args:
quantization/ac.py:            symbol (int): symbol to encode with the AC.
quantization/ac.py:            quantized_cdf (torch.Tensor): use `build_stable_quantized_cdf`
quantization/ac.py:                to build this from your pdf estimate.
quantization/ac.py:        """
quantization/ac.py:        while self.delta < 2 ** self.total_range_bits:
quantization/ac.py:            self.low *= 2
quantization/ac.py:            self.high = self.high * 2 + 1
quantization/ac.py:            self.max_bit += 1
quantization/ac.py:        range_low = 0 if symbol == 0 else quantized_cdf[symbol - 1].item()
quantization/ac.py:        range_high = quantized_cdf[symbol].item() - 1
quantization/ac.py:        effective_low = int(math.ceil(range_low * (self.delta / (2 ** self.total_range_bits))))
quantization/ac.py:        effective_high = int(math.floor(range_high * (self.delta / (2 ** self.total_range_bits))))
quantization/ac.py:        assert self.low <= self.high
quantization/ac.py:        self.high = self.low + effective_high
quantization/ac.py:        self.low = self.low + effective_low
quantization/ac.py:        assert self.low <= self.high, (effective_low, effective_high, range_low, range_high)
quantization/ac.py:        self._dbg.append((self.low, self.high))
quantization/ac.py:        self._dbg2.append((self.low, self.high))
quantization/ac.py:        outs = self._flush_common_prefix()
quantization/ac.py:        assert self.low <= self.high
quantization/ac.py:        assert self.max_bit >= -1
quantization/ac.py:        assert self.max_bit <= 61, self.max_bit
quantization/ac.py:        return outs
quantization/ac.py:    def flush(self):
quantization/ac.py:        """Flush the remaining information to the stream.
quantization/ac.py:        """
quantization/ac.py:        while self.max_bit >= 0:
quantization/ac.py:            b1 = (self.low >> self.max_bit) & 1
quantization/ac.py:            self.packer.push(b1)
quantization/ac.py:            self.max_bit -= 1
quantization/ac.py:        self.packer.flush()
quantization/ac.py:class ArithmeticDecoder:
quantization/ac.py:    """ArithmeticDecoder, see `ArithmeticCoder` for a detailed explanation.
quantization/ac.py:    Note that this must be called with **exactly** the same parameters and sequence
quantization/ac.py:    of quantized cdf as the arithmetic encoder or the wrong values will be decoded.
quantization/ac.py:    If the AC encoder current range is [L, H], with `L` and `H` having the some common
quantization/ac.py:    prefix (i.e. the same most significant bits), then this prefix will be flushed to the stream.
quantization/ac.py:    For instances, having read 3 bits `b1 b2 b3`, we know that `[L, H]` is contained inside
quantization/ac.py:    `[b1 b2 b3 0 ... 0 b1 b3 b3 1 ... 1]`. Now this specific sub-range can only be obtained
quantization/ac.py:    for a specific sequence of symbols and a binary-search allows us to decode those symbols.
quantization/ac.py:    At some point, the prefix `b1 b2 b3` will no longer be sufficient to decode new symbols,
quantization/ac.py:    and we will need to read new bits from the stream and repeat the process.
quantization/ac.py:    """
quantization/ac.py:    def __init__(self, fo: tp.IO[bytes], total_range_bits: int = 24):
quantization/ac.py:        self.total_range_bits = total_range_bits
quantization/ac.py:        self.low: int = 0
quantization/ac.py:        self.high: int = 0
quantization/ac.py:        self.current: int = 0
quantization/ac.py:        self.max_bit: int = -1
quantization/ac.py:        self.unpacker = BitUnpacker(bits=1, fo=fo)  # we pull single bits at a time.
quantization/ac.py:        # Following is for debugging
quantization/ac.py:        self._dbg: tp.List[tp.Any] = []
quantization/ac.py:        self._dbg2: tp.List[tp.Any] = []
quantization/ac.py:        self._last: tp.Any = None
quantization/ac.py:    @property
quantization/ac.py:    def delta(self) -> int:
quantization/ac.py:        return self.high - self.low + 1
quantization/ac.py:    def _flush_common_prefix(self):
quantization/ac.py:        # Given the current range [L, H], if both have a common prefix,
quantization/ac.py:        # we know we can remove it from our representation to avoid handling large numbers.
quantization/ac.py:        while self.max_bit >= 0:
quantization/ac.py:            b1 = self.low >> self.max_bit
quantization/ac.py:            b2 = self.high >> self.max_bit
quantization/ac.py:            if b1 == b2:
quantization/ac.py:                self.low -= (b1 << self.max_bit)
quantization/ac.py:                self.high -= (b1 << self.max_bit)
quantization/ac.py:                self.current -= (b1 << self.max_bit)
quantization/ac.py:                assert self.high >= self.low
quantization/ac.py:                assert self.low >= 0
quantization/ac.py:                self.max_bit -= 1
quantization/ac.py:            else:
quantization/ac.py:                break
quantization/ac.py:    def pull(self, quantized_cdf: torch.Tensor) -> tp.Optional[int]:
quantization/ac.py:        """Pull a symbol, reading as many bits from the stream as required.
quantization/ac.py:        This returns `None` when the stream has been exhausted.
quantization/ac.py:        Args:
quantization/ac.py:            quantized_cdf (torch.Tensor): use `build_stable_quantized_cdf`
quantization/ac.py:                to build this from your pdf estimate. This must be **exatly**
quantization/ac.py:                the same cdf as the one used at encoding time.
quantization/ac.py:        """
quantization/ac.py:        while self.delta < 2 ** self.total_range_bits:
quantization/ac.py:            bit = self.unpacker.pull()
quantization/ac.py:            if bit is None:
quantization/ac.py:                return None
quantization/ac.py:            self.low *= 2
quantization/ac.py:            self.high = self.high * 2 + 1
quantization/ac.py:            self.current = self.current * 2 + bit
quantization/ac.py:            self.max_bit += 1
quantization/ac.py:        def bin_search(low_idx: int, high_idx: int):
quantization/ac.py:            # Binary search is not just for coding interviews :)
quantization/ac.py:            if high_idx < low_idx:
quantization/ac.py:                raise RuntimeError("Binary search failed")
quantization/ac.py:            mid = (low_idx + high_idx) // 2
quantization/ac.py:            range_low = quantized_cdf[mid - 1].item() if mid > 0 else 0
quantization/ac.py:            range_high = quantized_cdf[mid].item() - 1
quantization/ac.py:            effective_low = int(math.ceil(range_low * (self.delta / (2 ** self.total_range_bits))))
quantization/ac.py:            effective_high = int(math.floor(range_high * (self.delta / (2 ** self.total_range_bits))))
quantization/ac.py:            low = effective_low + self.low
quantization/ac.py:            high = effective_high + self.low
quantization/ac.py:            if self.current >= low:
quantization/ac.py:                if self.current <= high:
quantization/ac.py:                    return (mid, low, high, self.current)
quantization/ac.py:                else:
quantization/ac.py:                    return bin_search(mid + 1, high_idx)
quantization/ac.py:            else:
quantization/ac.py:                return bin_search(low_idx, mid - 1)
quantization/ac.py:        self._last = (self.low, self.high, self.current, self.max_bit)
quantization/ac.py:        sym, self.low, self.high, self.current = bin_search(0, len(quantized_cdf) - 1)
quantization/ac.py:        self._dbg.append((self.low, self.high, self.current))
quantization/ac.py:        self._flush_common_prefix()
quantization/ac.py:        self._dbg2.append((self.low, self.high, self.current))
quantization/ac.py:        return sym
quantization/ac.py:def test():
quantization/ac.py:    torch.manual_seed(1234)
quantization/ac.py:    random.seed(1234)
quantization/ac.py:    for _ in range(4):
quantization/ac.py:        pdfs = []
quantization/ac.py:        cardinality = random.randrange(4000)
quantization/ac.py:        steps = random.randrange(100, 500)
quantization/ac.py:        fo = io.BytesIO()
quantization/ac.py:        encoder = ArithmeticCoder(fo)
quantization/ac.py:        symbols = []
quantization/ac.py:        for step in range(steps):
quantization/ac.py:            pdf = torch.softmax(torch.randn(cardinality), dim=0)
quantization/ac.py:            pdfs.append(pdf)
quantization/ac.py:            q_cdf = build_stable_quantized_cdf(pdf, encoder.total_range_bits)
quantization/ac.py:            symbol = torch.multinomial(pdf, 1).item()
quantization/ac.py:            symbols.append(symbol)
quantization/ac.py:            encoder.push(symbol, q_cdf)
quantization/ac.py:        encoder.flush()
quantization/ac.py:        fo.seek(0)
quantization/ac.py:        decoder = ArithmeticDecoder(fo)
quantization/ac.py:        for idx, (pdf, symbol) in enumerate(zip(pdfs, symbols)):
quantization/ac.py:            q_cdf = build_stable_quantized_cdf(pdf, encoder.total_range_bits)
quantization/ac.py:            decoded_symbol = decoder.pull(q_cdf)
quantization/ac.py:            assert decoded_symbol == symbol, idx
quantization/ac.py:        assert decoder.pull(torch.zeros(1)) is None
quantization/ac.py:if __name__ == "__main__":
quantization/ac.py:    test()
binary.py:"""Raw binary format for Encodec compressed audio. Actual compression API is in `encodec.compress`."""
binary.py:import io
binary.py:import json
binary.py:import struct
binary.py:import typing as tp
binary.py:# format is `ECDC` magic code, followed by the header size as uint32.
binary.py:# Then an uint8 indicates the protocol version (0.)
binary.py:# The header is then provided as json and should contain all required
binary.py:# informations for decoding. A raw stream of bytes is then provided
binary.py:# and should be interpretable using the json header.
binary.py:_encodec_header_struct = struct.Struct('!4sBI')
binary.py:_ENCODEC_MAGIC = b'ECDC'
binary.py:def write_ecdc_header(fo: tp.IO[bytes], metadata: tp.Any):
binary.py:    meta_dumped = json.dumps(metadata).encode('utf-8')
binary.py:    version = 0
binary.py:    header = _encodec_header_struct.pack(_ENCODEC_MAGIC, version, len(meta_dumped))
binary.py:    fo.write(header)
binary.py:    fo.write(meta_dumped)
binary.py:    fo.flush()
binary.py:def _read_exactly(fo: tp.IO[bytes], size: int) -> bytes:
binary.py:    buf = b""
binary.py:    while len(buf) < size:
binary.py:        new_buf = fo.read(size)
binary.py:        if not new_buf:
binary.py:            raise EOFError("Impossible to read enough data from the stream, "
binary.py:                           f"{size} bytes remaining.")
binary.py:        buf += new_buf
binary.py:        size -= len(new_buf)
binary.py:    return buf
binary.py:def read_ecdc_header(fo: tp.IO[bytes]):
binary.py:    header_bytes = _read_exactly(fo, _encodec_header_struct.size)
binary.py:    magic, version, meta_size = _encodec_header_struct.unpack(header_bytes)
binary.py:    if magic != _ENCODEC_MAGIC:
binary.py:        raise ValueError("File is not in ECDC format.")
binary.py:    if version != 0:
binary.py:        raise ValueError("Version not supported.")
binary.py:    meta_bytes = _read_exactly(fo, meta_size)
binary.py:    return json.loads(meta_bytes.decode('utf-8'))
binary.py:class BitPacker:
binary.py:    """Simple bit packer to handle ints with a non standard width, e.g. 10 bits.
binary.py:    Note that for some bandwidth (1.5, 3), the codebook representation
binary.py:    will not cover an integer number of bytes.
binary.py:    Args:
binary.py:        bits (int): number of bits per value that will be pushed.
binary.py:        fo (IO[bytes]): file-object to push the bytes to.
binary.py:    """
binary.py:    def __init__(self, bits: int, fo: tp.IO[bytes]):
binary.py:        self._current_value = 0
binary.py:        self._current_bits = 0
binary.py:        self.bits = bits
binary.py:        self.fo = fo
binary.py:    def push(self, value: int):
binary.py:        """Push a new value to the stream. This will immediately
binary.py:        write as many uint8 as possible to the underlying file-object."""
binary.py:        self._current_value += (value << self._current_bits)
binary.py:        self._current_bits += self.bits
binary.py:        while self._current_bits >= 8:
binary.py:            lower_8bits = self._current_value & 0xff
binary.py:            self._current_bits -= 8
binary.py:            self._current_value >>= 8
binary.py:            self.fo.write(bytes([lower_8bits]))
binary.py:    def flush(self):
binary.py:        """Flushes the remaining partial uint8, call this at the end
binary.py:        of the stream to encode."""
binary.py:        if self._current_bits:
binary.py:            self.fo.write(bytes([self._current_value]))
binary.py:            self._current_value = 0
binary.py:            self._current_bits = 0
binary.py:        self.fo.flush()
binary.py:class BitUnpacker:
binary.py:    """BitUnpacker does the opposite of `BitPacker`.
binary.py:    Args:
binary.py:        bits (int): number of bits of the values to decode.
binary.py:        fo (IO[bytes]): file-object to push the bytes to.
binary.py:        """
binary.py:    def __init__(self, bits: int, fo: tp.IO[bytes]):
binary.py:        self.bits = bits
binary.py:        self.fo = fo
binary.py:        self._mask = (1 << bits) - 1
binary.py:        self._current_value = 0
binary.py:        self._current_bits = 0
binary.py:    def pull(self) -> tp.Optional[int]:
binary.py:        """
binary.py:        Pull a single value from the stream, potentially reading some
binary.py:        extra bytes from the underlying file-object.
binary.py:        Returns `None` when reaching the end of the stream.
binary.py:        """
binary.py:        while self._current_bits < self.bits:
binary.py:            buf = self.fo.read(1)
binary.py:            if not buf:
binary.py:                return None
binary.py:            character = buf[0]
binary.py:            self._current_value += character << self._current_bits
binary.py:            self._current_bits += 8
binary.py:        out = self._current_value & self._mask
binary.py:        self._current_value >>= self.bits
binary.py:        self._current_bits -= self.bits
binary.py:        return out
binary.py:def test():
binary.py:    import torch
binary.py:    torch.manual_seed(1234)
binary.py:    for rep in range(4):
binary.py:        length: int = torch.randint(10, 2_000, (1,)).item()
binary.py:        bits: int = torch.randint(1, 16, (1,)).item()
binary.py:        tokens: tp.List[int] = torch.randint(2 ** bits, (length,)).tolist()
binary.py:        rebuilt: tp.List[int] = []
binary.py:        buf = io.BytesIO()
binary.py:        packer = BitPacker(bits, buf)
binary.py:        for token in tokens:
binary.py:            packer.push(token)
binary.py:        packer.flush()
binary.py:        buf.seek(0)
binary.py:        unpacker = BitUnpacker(bits, buf)
binary.py:        while True:
binary.py:            value = unpacker.pull()
binary.py:            if value is None:
binary.py:                break
binary.py:            rebuilt.append(value)
binary.py:        assert len(rebuilt) >= len(tokens), (len(rebuilt), len(tokens))
binary.py:        # The flushing mechanism might lead to "ghost" values at the end of the stream.
binary.py:        assert len(rebuilt) <= len(tokens) + 8 // bits, (len(rebuilt), len(tokens), bits)
binary.py:        for idx, (a, b) in enumerate(zip(tokens, rebuilt)):
binary.py:            assert a == b, (idx, a, b)
binary.py:if __name__ == '__main__':
binary.py:    test()

utils.py:"""Various utilities."""
utils.py:from hashlib import sha256
utils.py:from pathlib import Path
utils.py:import typing as tp
utils.py:import torch
utils.py:import torchaudio
utils.py:def _linear_overlap_add(frames: tp.List[torch.Tensor], stride: int):
utils.py:    # Generic overlap add, with linear fade-in/fade-out, supporting complex scenario
utils.py:    # e.g., more than 2 frames per position.
utils.py:    # The core idea is to use a weight function that is a triangle,
utils.py:    # with a maximum value at the middle of the segment.
utils.py:    # We use this weighting when summing the frames, and divide by the sum of weights
utils.py:    # for each positions at the end. Thus:
utils.py:    #   - if a frame is the only one to cover a position, the weighting is a no-op.
utils.py:    #   - if 2 frames cover a position:
utils.py:    #          ...  ...
utils.py:    #         /   \/   \
utils.py:    #        /    /\    \
utils.py:    #            S  T       , i.e. S offset of second frame starts, T end of first frame.
utils.py:    # Then the weight function for each one is: (t - S), (T - t), with `t` a given offset.
utils.py:    # After the final normalization, the weight of the second frame at position `t` is
utils.py:    # (t - S) / (t - S + (T - t)) = (t - S) / (T - S), which is exactly what we want.
utils.py:    #
utils.py:    #   - if more than 2 frames overlap at a given point, we hope that by induction
utils.py:    #      something sensible happens.
utils.py:    assert len(frames)
utils.py:    device = frames[0].device
utils.py:    dtype = frames[0].dtype
utils.py:    shape = frames[0].shape[:-1]
utils.py:    total_size = stride * (len(frames) - 1) + frames[-1].shape[-1]
utils.py:    frame_length = frames[0].shape[-1]
utils.py:    t = torch.linspace(0, 1, frame_length + 2, device=device, dtype=dtype)[1: -1]
utils.py:    weight = 0.5 - (t - 0.5).abs()
utils.py:    sum_weight = torch.zeros(total_size, device=device, dtype=dtype)
utils.py:    out = torch.zeros(*shape, total_size, device=device, dtype=dtype)
utils.py:    offset: int = 0
utils.py:    for frame in frames:
utils.py:        frame_length = frame.shape[-1]
utils.py:        out[..., offset:offset + frame_length] += weight[:frame_length] * frame
utils.py:        sum_weight[offset:offset + frame_length] += weight[:frame_length]
utils.py:        offset += stride
utils.py:    assert sum_weight.min() > 0
utils.py:    return out / sum_weight
utils.py:def _get_checkpoint_url(root_url: str, checkpoint: str):
utils.py:    if not root_url.endswith('/'):
utils.py:        root_url += '/'
utils.py:    return root_url + checkpoint
utils.py:def _check_checksum(path: Path, checksum: str):
utils.py:    sha = sha256()
utils.py:    with open(path, 'rb') as file:
utils.py:        while True:
utils.py:            buf = file.read(2**20)
utils.py:            if not buf:
utils.py:                break
utils.py:            sha.update(buf)
utils.py:    actual_checksum = sha.hexdigest()[:len(checksum)]
utils.py:    if actual_checksum != checksum:
utils.py:        raise RuntimeError(f'Invalid checksum for file {path}, '
utils.py:                           f'expected {checksum} but got {actual_checksum}')
utils.py:def convert_audio(wav: torch.Tensor, sr: int, target_sr: int, target_channels: int):
utils.py:    assert wav.dim() >= 2, "Audio tensor must have at least 2 dimensions"
utils.py:    assert wav.shape[-2] in [1, 2], "Audio must be mono or stereo."
utils.py:    *shape, channels, length = wav.shape
utils.py:    if target_channels == 1:
utils.py:        wav = wav.mean(-2, keepdim=True)
utils.py:    elif target_channels == 2:
utils.py:        wav = wav.expand(*shape, target_channels, length)
utils.py:    elif channels == 1:
utils.py:        wav = wav.expand(target_channels, -1)
utils.py:    else:
utils.py:        raise RuntimeError(f"Impossible to convert from {channels} to {target_channels}")
utils.py:    wav = torchaudio.transforms.Resample(sr, target_sr)(wav)
utils.py:    return wav
utils.py:def save_audio(wav: torch.Tensor, path: tp.Union[Path, str],
utils.py:               sample_rate: int, rescale: bool = False):
utils.py:    limit = 0.99
utils.py:    mx = wav.abs().max()
utils.py:    if rescale:
utils.py:        wav = wav * min(limit / mx, 1)
utils.py:    else:
utils.py:        wav = wav.clamp(-limit, limit)
utils.py:    torchaudio.save(str(path), wav, sample_rate=sample_rate, encoding='PCM_S', bits_per_sample=16)
msstftd.py:"""MS-STFT discriminator, provided here for reference."""
msstftd.py:import typing as tp
msstftd.py:import torchaudio
msstftd.py:import torch
msstftd.py:from torch import nn
msstftd.py:from einops import rearrange
msstftd.py:from .modules import NormConv2d
msstftd.py:FeatureMapType = tp.List[torch.Tensor]
msstftd.py:LogitsType = torch.Tensor
msstftd.py:DiscriminatorOutput = tp.Tuple[tp.List[LogitsType], tp.List[FeatureMapType]]
msstftd.py:def get_2d_padding(kernel_size: tp.Tuple[int, int], dilation: tp.Tuple[int, int] = (1, 1)):
msstftd.py:    return (((kernel_size[0] - 1) * dilation[0]) // 2, ((kernel_size[1] - 1) * dilation[1]) // 2)
msstftd.py:class DiscriminatorSTFT(nn.Module):
msstftd.py:    """STFT sub-discriminator.
msstftd.py:    Args:
msstftd.py:        filters (int): Number of filters in convolutions
msstftd.py:        in_channels (int): Number of input channels. Default: 1
msstftd.py:        out_channels (int): Number of output channels. Default: 1
msstftd.py:        n_fft (int): Size of FFT for each scale. Default: 1024
msstftd.py:        hop_length (int): Length of hop between STFT windows for each scale. Default: 256
msstftd.py:        kernel_size (tuple of int): Inner Conv2d kernel sizes. Default: ``(3, 9)``
msstftd.py:        stride (tuple of int): Inner Conv2d strides. Default: ``(1, 2)``
msstftd.py:        dilations (list of int): Inner Conv2d dilation on the time dimension. Default: ``[1, 2, 4]``
msstftd.py:        win_length (int): Window size for each scale. Default: 1024
msstftd.py:        normalized (bool): Whether to normalize by magnitude after stft. Default: True
msstftd.py:        norm (str): Normalization method. Default: `'weight_norm'`
msstftd.py:        activation (str): Activation function. Default: `'LeakyReLU'`
msstftd.py:        activation_params (dict): Parameters to provide to the activation function.
msstftd.py:        growth (int): Growth factor for the filters. Default: 1
msstftd.py:    """
msstftd.py:    def __init__(self, filters: int, in_channels: int = 1, out_channels: int = 1,
msstftd.py:                 n_fft: int = 1024, hop_length: int = 256, win_length: int = 1024, max_filters: int = 1024,
msstftd.py:                 filters_scale: int = 1, kernel_size: tp.Tuple[int, int] = (3, 9), dilations: tp.List = [1, 2, 4],
msstftd.py:                 stride: tp.Tuple[int, int] = (1, 2), normalized: bool = True, norm: str = 'weight_norm',
msstftd.py:                 activation: str = 'LeakyReLU', activation_params: dict = {'negative_slope': 0.2}):
msstftd.py:        super().__init__()
msstftd.py:        assert len(kernel_size) == 2
msstftd.py:        assert len(stride) == 2
msstftd.py:        self.filters = filters
msstftd.py:        self.in_channels = in_channels
msstftd.py:        self.out_channels = out_channels
msstftd.py:        self.n_fft = n_fft
msstftd.py:        self.hop_length = hop_length
msstftd.py:        self.win_length = win_length
msstftd.py:        self.normalized = normalized
msstftd.py:        self.activation = getattr(torch.nn, activation)(**activation_params)
msstftd.py:        self.spec_transform = torchaudio.transforms.Spectrogram(
msstftd.py:            n_fft=self.n_fft, hop_length=self.hop_length, win_length=self.win_length, window_fn=torch.hann_window,
msstftd.py:            normalized=self.normalized, center=False, pad_mode=None, power=None)
msstftd.py:        spec_channels = 2 * self.in_channels
msstftd.py:        self.convs = nn.ModuleList()
msstftd.py:        self.convs.append(
msstftd.py:            NormConv2d(spec_channels, self.filters, kernel_size=kernel_size, padding=get_2d_padding(kernel_size))
msstftd.py:        )
msstftd.py:        in_chs = min(filters_scale * self.filters, max_filters)
msstftd.py:        for i, dilation in enumerate(dilations):
msstftd.py:            out_chs = min((filters_scale ** (i + 1)) * self.filters, max_filters)
msstftd.py:            self.convs.append(NormConv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride,
msstftd.py:                                         dilation=(dilation, 1), padding=get_2d_padding(kernel_size, (dilation, 1)),
msstftd.py:                                         norm=norm))
msstftd.py:            in_chs = out_chs
msstftd.py:        out_chs = min((filters_scale ** (len(dilations) + 1)) * self.filters, max_filters)
msstftd.py:        self.convs.append(NormConv2d(in_chs, out_chs, kernel_size=(kernel_size[0], kernel_size[0]),
msstftd.py:                                     padding=get_2d_padding((kernel_size[0], kernel_size[0])),
msstftd.py:                                     norm=norm))
msstftd.py:        self.conv_post = NormConv2d(out_chs, self.out_channels,
msstftd.py:                                    kernel_size=(kernel_size[0], kernel_size[0]),
msstftd.py:                                    padding=get_2d_padding((kernel_size[0], kernel_size[0])),
msstftd.py:                                    norm=norm)
msstftd.py:    def forward(self, x: torch.Tensor):
msstftd.py:        fmap = []
msstftd.py:        z = self.spec_transform(x)  # [B, 2, Freq, Frames, 2]
msstftd.py:        z = torch.cat([z.real, z.imag], dim=1)
msstftd.py:        z = rearrange(z, 'b c w t -> b c t w')
msstftd.py:        for i, layer in enumerate(self.convs):
msstftd.py:            z = layer(z)
msstftd.py:            z = self.activation(z)
msstftd.py:            fmap.append(z)
msstftd.py:        z = self.conv_post(z)
msstftd.py:        return z, fmap
msstftd.py:class MultiScaleSTFTDiscriminator(nn.Module):
msstftd.py:    """Multi-Scale STFT (MS-STFT) discriminator.
msstftd.py:    Args:
msstftd.py:        filters (int): Number of filters in convolutions
msstftd.py:        in_channels (int): Number of input channels. Default: 1
msstftd.py:        out_channels (int): Number of output channels. Default: 1
msstftd.py:        n_ffts (Sequence[int]): Size of FFT for each scale
msstftd.py:        hop_lengths (Sequence[int]): Length of hop between STFT windows for each scale
msstftd.py:        win_lengths (Sequence[int]): Window size for each scale
msstftd.py:        **kwargs: additional args for STFTDiscriminator
msstftd.py:    """
msstftd.py:    def __init__(self, filters: int, in_channels: int = 1, out_channels: int = 1,
msstftd.py:                 n_ffts: tp.List[int] = [1024, 2048, 512], hop_lengths: tp.List[int] = [256, 512, 128],
msstftd.py:                 win_lengths: tp.List[int] = [1024, 2048, 512], **kwargs):
msstftd.py:        super().__init__()
msstftd.py:        assert len(n_ffts) == len(hop_lengths) == len(win_lengths)
msstftd.py:        self.discriminators = nn.ModuleList([
msstftd.py:            DiscriminatorSTFT(filters, in_channels=in_channels, out_channels=out_channels,
msstftd.py:                              n_fft=n_ffts[i], win_length=win_lengths[i], hop_length=hop_lengths[i], **kwargs)
msstftd.py:            for i in range(len(n_ffts))
msstftd.py:        ])
msstftd.py:        self.num_discriminators = len(self.discriminators)
msstftd.py:    def forward(self, x: torch.Tensor) -> DiscriminatorOutput:
msstftd.py:        logits = []
msstftd.py:        fmaps = []
msstftd.py:        for disc in self.discriminators:
msstftd.py:            logit, fmap = disc(x)
msstftd.py:            logits.append(logit)
msstftd.py:            fmaps.append(fmap)
msstftd.py:        return logits, fmaps
msstftd.py:def test():
msstftd.py:    disc = MultiScaleSTFTDiscriminator(filters=32)
msstftd.py:    y = torch.randn(1, 1, 24000)
msstftd.py:    y_hat = torch.randn(1, 1, 24000)
msstftd.py:    y_disc_r, fmap_r = disc(y)
msstftd.py:    y_disc_gen, fmap_gen = disc(y_hat)
msstftd.py:    assert len(y_disc_r) == len(y_disc_gen) == len(fmap_r) == len(fmap_gen) == disc.num_discriminators
msstftd.py:    assert all([len(fm) == 5 for fm in fmap_r + fmap_gen])
msstftd.py:    assert all([list(f.shape)[:2] == [1, 32] for fm in fmap_r + fmap_gen for f in fm])
msstftd.py:    assert all([len(logits.shape) == 4 for logits in y_disc_r + y_disc_gen])
msstftd.py:if __name__ == '__main__':
msstftd.py:    test()
__main__.py:"""Command-line for audio compression."""
__main__.py:import argparse
__main__.py:from pathlib import Path
__main__.py:import sys
__main__.py:import torchaudio
__main__.py:from .compress import compress, decompress, MODELS
__main__.py:from .utils import save_audio, convert_audio
__main__.py:SUFFIX = '.ecdc'
__main__.py:def get_parser():
__main__.py:    parser = argparse.ArgumentParser(
__main__.py:        'encodec',
__main__.py:        description='High fidelity neural audio codec. '
__main__.py:                    'If input is a .ecdc, decompresses it. '
__main__.py:                    'If input is .wav, compresses it. If output is also wav, '
__main__.py:                    'do a compression/decompression cycle.')
__main__.py:    parser.add_argument(
__main__.py:        'input', type=Path,
__main__.py:        help='Input file, whatever is supported by torchaudio on your system.')
__main__.py:    parser.add_argument(
__main__.py:        'output', type=Path, nargs='?',
__main__.py:        help='Output file, otherwise inferred from input file.')
__main__.py:    parser.add_argument(
__main__.py:        '-b', '--bandwidth', type=float, default=6, choices=[1.5, 3., 6., 12., 24.],
__main__.py:        help='Target bandwidth (1.5, 3, 6, 12 or 24). 1.5 is not supported with --hq.')
__main__.py:    parser.add_argument(
__main__.py:        '-q', '--hq', action='store_true',
__main__.py:        help='Use HQ stereo model operating on 48 kHz sampled audio.')
__main__.py:    parser.add_argument(
__main__.py:        '-l', '--lm', action='store_true',
__main__.py:        help='Use a language model to reduce the model size (5x slower though).')
__main__.py:    parser.add_argument(
__main__.py:        '-f', '--force', action='store_true',
__main__.py:        help='Overwrite output file if it exists.')
__main__.py:    parser.add_argument(
__main__.py:        '-s', '--decompress_suffix', type=str, default='_decompressed',
__main__.py:        help='Suffix for the decompressed output file (if no output path specified)')
__main__.py:    parser.add_argument(
__main__.py:        '-r', '--rescale', action='store_true',
__main__.py:        help='Automatically rescale the output to avoid clipping.')
__main__.py:    return parser
__main__.py:def fatal(*args):
__main__.py:    print(*args, file=sys.stderr)
__main__.py:    sys.exit(1)
__main__.py:def check_output_exists(args):
__main__.py:    if not args.output.parent.exists():
__main__.py:        fatal(f"Output folder for {args.output} does not exist.")
__main__.py:    if args.output.exists() and not args.force:
__main__.py:        fatal(f"Output file {args.output} exist. Use -f / --force to overwrite.")
__main__.py:def check_clipping(wav, args):
__main__.py:    if args.rescale:
__main__.py:        return
__main__.py:    mx = wav.abs().max()
__main__.py:    limit = 0.99
__main__.py:    if mx > limit:
__main__.py:        print(
__main__.py:            f"Clipping!! max scale {mx}, limit is {limit}. "
__main__.py:            "To avoid clipping, use the `-r` option to rescale the output.",
__main__.py:            file=sys.stderr)
__main__.py:def main():
__main__.py:    args = get_parser().parse_args()
__main__.py:    if not args.input.exists():
__main__.py:        fatal(f"Input file {args.input} does not exist.")
__main__.py:    if args.input.suffix.lower() == SUFFIX:
__main__.py:        # Decompression
__main__.py:        if args.output is None:
__main__.py:            args.output = args.input.with_name(args.input.stem + args.decompress_suffix).with_suffix('.wav')
__main__.py:        elif args.output.suffix.lower() != '.wav':
__main__.py:            fatal("Output extension must be .wav")
__main__.py:        check_output_exists(args)
__main__.py:        out, out_sample_rate = decompress(args.input.read_bytes())
__main__.py:        check_clipping(out, args)
__main__.py:        save_audio(out, args.output, out_sample_rate, rescale=args.rescale)
__main__.py:    else:
__main__.py:        # Compression
__main__.py:        if args.output is None:
__main__.py:            args.output = args.input.with_suffix(SUFFIX)
__main__.py:        elif args.output.suffix.lower() not in [SUFFIX, '.wav']:
__main__.py:            fatal(f"Output extension must be .wav or {SUFFIX}")
__main__.py:        check_output_exists(args)
__main__.py:        model_name = 'encodec_48khz' if args.hq else 'encodec_24khz'
__main__.py:        model = MODELS[model_name]()
__main__.py:        if args.bandwidth not in model.target_bandwidths:
__main__.py:            fatal(f"Bandwidth {args.bandwidth} is not supported by the model {model_name}")
__main__.py:        model.set_target_bandwidth(args.bandwidth)
__main__.py:        wav, sr = torchaudio.load(args.input)
__main__.py:        wav = convert_audio(wav, sr, model.sample_rate, model.channels)
__main__.py:        compressed = compress(model, wav, use_lm=args.lm)
__main__.py:        if args.output.suffix.lower() == SUFFIX:
__main__.py:            args.output.write_bytes(compressed)
__main__.py:        else:
__main__.py:            # Directly run decompression stage
__main__.py:            assert args.output.suffix.lower() == '.wav'
__main__.py:            out, out_sample_rate = decompress(compressed)
__main__.py:            check_clipping(out, args)
__main__.py:            save_audio(out, args.output, out_sample_rate, rescale=args.rescale)
__main__.py:if __name__ == '__main__':
__main__.py:    main()
compress.py:"""API to compress/decompress audio to bytestreams."""
compress.py:import io
compress.py:import math
compress.py:import struct
compress.py:import time
compress.py:import typing as tp
compress.py:import torch
compress.py:from . import binary
compress.py:from .quantization.ac import ArithmeticCoder, ArithmeticDecoder, build_stable_quantized_cdf
compress.py:from .model import EncodecModel, EncodedFrame
compress.py:MODELS = {
compress.py:    'encodec_24khz': EncodecModel.encodec_model_24khz,
compress.py:    'encodec_48khz': EncodecModel.encodec_model_48khz,
compress.py:}
compress.py:def compress_to_file(model: EncodecModel, wav: torch.Tensor, fo: tp.IO[bytes],
compress.py:                     use_lm: bool = True):
compress.py:    """Compress a waveform to a file-object using the given model.
compress.py:    Args:
compress.py:        model (EncodecModel): a pre-trained EncodecModel to use to compress the audio.
compress.py:        wav (torch.Tensor): waveform to compress, should have a shape `[C, T]`, with `C`
compress.py:            matching `model.channels`, and the proper sample rate (e.g. `model.sample_rate`).
compress.py:            Use `utils.convert_audio` if this is not the case.
compress.py:        fo (IO[bytes]): file-object to which the compressed bits will be written.
compress.py:            See `compress` if you want obtain a `bytes` object instead.
compress.py:        use_lm (bool): if True, use a pre-trained language model to further
compress.py:            compress the stream using Entropy Coding. This will slow down compression
compress.py:            quite a bit, expect between 20 to 30% of size reduction.
compress.py:    """
compress.py:    assert wav.dim() == 2, "Only single waveform can be encoded."
compress.py:    if model.name not in MODELS:
compress.py:        raise ValueError(f"The provided model {model.name} is not supported.")
compress.py:    if use_lm:
compress.py:        lm = model.get_lm_model()
compress.py:    with torch.no_grad():
compress.py:        frames = model.encode(wav[None])
compress.py:    metadata = {
compress.py:        'm': model.name,                 # model name
compress.py:        'al': wav.shape[-1],             # audio_length
compress.py:        'nc': frames[0][0].shape[1],     # num_codebooks
compress.py:        'lm': use_lm,                    # use lm?
compress.py:    }
compress.py:    binary.write_ecdc_header(fo, metadata)
compress.py:    for (frame, scale) in frames:
compress.py:        if scale is not None:
compress.py:            fo.write(struct.pack('!f', scale.cpu().item()))
compress.py:        _, K, T = frame.shape
compress.py:        if use_lm:
compress.py:            coder = ArithmeticCoder(fo)
compress.py:            states: tp.Any = None
compress.py:            offset = 0
compress.py:            input_ = torch.zeros(1, K, 1, dtype=torch.long, device=wav.device)
compress.py:        else:
compress.py:            packer = binary.BitPacker(model.bits_per_codebook, fo)
compress.py:        for t in range(T):
compress.py:            if use_lm:
compress.py:                with torch.no_grad():
compress.py:                    probas, states, offset = lm(input_, states, offset)
compress.py:                # We emulate a streaming scenario even though we do not provide an API for it.
compress.py:                # This gives us a more accurate benchmark.
compress.py:                input_ = 1 + frame[:, :, t: t + 1]
compress.py:            for k, value in enumerate(frame[0, :, t].tolist()):
compress.py:                if use_lm:
compress.py:                    q_cdf = build_stable_quantized_cdf(
compress.py:                        probas[0, :, k, 0], coder.total_range_bits, check=False)
compress.py:                    coder.push(value, q_cdf)
compress.py:                else:
compress.py:                    packer.push(value)
compress.py:        if use_lm:
compress.py:            coder.flush()
compress.py:        else:
compress.py:            packer.flush()
compress.py:def decompress_from_file(fo: tp.IO[bytes], device='cpu') -> tp.Tuple[torch.Tensor, int]:
compress.py:    """Decompress from a file-object.
compress.py:    Returns a tuple `(wav, sample_rate)`.
compress.py:    Args:
compress.py:        fo (IO[bytes]): file-object from which to read. If you want to decompress
compress.py:            from `bytes` instead, see `decompress`.
compress.py:        device: device to use to perform the computations.
compress.py:    """
compress.py:    metadata = binary.read_ecdc_header(fo)
compress.py:    model_name = metadata['m']
compress.py:    audio_length = metadata['al']
compress.py:    num_codebooks = metadata['nc']
compress.py:    use_lm = metadata['lm']
compress.py:    assert isinstance(audio_length, int)
compress.py:    assert isinstance(num_codebooks, int)
compress.py:    if model_name not in MODELS:
compress.py:        raise ValueError(f"The audio was compressed with an unsupported model {model_name}.")
compress.py:    model = MODELS[model_name]().to(device)
compress.py:    if use_lm:
compress.py:        lm = model.get_lm_model()
compress.py:    frames: tp.List[EncodedFrame] = []
compress.py:    segment_length = model.segment_length or audio_length
compress.py:    segment_stride = model.segment_stride or audio_length
compress.py:    for offset in range(0, audio_length, segment_stride):
compress.py:        this_segment_length = min(audio_length - offset, segment_length)
compress.py:        frame_length = int(math.ceil(this_segment_length * model.frame_rate / model.sample_rate))
compress.py:        if model.normalize:
compress.py:            scale_f, = struct.unpack('!f', binary._read_exactly(fo, struct.calcsize('!f')))
compress.py:            scale = torch.tensor(scale_f, device=device).view(1)
compress.py:        else:
compress.py:            scale = None
compress.py:        if use_lm:
compress.py:            decoder = ArithmeticDecoder(fo)
compress.py:            states: tp.Any = None
compress.py:            offset = 0
compress.py:            input_ = torch.zeros(1, num_codebooks, 1, dtype=torch.long, device=device)
compress.py:        else:
compress.py:            unpacker = binary.BitUnpacker(model.bits_per_codebook, fo)
compress.py:        frame = torch.zeros(1, num_codebooks, frame_length, dtype=torch.long, device=device)
compress.py:        for t in range(frame_length):
compress.py:            if use_lm:
compress.py:                with torch.no_grad():
compress.py:                    probas, states, offset = lm(input_, states, offset)
compress.py:            code_list: tp.List[int] = []
compress.py:            for k in range(num_codebooks):
compress.py:                if use_lm:
compress.py:                    q_cdf = build_stable_quantized_cdf(
compress.py:                        probas[0, :, k, 0], decoder.total_range_bits, check=False)
compress.py:                    code = decoder.pull(q_cdf)
compress.py:                else:
compress.py:                    code = unpacker.pull()
compress.py:                if code is None:
compress.py:                    raise EOFError("The stream ended sooner than expected.")
compress.py:                code_list.append(code)
compress.py:            codes = torch.tensor(code_list, dtype=torch.long, device=device)
compress.py:            frame[0, :, t] = codes
compress.py:            if use_lm:
compress.py:                input_ = 1 + frame[:, :, t: t + 1]
compress.py:        frames.append((frame, scale))
compress.py:    with torch.no_grad():
compress.py:        wav = model.decode(frames)
compress.py:    return wav[0, :, :audio_length], model.sample_rate
compress.py:def compress(model: EncodecModel, wav: torch.Tensor, use_lm: bool = False) -> bytes:
compress.py:    """Compress a waveform using the given model. Returns the compressed bytes.
compress.py:    Args:
compress.py:        model (EncodecModel): a pre-trained EncodecModel to use to compress the audio.
compress.py:        wav (torch.Tensor): waveform to compress, should have a shape `[C, T]`, with `C`
compress.py:            matching `model.channels`, and the proper sample rate (e.g. `model.sample_rate`).
compress.py:            Use `utils.convert_audio` if this is not the case.
compress.py:        use_lm (bool): if True, use a pre-trained language model to further
compress.py:            compress the stream using Entropy Coding. This will slow down compression
compress.py:            quite a bit, expect between 20 to 30% of size reduction.
compress.py:    """
compress.py:    fo = io.BytesIO()
compress.py:    compress_to_file(model, wav, fo, use_lm=use_lm)
compress.py:    return fo.getvalue()
compress.py:def decompress(compressed: bytes, device='cpu') -> tp.Tuple[torch.Tensor, int]:
compress.py:    """Decompress from a file-object.
compress.py:    Returns a tuple `(wav, sample_rate)`.
compress.py:    Args:
compress.py:        compressed (bytes): compressed bytes.
compress.py:        device: device to use to perform the computations.
compress.py:    """
compress.py:    fo = io.BytesIO(compressed)
compress.py:    return decompress_from_file(fo, device=device)
compress.py:def test():
compress.py:    import torchaudio
compress.py:    torch.set_num_threads(1)
compress.py:    for name in MODELS.keys():
compress.py:        model = MODELS[name]()
compress.py:        sr = model.sample_rate // 1000
compress.py:        x, _ = torchaudio.load(f'test_{sr}k.wav')
compress.py:        x = x[:, :model.sample_rate * 5]
compress.py:        model.set_target_bandwidth(12)
compress.py:        for use_lm in [False, True]:
compress.py:            print(f"Doing {name}, use_lm={use_lm}")
compress.py:            begin = time.time()
compress.py:            res = compress(model, x, use_lm=use_lm)
compress.py:            t_comp = time.time() - begin
compress.py:            x_dec, _ = decompress(res)
compress.py:            t_decomp = time.time() - begin - t_comp
compress.py:            kbps = 8 * len(res) / 1000 / (x.shape[-1] / model.sample_rate)
compress.py:            print(f"kbps: {kbps:.1f}, time comp: {t_comp:.1f} sec. "
compress.py:                  f"time decomp:{t_decomp:.1f}.")
compress.py:            assert x_dec.shape == x.shape
compress.py:if __name__ == '__main__':
compress.py:    test()
transformer.py:"""A streamable transformer."""
transformer.py:import typing as tp
transformer.py:import torch
transformer.py:import torch.nn as nn
transformer.py:import torch.nn.functional as F
transformer.py:def create_sin_embedding(positions: torch.Tensor, dim: int, max_period: float = 10000):
transformer.py:    """Create time embedding for the given positions, target dimension `dim`.
transformer.py:    """
transformer.py:    # We aim for BTC format
transformer.py:    assert dim % 2 == 0
transformer.py:    half_dim = dim // 2
transformer.py:    adim = torch.arange(half_dim, device=positions.device).view(1, 1, -1)
transformer.py:    phase = positions / (max_period ** (adim / (half_dim - 1)))
transformer.py:    return torch.cat([
transformer.py:        torch.cos(phase),
transformer.py:        torch.sin(phase),
transformer.py:    ], dim=-1)
transformer.py:class StreamingTransformerEncoderLayer(nn.TransformerEncoderLayer):
transformer.py:    def forward(self, x: torch.Tensor, x_past: torch.Tensor, past_context: int):  # type: ignore
transformer.py:        if self.norm_first:
transformer.py:            sa_input = self.norm1(x)
transformer.py:            x = x + self._sa_block(sa_input, x_past, past_context)
transformer.py:            x = x + self._ff_block(self.norm2(x))
transformer.py:        else:
transformer.py:            sa_input = x
transformer.py:            x = self.norm1(x + self._sa_block(sa_input, x_past, past_context))
transformer.py:            x = self.norm2(x + self._ff_block(x))
transformer.py:        return x, sa_input
transformer.py:    # self-attention block
transformer.py:    def _sa_block(self, x: torch.Tensor, x_past: torch.Tensor, past_context: int):  # type: ignore
transformer.py:        _, T, _ = x.shape
transformer.py:        _, H, _ = x_past.shape
transformer.py:        queries = x
transformer.py:        keys = torch.cat([x_past, x], dim=1)
transformer.py:        values = keys
transformer.py:        queries_pos = torch.arange(H, T + H, device=x.device).view(-1, 1)
transformer.py:        keys_pos = torch.arange(T + H, device=x.device).view(1, -1)
transformer.py:        delta = queries_pos - keys_pos
transformer.py:        valid_access = (delta >= 0) & (delta <= past_context)
transformer.py:        x = self.self_attn(queries, keys, values,
transformer.py:                           attn_mask=~valid_access,
transformer.py:                           need_weights=False)[0]
transformer.py:        return self.dropout1(x)
transformer.py:class StreamingTransformerEncoder(nn.Module):
transformer.py:    """TransformerEncoder with streaming support.
transformer.py:    Args:
transformer.py:        dim (int): dimension of the data.
transformer.py:        hidden_scale (int): intermediate dimension of FF module is this times the dimension.
transformer.py:        num_heads (int): number of heads.
transformer.py:        num_layers (int): number of layers.
transformer.py:        max_period (float): maxium period of cosines in the positional embedding.
transformer.py:        past_context (int or None): receptive field for the causal mask, infinite if None.
transformer.py:        gelu (bool): if true uses GeLUs, otherwise use ReLUs.
transformer.py:        norm_in (bool): normalize the input.
transformer.py:        dropout (float): dropout probability.
transformer.py:        **kwargs: See `nn.TransformerEncoderLayer`.
transformer.py:    """
transformer.py:    def __init__(self, dim, hidden_scale: float = 4., num_heads: int = 8, num_layers: int = 5,
transformer.py:                 max_period: float = 10000, past_context: int = 1000, gelu: bool = True,
transformer.py:                 norm_in: bool = True, dropout: float = 0., **kwargs):
transformer.py:        super().__init__()
transformer.py:        assert dim % num_heads == 0
transformer.py:        hidden_dim = int(dim * hidden_scale)
transformer.py:        self.max_period = max_period
transformer.py:        self.past_context = past_context
transformer.py:        activation: tp.Any = F.gelu if gelu else F.relu
transformer.py:        self.norm_in: nn.Module
transformer.py:        if norm_in:
transformer.py:            self.norm_in = nn.LayerNorm(dim)
transformer.py:        else:
transformer.py:            self.norm_in = nn.Identity()
transformer.py:        self.layers = nn.ModuleList()
transformer.py:        for idx in range(num_layers):
transformer.py:            self.layers.append(
transformer.py:                StreamingTransformerEncoderLayer(
transformer.py:                    dim, num_heads, hidden_dim,
transformer.py:                    activation=activation, batch_first=True, dropout=dropout, **kwargs))
transformer.py:    def forward(self, x: torch.Tensor,
transformer.py:                states: tp.Optional[tp.List[torch.Tensor]] = None,
transformer.py:                offset: tp.Union[int, torch.Tensor] = 0):
transformer.py:        B, T, C = x.shape
transformer.py:        if states is None:
transformer.py:            states = [torch.zeros_like(x[:, :1]) for _ in range(1 + len(self.layers))]
transformer.py:        positions = torch.arange(T, device=x.device).view(1, -1, 1) + offset
transformer.py:        pos_emb = create_sin_embedding(positions, C, max_period=self.max_period)
transformer.py:        new_state: tp.List[torch.Tensor] = []
transformer.py:        x = self.norm_in(x)
transformer.py:        x = x + pos_emb
transformer.py:        for layer_state, layer in zip(states, self.layers):
transformer.py:            x, new_layer_state = layer(x, layer_state, self.past_context)
transformer.py:            new_layer_state = torch.cat([layer_state, new_layer_state], dim=1)
transformer.py:            new_state.append(new_layer_state[:, -self.past_context:, :])
transformer.py:        return x, new_state, offset + T
__init__.py:"""Torch modules."""
__init__.py:# flake8: noqa
__init__.py:from .conv import (
__init__.py:    pad1d,
__init__.py:    unpad1d,
__init__.py:    NormConv1d,
__init__.py:    NormConvTranspose1d,
__init__.py:    NormConv2d,
__init__.py:    NormConvTranspose2d,
__init__.py:    SConv1d,
__init__.py:    SConvTranspose1d,
__init__.py:)
__init__.py:from .lstm import SLSTM
__init__.py:from .seanet import SEANetEncoder, SEANetDecoder
__init__.py:from .transformer import StreamingTransformerEncoder
norm.py:"""Normalization modules."""
norm.py:import typing as tp
norm.py:import einops
norm.py:import torch
norm.py:from torch import nn
norm.py:class ConvLayerNorm(nn.LayerNorm):
norm.py:    """
norm.py:    Convolution-friendly LayerNorm that moves channels to last dimensions
norm.py:    before running the normalization and moves them back to original position right after.
norm.py:    """
norm.py:    def __init__(self, normalized_shape: tp.Union[int, tp.List[int], torch.Size], **kwargs):
norm.py:        super().__init__(normalized_shape, **kwargs)
norm.py:    def forward(self, x):
norm.py:        x = einops.rearrange(x, 'b ... t -> b t ...')
norm.py:        x = super().forward(x)
norm.py:        x = einops.rearrange(x, 'b t ... -> b ... t')
norm.py:        return

lstm.py:from torch import nn
lstm.py:class SLSTM(nn.Module):
lstm.py:    """
lstm.py:    LSTM without worrying about the hidden state, nor the layout of the data.
lstm.py:    Expects input as convolutional layout.
lstm.py:    """
lstm.py:    def __init__(self, dimension: int, num_layers: int = 2, skip: bool = True):
lstm.py:        super().__init__()
lstm.py:        self.skip = skip
lstm.py:        self.lstm = nn.LSTM(dimension, dimension, num_layers)
lstm.py:    def forward(self, x):
lstm.py:        x = x.permute(2, 0, 1)
lstm.py:        y, _ = self.lstm(x)
lstm.py:        if self.skip:
lstm.py:            y = y + x
lstm.py:        y = y.permute(1, 2, 0)
lstm.py:        return y
seanet.py:"""Encodec SEANet-based encoder and decoder implementation."""
seanet.py:import typing as tp
seanet.py:import numpy as np
seanet.py:import torch.nn as nn
seanet.py:from . import (
seanet.py:    SConv1d,
seanet.py:    SConvTranspose1d,
seanet.py:    SLSTM
seanet.py:)
seanet.py:class SEANetResnetBlock(nn.Module):
seanet.py:    """Residual block from SEANet model.
seanet.py:    Args:
seanet.py:        dim (int): Dimension of the input/output
seanet.py:        kernel_sizes (list): List of kernel sizes for the convolutions.
seanet.py:        dilations (list): List of dilations for the convolutions.
seanet.py:        activation (str): Activation function.
seanet.py:        activation_params (dict): Parameters to provide to the activation function
seanet.py:        norm (str): Normalization method.
seanet.py:        norm_params (dict): Parameters to provide to the underlying normalization used along with the convolution.
seanet.py:        causal (bool): Whether to use fully causal convolution.
seanet.py:        pad_mode (str): Padding mode for the convolutions.
seanet.py:        compress (int): Reduced dimensionality in residual branches (from Demucs v3)
seanet.py:        true_skip (bool): Whether to use true skip connection or a simple convolution as the skip connection.
seanet.py:    """
seanet.py:    def __init__(self, dim: int, kernel_sizes: tp.List[int] = [3, 1], dilations: tp.List[int] = [1, 1],
seanet.py:                 activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},
seanet.py:                 norm: str = 'weight_norm', norm_params: tp.Dict[str, tp.Any] = {}, causal: bool = False,
seanet.py:                 pad_mode: str = 'reflect', compress: int = 2, true_skip: bool = True):
seanet.py:        super().__init__()
seanet.py:        assert len(kernel_sizes) == len(dilations), 'Number of kernel sizes should match number of dilations'
seanet.py:        act = getattr(nn, activation)
seanet.py:        hidden = dim // compress
seanet.py:        block = []
seanet.py:        for i, (kernel_size, dilation) in enumerate(zip(kernel_sizes, dilations)):
seanet.py:            in_chs = dim if i == 0 else hidden
seanet.py:            out_chs = dim if i == len(kernel_sizes) - 1 else hidden
seanet.py:            block += [
seanet.py:                act(**activation_params),
seanet.py:                SConv1d(in_chs, out_chs, kernel_size=kernel_size, dilation=dilation,
seanet.py:                        norm=norm, norm_kwargs=norm_params,
seanet.py:                        causal=causal, pad_mode=pad_mode),
seanet.py:            ]
seanet.py:        self.block = nn.Sequential(*block)
seanet.py:        self.shortcut: nn.Module
seanet.py:        if true_skip:
seanet.py:            self.shortcut = nn.Identity()
seanet.py:        else:
seanet.py:            self.shortcut = SConv1d(dim, dim, kernel_size=1, norm=norm, norm_kwargs=norm_params,
seanet.py:                                    causal=causal, pad_mode=pad_mode)
seanet.py:    def forward(self, x):
seanet.py:        return self.shortcut(x) + self.block(x)
seanet.py:class SEANetEncoder(nn.Module):
seanet.py:    """SEANet encoder.
seanet.py:    Args:
seanet.py:        channels (int): Audio channels.
seanet.py:        dimension (int): Intermediate representation dimension.
seanet.py:        n_filters (int): Base width for the model.
seanet.py:        n_residual_layers (int): nb of residual layers.
seanet.py:        ratios (Sequence[int]): kernel size and stride ratios. The encoder uses downsampling ratios instead of
seanet.py:            upsampling ratios, hence it will use the ratios in the reverse order to the ones specified here
seanet.py:            that must match the decoder order
seanet.py:        activation (str): Activation function.
seanet.py:        activation_params (dict): Parameters to provide to the activation function
seanet.py:        norm (str): Normalization method.
seanet.py:        norm_params (dict): Parameters to provide to the underlying normalization used along with the convolution.
seanet.py:        kernel_size (int): Kernel size for the initial convolution.
seanet.py:        last_kernel_size (int): Kernel size for the initial convolution.
seanet.py:        residual_kernel_size (int): Kernel size for the residual layers.
seanet.py:        dilation_base (int): How much to increase the dilation with each layer.
seanet.py:        causal (bool): Whether to use fully causal convolution.
seanet.py:        pad_mode (str): Padding mode for the convolutions.
seanet.py:        true_skip (bool): Whether to use true skip connection or a simple
seanet.py:            (streamable) convolution as the skip connection in the residual network blocks.
seanet.py:        compress (int): Reduced dimensionality in residual branches (from Demucs v3).
seanet.py:        lstm (int): Number of LSTM layers at the end of the encoder.
seanet.py:    """
seanet.py:    def __init__(self, channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 1,
seanet.py:                 ratios: tp.List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},
seanet.py:                 norm: str = 'weight_norm', norm_params: tp.Dict[str, tp.Any] = {}, kernel_size: int = 7,
seanet.py:                 last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False,
seanet.py:                 pad_mode: str = 'reflect', true_skip: bool = False, compress: int = 2, lstm: int = 2):
seanet.py:        super().__init__()
seanet.py:        self.channels = channels
seanet.py:        self.dimension = dimension
seanet.py:        self.n_filters = n_filters
seanet.py:        self.ratios = list(reversed(ratios))
seanet.py:        del ratios
seanet.py:        self.n_residual_layers = n_residual_layers
seanet.py:        self.hop_length = np.prod(self.ratios)
seanet.py:        act = getattr(nn, activation)
seanet.py:        mult = 1
seanet.py:        model: tp.List[nn.Module] = [
seanet.py:            SConv1d(channels, mult * n_filters, kernel_size, norm=norm, norm_kwargs=norm_params,
seanet.py:                    causal=causal, pad_mode=pad_mode)
seanet.py:        ]
seanet.py:        # Downsample to raw audio scale
seanet.py:        for i, ratio in enumerate(self.ratios):
seanet.py:            # Add residual layers
seanet.py:            for j in range(n_residual_layers):
seanet.py:                model += [
seanet.py:                    SEANetResnetBlock(mult * n_filters, kernel_sizes=[residual_kernel_size, 1],
seanet.py:                                      dilations=[dilation_base ** j, 1],
seanet.py:                                      norm=norm, norm_params=norm_params,
seanet.py:                                      activation=activation, activation_params=activation_params,
seanet.py:                                      causal=causal, pad_mode=pad_mode, compress=compress, true_skip=true_skip)]
seanet.py:            # Add downsampling layers
seanet.py:            model += [
seanet.py:                act(**activation_params),
seanet.py:                SConv1d(mult * n_filters, mult * n_filters * 2,
seanet.py:                        kernel_size=ratio * 2, stride=ratio,
seanet.py:                        norm=norm, norm_kwargs=norm_params,
seanet.py:                        causal=causal, pad_mode=pad_mode),
seanet.py:            ]
seanet.py:            mult *= 2
seanet.py:        if lstm:
seanet.py:            model += [SLSTM(mult * n_filters, num_layers=lstm)]
seanet.py:        model += [
seanet.py:            act(**activation_params),
seanet.py:            SConv1d(mult * n_filters, dimension, last_kernel_size, norm=norm, norm_kwargs=norm_params,
seanet.py:                    causal=causal, pad_mode=pad_mode)
seanet.py:        ]
seanet.py:        self.model = nn.Sequential(*model)
seanet.py:    def forward(self, x):
seanet.py:        return self.model(x)
seanet.py:class SEANetDecoder(nn.Module):
seanet.py:    """SEANet decoder.
seanet.py:    Args:
seanet.py:        channels (int): Audio channels.
seanet.py:        dimension (int): Intermediate representation dimension.
seanet.py:        n_filters (int): Base width for the model.
seanet.py:        n_residual_layers (int): nb of residual layers.
seanet.py:        ratios (Sequence[int]): kernel size and stride ratios
seanet.py:        activation (str): Activation function.
seanet.py:        activation_params (dict): Parameters to provide to the activation function
seanet.py:        final_activation (str): Final activation function after all convolutions.
seanet.py:        final_activation_params (dict): Parameters to provide to the activation function
seanet.py:        norm (str): Normalization method.
seanet.py:        norm_params (dict): Parameters to provide to the underlying normalization used along with the convolution.
seanet.py:        kernel_size (int): Kernel size for the initial convolution.
seanet.py:        last_kernel_size (int): Kernel size for the initial convolution.
seanet.py:        residual_kernel_size (int): Kernel size for the residual layers.
seanet.py:        dilation_base (int): How much to increase the dilation with each layer.
seanet.py:        causal (bool): Whether to use fully causal convolution.
seanet.py:        pad_mode (str): Padding mode for the convolutions.
seanet.py:        true_skip (bool): Whether to use true skip connection or a simple
seanet.py:            (streamable) convolution as the skip connection in the residual network blocks.
seanet.py:        compress (int): Reduced dimensionality in residual branches (from Demucs v3).
seanet.py:        lstm (int): Number of LSTM layers at the end of the encoder.
seanet.py:        trim_right_ratio (float): Ratio for trimming at the right of the transposed convolution under the causal setup.
seanet.py:            If equal to 1.0, it means that all the trimming is done at the right.
seanet.py:    """
seanet.py:    def __init__(self, channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 1,
seanet.py:                 ratios: tp.List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},
seanet.py:                 final_activation: tp.Optional[str] = None, final_activation_params: tp.Optional[dict] = None,
seanet.py:                 norm: str = 'weight_norm', norm_params: tp.Dict[str, tp.Any] = {}, kernel_size: int = 7,
seanet.py:                 last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False,
seanet.py:                 pad_mode: str = 'reflect', true_skip: bool = False, compress: int = 2, lstm: int = 2,
seanet.py:                 trim_right_ratio: float = 1.0):
seanet.py:        super().__init__()
seanet.py:        self.dimension = dimension
seanet.py:        self.channels = channels
seanet.py:        self.n_filters = n_filters
seanet.py:        self.ratios = ratios
seanet.py:        del ratios
seanet.py:        self.n_residual_layers = n_residual_layers
seanet.py:        self.hop_length = np.prod(self.ratios)
seanet.py:        act = getattr(nn, activation)
seanet.py:        mult = int(2 ** len(self.ratios))
seanet.py:        model: tp.List[nn.Module] = [
seanet.py:            SConv1d(dimension, mult * n_filters, kernel_size, norm=norm, norm_kwargs=norm_params,
seanet.py:                    causal=causal, pad_mode=pad_mode)
seanet.py:        ]
seanet.py:        if lstm:
seanet.py:            model += [SLSTM(mult * n_filters, num_layers=lstm)]
seanet.py:        # Upsample to raw audio scale
seanet.py:        for i, ratio in enumerate(self.ratios):
seanet.py:            # Add upsampling layers
seanet.py:            model += [
seanet.py:                act(**activation_params),
seanet.py:                SConvTranspose1d(mult * n_filters, mult * n_filters // 2,
seanet.py:                                 kernel_size=ratio * 2, stride=ratio,
seanet.py:                                 norm=norm, norm_kwargs=norm_params,
seanet.py:                                 causal=causal, trim_right_ratio=trim_right_ratio),
seanet.py:            ]
seanet.py:            # Add residual layers
seanet.py:            for j in range(n_residual_layers):
seanet.py:                model += [
seanet.py:                    SEANetResnetBlock(mult * n_filters // 2, kernel_sizes=[residual_kernel_size, 1],
seanet.py:                                      dilations=[dilation_base ** j, 1],
seanet.py:                                      activation=activation, activation_params=activation_params,
seanet.py:                                      norm=norm, norm_params=norm_params, causal=causal,
seanet.py:                                      pad_mode=pad_mode, compress=compress, true_skip=true_skip)]
seanet.py:            mult //= 2
seanet.py:        # Add final layers
seanet.py:        model += [
seanet.py:            act(**activation_params),
seanet.py:            SConv1d(n_filters, channels, last_kernel_size, norm=norm, norm_kwargs=norm_params,
seanet.py:                    causal=causal, pad_mode=pad_mode)
seanet.py:        ]
seanet.py:        # Add optional final activation to decoder (eg. tanh)
seanet.py:        if final_activation is not None:
seanet.py:            final_act = getattr(nn, final_activation)
seanet.py:            final_activation_params = final_activation_params or {}
seanet.py:            model += [
seanet.py:                final_act(**final_activation_params)
seanet.py:            ]
seanet.py:        self.model = nn.Sequential(*model)
seanet.py:    def forward(self, z):
seanet.py:        y = self.model(z)
seanet.py:        return y
seanet.py:def test():
seanet.py:    import torch
seanet.py:    encoder = SEANetEncoder()
seanet.py:    decoder = SEANetDecoder()
seanet.py:    x = torch.randn(1, 1, 24000)
seanet.py:    z = encoder(x)
seanet.py:    assert list(z.shape) == [1, 128, 75], z.shape
seanet.py:    y = decoder(z)
seanet.py:    assert y.shape == x.shape, (x.shape, y.shape)
seanet.py:if __name__ == '__main__':
seanet.py:    test()
conv.py:"""Convolutional layers wrappers and utilities."""
conv.py:import math
conv.py:import typing as tp
conv.py:import warnings
conv.py:import torch
conv.py:from torch import nn
conv.py:from torch.nn import functional as F
conv.py:from torch.nn.utils import spectral_norm, weight_norm
conv.py:from .norm import ConvLayerNorm
conv.py:CONV_NORMALIZATIONS = frozenset(['none', 'weight_norm', 'spectral_norm',
conv.py:                                 'time_layer_norm', 'layer_norm', 'time_group_norm'])
conv.py:def apply_parametrization_norm(module: nn.Module, norm: str = 'none') -> nn.Module:
conv.py:    assert norm in CONV_NORMALIZATIONS
conv.py:    if norm == 'weight_norm':
conv.py:        return weight_norm(module)
conv.py:    elif norm == 'spectral_norm':
conv.py:        return spectral_norm(module)
conv.py:    else:
conv.py:        # We already check was in CONV_NORMALIZATION, so any other choice
conv.py:        # doesn't need reparametrization.
conv.py:        return module
conv.py:def get_norm_module(module: nn.Module, causal: bool = False, norm: str = 'none', **norm_kwargs) -> nn.Module:
conv.py:    """Return the proper normalization module. If causal is True, this will ensure the returned
conv.py:    module is causal, or return an error if the normalization doesn't support causal evaluation.
conv.py:    """
conv.py:    assert norm in CONV_NORMALIZATIONS
conv.py:    if norm == 'layer_norm':
conv.py:        assert isinstance(module, nn.modules.conv._ConvNd)
conv.py:        return ConvLayerNorm(module.out_channels, **norm_kwargs)
conv.py:    elif norm == 'time_group_norm':
conv.py:        if causal:
conv.py:            raise ValueError("GroupNorm doesn't support causal evaluation.")
conv.py:        assert isinstance(module, nn.modules.conv._ConvNd)
conv.py:        return nn.GroupNorm(1, module.out_channels, **norm_kwargs)
conv.py:    else:
conv.py:        return nn.Identity()
conv.py:def get_extra_padding_for_conv1d(x: torch.Tensor, kernel_size: int, stride: int,
conv.py:                                 padding_total: int = 0) -> int:
conv.py:    """See `pad_for_conv1d`.
conv.py:    """
conv.py:    length = x.shape[-1]
conv.py:    n_frames = (length - kernel_size + padding_total) / stride + 1
conv.py:    ideal_length = (math.ceil(n_frames) - 1) * stride + (kernel_size - padding_total)
conv.py:    return ideal_length - length
conv.py:def pad_for_conv1d(x: torch.Tensor, kernel_size: int, stride: int, padding_total: int = 0):
conv.py:    """Pad for a convolution to make sure that the last window is full.
conv.py:    Extra padding is added at the end. This is required to ensure that we can rebuild
conv.py:    an output of the same length, as otherwise, even with padding, some time steps
conv.py:    might get removed.
conv.py:    For instance, with total padding = 4, kernel size = 4, stride = 2:
conv.py:        0 0 1 2 3 4 5 0 0   # (0s are padding)
conv.py:        1   2   3           # (output frames of a convolution, last 0 is never used)
conv.py:        0 0 1 2 3 4 5 0     # (output of tr. conv., but pos. 5 is going to get removed as padding)
conv.py:            1 2 3 4         # once you removed padding, we are missing one time step !
conv.py:    """
conv.py:    extra_padding = get_extra_padding_for_conv1d(x, kernel_size, stride, padding_total)
conv.py:    return F.pad(x, (0, extra_padding))
conv.py:def pad1d(x: torch.Tensor, paddings: tp.Tuple[int, int], mode: str = 'zero', value: float = 0.):
conv.py:    """Tiny wrapper around F.pad, just to allow for reflect padding on small input.
conv.py:    If this is the case, we insert extra 0 padding to the right before the reflection happen.
conv.py:    """
conv.py:    length = x.shape[-1]
conv.py:    padding_left, padding_right = paddings
conv.py:    assert padding_left >= 0 and padding_right >= 0, (padding_left, padding_right)
conv.py:    if mode == 'reflect':
conv.py:        max_pad = max(padding_left, padding_right)
conv.py:        extra_pad = 0
conv.py:        if length <= max_pad:
conv.py:            extra_pad = max_pad - length + 1
conv.py:            x = F.pad(x, (0, extra_pad))
conv.py:        padded = F.pad(x, paddings, mode, value)
conv.py:        end = padded.shape[-1] - extra_pad
conv.py:        return padded[..., :end]
conv.py:    else:
conv.py:        return F.pad(x, paddings, mode, value)
conv.py:def unpad1d(x: torch.Tensor, paddings: tp.Tuple[int, int]):
conv.py:    """Remove padding from x, handling properly zero padding. Only for 1d!"""
conv.py:    padding_left, padding_right = paddings
conv.py:    assert padding_left >= 0 and padding_right >= 0, (padding_left, padding_right)
conv.py:    assert (padding_left + padding_right) <= x.shape[-1]
conv.py:    end = x.shape[-1] - padding_right
conv.py:    return x[..., padding_left: end]
conv.py:class NormConv1d(nn.Module):
conv.py:    """Wrapper around Conv1d and normalization applied to this conv
conv.py:    to provide a uniform interface across normalization approaches.
conv.py:    """
conv.py:    def __init__(self, *args, causal: bool = False, norm: str = 'none',
conv.py:                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
conv.py:        super().__init__()
conv.py:        self.conv = apply_parametrization_norm(nn.Conv1d(*args, **kwargs), norm)
conv.py:        self.norm = get_norm_module(self.conv, causal, norm, **norm_kwargs)
conv.py:        self.norm_type = norm
conv.py:    def forward(self, x):
conv.py:        x = self.conv(x)
conv.py:        x = self.norm(x)
conv.py:        return x
conv.py:class NormConv2d(nn.Module):
conv.py:    """Wrapper around Conv2d and normalization applied to this conv
conv.py:    to provide a uniform interface across normalization approaches.
conv.py:    """
conv.py:    def __init__(self, *args, norm: str = 'none',
conv.py:                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
conv.py:        super().__init__()
conv.py:        self.conv = apply_parametrization_norm(nn.Conv2d(*args, **kwargs), norm)
conv.py:        self.norm = get_norm_module(self.conv, causal=False, norm=norm, **norm_kwargs)
conv.py:        self.norm_type = norm
conv.py:    def forward(self, x):
conv.py:        x = self.conv(x)
conv.py:        x = self.norm(x)
conv.py:        return x
conv.py:class NormConvTranspose1d(nn.Module):
conv.py:    """Wrapper around ConvTranspose1d and normalization applied to this conv
conv.py:    to provide a uniform interface across normalization approaches.
conv.py:    """
conv.py:    def __init__(self, *args, causal: bool = False, norm: str = 'none',
conv.py:                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
conv.py:        super().__init__()
conv.py:        self.convtr = apply_parametrization_norm(nn.ConvTranspose1d(*args, **kwargs), norm)
conv.py:        self.norm = get_norm_module(self.convtr, causal, norm, **norm_kwargs)
conv.py:        self.norm_type = norm
conv.py:    def forward(self, x):
conv.py:        x = self.convtr(x)
conv.py:        x = self.norm(x)
conv.py:        return x
conv.py:class NormConvTranspose2d(nn.Module):
conv.py:    """Wrapper around ConvTranspose2d and normalization applied to this conv
conv.py:    to provide a uniform interface across normalization approaches.
conv.py:    """
conv.py:    def __init__(self, *args, norm: str = 'none',
conv.py:                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
conv.py:        super().__init__()
conv.py:        self.convtr = apply_parametrization_norm(nn.ConvTranspose2d(*args, **kwargs), norm)
conv.py:        self.norm = get_norm_module(self.convtr, causal=False, norm=norm, **norm_kwargs)
conv.py:    def forward(self, x):
conv.py:        x = self.convtr(x)
conv.py:        x = self.norm(x)
conv.py:        return x
conv.py:class SConv1d(nn.Module):
conv.py:    """Conv1d with some builtin handling of asymmetric or causal padding
conv.py:    and normalization.
conv.py:    """
conv.py:    def __init__(self, in_channels: int, out_channels: int,
conv.py:                 kernel_size: int, stride: int = 1, dilation: int = 1,
conv.py:                 groups: int = 1, bias: bool = True, causal: bool = False,
conv.py:                 norm: str = 'none', norm_kwargs: tp.Dict[str, tp.Any] = {},
conv.py:                 pad_mode: str = 'reflect'):
conv.py:        super().__init__()
conv.py:        # warn user on unusual setup between dilation and stride
conv.py:        if stride > 1 and dilation > 1:
conv.py:            warnings.warn('SConv1d has been initialized with stride > 1 and dilation > 1'
conv.py:                          f' (kernel_size={kernel_size} stride={stride}, dilation={dilation}).')
conv.py:        self.conv = NormConv1d(in_channels, out_channels, kernel_size, stride,
conv.py:                               dilation=dilation, groups=groups, bias=bias, causal=causal,
conv.py:                               norm=norm, norm_kwargs=norm_kwargs)
conv.py:        self.causal = causal
conv.py:        self.pad_mode = pad_mode
conv.py:    def forward(self, x):
conv.py:        B, C, T = x.shape
conv.py:        kernel_size = self.conv.conv.kernel_size[0]
conv.py:        stride = self.conv.conv.stride[0]
conv.py:        dilation = self.conv.conv.dilation[0]
conv.py:        kernel_size = (kernel_size - 1) * dilation + 1  # effective kernel size with dilations
conv.py:        padding_total = kernel_size - stride
conv.py:        extra_padding = get_extra_padding_for_conv1d(x, kernel_size, stride, padding_total)
conv.py:        if self.causal:
conv.py:            # Left padding for causal
conv.py:            x = pad1d(x, (padding_total, extra_padding), mode=self.pad_mode)
conv.py:        else:
conv.py:            # Asymmetric padding required for odd strides
conv.py:            padding_right = padding_total // 2
conv.py:            padding_left = padding_total - padding_right
conv.py:            x = pad1d(x, (padding_left, padding_right + extra_padding), mode=self.pad_mode)
conv.py:        return self.conv(x)
conv.py:class SConvTranspose1d(nn.Module):
conv.py:    """ConvTranspose1d with some builtin handling of asymmetric or causal padding
conv.py:    and normalization.
conv.py:    """
conv.py:    def __init__(self, in_channels: int, out_channels: int,
conv.py:                 kernel_size: int, stride: int = 1, causal: bool = False,
conv.py:                 norm: str = 'none', trim_right_ratio: float = 1.,
conv.py:                 norm_kwargs: tp.Dict[str, tp.Any] = {}):
conv.py:        super().__init__()
conv.py:        self.convtr = NormConvTranspose1d(in_channels, out_channels, kernel_size, stride,
conv.py:                                          causal=causal, norm=norm, norm_kwargs=norm_kwargs)
conv.py:        self.causal = causal
conv.py:        self.trim_right_ratio = trim_right_ratio
conv.py:        assert self.causal or self.trim_right_ratio == 1., \
conv.py:            "`trim_right_ratio` != 1.0 only makes sense for causal convolutions"
conv.py:        assert self.trim_right_ratio >= 0. and self.trim_right_ratio <= 1.
conv.py:    def forward(self, x):
conv.py:        kernel_size = self.convtr.convtr.kernel_size[0]
conv.py:        stride = self.convtr.convtr.stride[0]
conv.py:        padding_total = kernel_size - stride
conv.py:        y = self.convtr(x)
conv.py:        # We will only trim fixed padding. Extra padding from `pad_for_conv1d` would be
conv.py:        # removed at the very end, when keeping only the right length for the output,
conv.py:        # as removing it here would require also passing the length at the matching layer
conv.py:        # in the encoder.
conv.py:        if self.causal:
conv.py:            # Trim the padding on the right according to the specified ratio
conv.py:            # if trim_right_ratio = 1.0, trim everything from right
conv.py:            padding_right = math.ceil(padding_total * self.trim_right_ratio)
conv.py:            padding_left = padding_total - padding_right
conv.py:            y = unpad1d(y, (padding_left, padding_right))
conv.py:        else:
conv.py:            # Asymmetric padding required for odd strides
conv.py:            padding_right = padding_total // 2
conv.py:            padding_left = padding_total - padding_right
conv.py:            y = unpad1d(y, (padding_left, padding_right))
conv.py:        return y
