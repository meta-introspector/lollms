./setup.py:#!/usr/bin/env python3
./setup.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./setup.py:# All rights reserved.
./setup.py:#
./setup.py:# This source code is licensed under the license found in the
./setup.py:# LICENSE file in the root directory of this source tree.
./setup.py:#
./setup.py:# Inspired from https://github.com/kennethreitz/setup.py
./setup.py:from pathlib import Path
./setup.py:from setuptools import setup
./setup.py:NAME = 'encodec'
./setup.py:DESCRIPTION = 'High fidelity neural audio codec'
./setup.py:URL = 'https://github.com/facebookresearch/encodec'
./setup.py:EMAIL = 'defossez@fb.com'
./setup.py:AUTHOR = 'Alexandre DÃ©fossez, Jade Copet, Yossi Adi, Gabriel Synnaeve'
./setup.py:REQUIRES_PYTHON = '>=3.8.0'
./setup.py:for line in open('encodec/__init__.py'):
./setup.py:    line = line.strip()
./setup.py:    if '__version__' in line:
./setup.py:        context = {}
./setup.py:        exec(line, context)
./setup.py:        VERSION = context['__version__']
./setup.py:HERE = Path(__file__).parent
./setup.py:try:
./setup.py:    with open(HERE / "README.md", encoding='utf-8') as f:
./setup.py:        long_description = '\n' + f.read()
./setup.py:except FileNotFoundError:
./setup.py:    long_description = DESCRIPTION
./setup.py:setup(
./setup.py:    name=NAME,
./setup.py:    version=VERSION,
./setup.py:    description=DESCRIPTION,
./setup.py:    long_description=long_description,
./setup.py:    long_description_content_type='text/markdown',
./setup.py:    author=AUTHOR,
./setup.py:    author_email=EMAIL,
./setup.py:    python_requires=REQUIRES_PYTHON,
./setup.py:    url=URL,
./setup.py:    packages=['encodec', 'encodec.quantization', 'encodec.modules'],
./setup.py:    extras_require={
./setup.py:        'dev': ['flake8', 'mypy', 'pdoc3'],
./setup.py:    },
./setup.py:    install_requires=['numpy', 'torch', 'torchaudio', 'einops'],
./setup.py:    include_package_data=True,
./setup.py:    entry_points={
./setup.py:        'console_scripts': ['encodec=encodec.__main__:main'],
./setup.py:    },
./setup.py:    license='MIT License',
./setup.py:    classifiers=[
./setup.py:        # Trove classifiers
./setup.py:        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers
./setup.py:        'Topic :: Multimedia :: Sound/Audio',
./setup.py:        'Topic :: Scientific/Engineering :: Artificial Intelligence',
./setup.py:        'License :: OSI Approved :: MIT License',
./setup.py:    ])
./encodec/distrib.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/distrib.py:# All rights reserved.
./encodec/distrib.py:#
./encodec/distrib.py:# This source code is licensed under the license found in the
./encodec/distrib.py:# LICENSE file in the root directory of this source tree.
./encodec/distrib.py:"""Torch distributed utilities."""
./encodec/distrib.py:import typing as tp
./encodec/distrib.py:import torch
./encodec/distrib.py:def rank():
./encodec/distrib.py:    if torch.distributed.is_initialized():
./encodec/distrib.py:        return torch.distributed.get_rank()
./encodec/distrib.py:    else:
./encodec/distrib.py:        return 0
./encodec/distrib.py:def world_size():
./encodec/distrib.py:    if torch.distributed.is_initialized():
./encodec/distrib.py:        return torch.distributed.get_world_size()
./encodec/distrib.py:    else:
./encodec/distrib.py:        return 1
./encodec/distrib.py:def is_distributed():
./encodec/distrib.py:    return world_size() > 1
./encodec/distrib.py:def all_reduce(tensor: torch.Tensor, op=torch.distributed.ReduceOp.SUM):
./encodec/distrib.py:    if is_distributed():
./encodec/distrib.py:        return torch.distributed.all_reduce(tensor, op)
./encodec/distrib.py:def _is_complex_or_float(tensor):
./encodec/distrib.py:    return torch.is_floating_point(tensor) or torch.is_complex(tensor)
./encodec/distrib.py:def _check_number_of_params(params: tp.List[torch.Tensor]):
./encodec/distrib.py:    # utility function to check that the number of params in all workers is the same,
./encodec/distrib.py:    # and thus avoid a deadlock with distributed all reduce.
./encodec/distrib.py:    if not is_distributed() or not params:
./encodec/distrib.py:        return
./encodec/distrib.py:    tensor = torch.tensor([len(params)], device=params[0].device, dtype=torch.long)
./encodec/distrib.py:    all_reduce(tensor)
./encodec/distrib.py:    if tensor.item() != len(params) * world_size():
./encodec/distrib.py:        # If not all the workers have the same number, for at least one of them,
./encodec/distrib.py:        # this inequality will be verified.
./encodec/distrib.py:        raise RuntimeError(f"Mismatch in number of params: ours is {len(params)}, "
./encodec/distrib.py:                           "at least one worker has a different one.")
./encodec/distrib.py:def broadcast_tensors(tensors: tp.Iterable[torch.Tensor], src: int = 0):
./encodec/distrib.py:    """Broadcast the tensors from the given parameters to all workers.
./encodec/distrib.py:    This can be used to ensure that all workers have the same model to start with.
./encodec/distrib.py:    """
./encodec/distrib.py:    if not is_distributed():
./encodec/distrib.py:        return
./encodec/distrib.py:    tensors = [tensor for tensor in tensors if _is_complex_or_float(tensor)]
./encodec/distrib.py:    _check_number_of_params(tensors)
./encodec/distrib.py:    handles = []
./encodec/distrib.py:    for tensor in tensors:
./encodec/distrib.py:        handle = torch.distributed.broadcast(tensor.data, src=src, async_op=True)
./encodec/distrib.py:        handles.append(handle)
./encodec/distrib.py:    for handle in handles:
./encodec/distrib.py:        handle.wait()
./encodec/distrib.py:def sync_buffer(buffers, average=True):
./encodec/distrib.py:    """
./encodec/distrib.py:    Sync grad for buffers. If average is False, broadcast instead of averaging.
./encodec/distrib.py:    """
./encodec/distrib.py:    if not is_distributed():
./encodec/distrib.py:        return
./encodec/distrib.py:    handles = []
./encodec/distrib.py:    for buffer in buffers:
./encodec/distrib.py:        if torch.is_floating_point(buffer.data):
./encodec/distrib.py:            if average:
./encodec/distrib.py:                handle = torch.distributed.all_reduce(
./encodec/distrib.py:                    buffer.data, op=torch.distributed.ReduceOp.SUM, async_op=True)
./encodec/distrib.py:            else:
./encodec/distrib.py:                handle = torch.distributed.broadcast(
./encodec/distrib.py:                    buffer.data, src=0, async_op=True)
./encodec/distrib.py:            handles.append((buffer, handle))
./encodec/distrib.py:    for buffer, handle in handles:
./encodec/distrib.py:        handle.wait()
./encodec/distrib.py:        if average:
./encodec/distrib.py:            buffer.data /= world_size
./encodec/distrib.py:def sync_grad(params):
./encodec/distrib.py:    """
./encodec/distrib.py:    Simpler alternative to DistributedDataParallel, that doesn't rely
./encodec/distrib.py:    on any black magic. For simple models it can also be as fast.
./encodec/distrib.py:    Just call this on your model parameters after the call to backward!
./encodec/distrib.py:    """
./encodec/distrib.py:    if not is_distributed():
./encodec/distrib.py:        return
./encodec/distrib.py:    handles = []
./encodec/distrib.py:    for p in params:
./encodec/distrib.py:        if p.grad is not None:
./encodec/distrib.py:            handle = torch.distributed.all_reduce(
./encodec/distrib.py:                p.grad.data, op=torch.distributed.ReduceOp.SUM, async_op=True)
./encodec/distrib.py:            handles.append((p, handle))
./encodec/distrib.py:    for p, handle in handles:
./encodec/distrib.py:        handle.wait()
./encodec/distrib.py:        p.grad.data /= world_size()
./encodec/distrib.py:def average_metrics(metrics: tp.Dict[str, float], count=1.):
./encodec/distrib.py:    """Average a dictionary of metrics across all workers, using the optional
./encodec/distrib.py:    `count` as unnormalized weight.
./encodec/distrib.py:    """
./encodec/distrib.py:    if not is_distributed():
./encodec/distrib.py:        return metrics
./encodec/distrib.py:    keys, values = zip(*metrics.items())
./encodec/distrib.py:    device = 'cuda' if torch.cuda.is_available() else 'cpu'
./encodec/distrib.py:    tensor = torch.tensor(list(values) + [1], device=device, dtype=torch.float32)
./encodec/distrib.py:    tensor *= count
./encodec/distrib.py:    all_reduce(tensor)
./encodec/distrib.py:    averaged = (tensor[:-1] / tensor[-1]).cpu().tolist()
./encodec/distrib.py:    return dict(zip(keys, averaged))
./encodec/__init__.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/__init__.py:# All rights reserved.
./encodec/__init__.py:#
./encodec/__init__.py:# This source code is licensed under the license found in the
./encodec/__init__.py:# LICENSE file in the root directory of this source tree.
./encodec/__init__.py:# flake8: noqa
./encodec/__init__.py:"""EnCodec neural audio codec."""
./encodec/__init__.py:__version__ = "0.1.2a3"
./encodec/__init__.py:from .model import EncodecModel
./encodec/__init__.py:from .compress import compress, decompress
./encodec/balancer.py:from collections import defaultdict
./encodec/balancer.py:import typing as tp
./encodec/balancer.py:import torch
./encodec/balancer.py:from torch import autograd
./encodec/balancer.py:from .distrib import average_metrics
./encodec/balancer.py:def averager(beta: float = 1):
./encodec/balancer.py:    """
./encodec/balancer.py:    Exponential Moving Average callback.
./encodec/balancer.py:    Returns a single function that can be called to repeatidly update the EMA
./encodec/balancer.py:    with a dict of metrics. The callback will return
./encodec/balancer.py:    the new averaged dict of metrics.
./encodec/balancer.py:    Note that for `beta=1`, this is just plain averaging.
./encodec/balancer.py:    """
./encodec/balancer.py:    fix: tp.Dict[str, float] = defaultdict(float)
./encodec/balancer.py:    total: tp.Dict[str, float] = defaultdict(float)
./encodec/balancer.py:    def _update(metrics: tp.Dict[str, tp.Any], weight: float = 1) -> tp.Dict[str, float]:
./encodec/balancer.py:        nonlocal total, fix
./encodec/balancer.py:        for key, value in metrics.items():
./encodec/balancer.py:            total[key] = total[key] * beta + weight * float(value)
./encodec/balancer.py:            fix[key] = fix[key] * beta + weight
./encodec/balancer.py:        return {key: tot / fix[key] for key, tot in total.items()}
./encodec/balancer.py:    return _update
./encodec/balancer.py:class Balancer:
./encodec/balancer.py:    """Loss balancer.
./encodec/balancer.py:    The loss balancer combines losses together to compute gradients for the backward.
./encodec/balancer.py:    A call to the balancer will weight the losses according the specified weight coefficients.
./encodec/balancer.py:    A call to the backward method of the balancer will compute the gradients, combining all the losses and
./encodec/balancer.py:    potentially rescaling the gradients, which can help stabilize the training and reasonate
./encodec/balancer.py:    about multiple losses with varying scales.
./encodec/balancer.py:    Expected usage:
./encodec/balancer.py:        weights = {'loss_a': 1, 'loss_b': 4}
./encodec/balancer.py:        balancer = Balancer(weights, ...)
./encodec/balancer.py:        losses: dict = {}
./encodec/balancer.py:        losses['loss_a'] = compute_loss_a(x, y)
./encodec/balancer.py:        losses['loss_b'] = compute_loss_b(x, y)
./encodec/balancer.py:        if model.training():
./encodec/balancer.py:            balancer.backward(losses, x)
./encodec/balancer.py:    ..Warning:: It is unclear how this will interact with DistributedDataParallel,
./encodec/balancer.py:        in particular if you have some losses not handled by the balancer. In that case
./encodec/balancer.py:        you can use `encodec.distrib.sync_grad(model.parameters())` and
./encodec/balancer.py:        `encodec.distrib.sync_buffwers(model.buffers())` as a safe alternative.
./encodec/balancer.py:    Args:
./encodec/balancer.py:        weights (Dict[str, float]): Weight coefficient for each loss. The balancer expect the losses keys
./encodec/balancer.py:            from the backward method to match the weights keys to assign weight to each of the provided loss.
./encodec/balancer.py:        rescale_grads (bool): Whether to rescale gradients or not, without. If False, this is just
./encodec/balancer.py:            a regular weighted sum of losses.
./encodec/balancer.py:        total_norm (float): Reference norm when rescaling gradients, ignored otherwise.
./encodec/balancer.py:        emay_decay (float): EMA decay for averaging the norms when `rescale_grads` is True.
./encodec/balancer.py:        per_batch_item (bool): Whether to compute the averaged norm per batch item or not. This only holds
./encodec/balancer.py:            when rescaling the gradients.
./encodec/balancer.py:        epsilon (float): Epsilon value for numerical stability.
./encodec/balancer.py:        monitor (bool): Whether to store additional ratio for each loss key in metrics.
./encodec/balancer.py:    """
./encodec/balancer.py:    def __init__(self, weights: tp.Dict[str, float], rescale_grads: bool = True, total_norm: float = 1.,
./encodec/balancer.py:                 ema_decay: float = 0.999, per_batch_item: bool = True, epsilon: float = 1e-12,
./encodec/balancer.py:                 monitor: bool = False):
./encodec/balancer.py:        self.weights = weights
./encodec/balancer.py:        self.per_batch_item = per_batch_item
./encodec/balancer.py:        self.total_norm = total_norm
./encodec/balancer.py:        self.averager = averager(ema_decay)
./encodec/balancer.py:        self.epsilon = epsilon
./encodec/balancer.py:        self.monitor = monitor
./encodec/balancer.py:        self.rescale_grads = rescale_grads
./encodec/balancer.py:        self._metrics: tp.Dict[str, tp.Any] = {}
./encodec/balancer.py:    @property
./encodec/balancer.py:    def metrics(self):
./encodec/balancer.py:        return self._metrics
./encodec/balancer.py:    def backward(self, losses: tp.Dict[str, torch.Tensor], input: torch.Tensor):
./encodec/balancer.py:        norms = {}
./encodec/balancer.py:        grads = {}
./encodec/balancer.py:        for name, loss in losses.items():
./encodec/balancer.py:            grad, = autograd.grad(loss, [input], retain_graph=True)
./encodec/balancer.py:            if self.per_batch_item:
./encodec/balancer.py:                dims = tuple(range(1, grad.dim()))
./encodec/balancer.py:                norm = grad.norm(dim=dims).mean()
./encodec/balancer.py:            else:
./encodec/balancer.py:                norm = grad.norm()
./encodec/balancer.py:            norms[name] = norm
./encodec/balancer.py:            grads[name] = grad
./encodec/balancer.py:        count = 1
./encodec/balancer.py:        if self.per_batch_item:
./encodec/balancer.py:            count = len(grad)
./encodec/balancer.py:        avg_norms = average_metrics(self.averager(norms), count)
./encodec/balancer.py:        total = sum(avg_norms.values())
./encodec/balancer.py:        self._metrics = {}
./encodec/balancer.py:        if self.monitor:
./encodec/balancer.py:            for k, v in avg_norms.items():
./encodec/balancer.py:                self._metrics[f'ratio_{k}'] = v / total
./encodec/balancer.py:        total_weights = sum([self.weights[k] for k in avg_norms])
./encodec/balancer.py:        ratios = {k: w / total_weights for k, w in self.weights.items()}
./encodec/balancer.py:        out_grad: tp.Any = 0
./encodec/balancer.py:        for name, avg_norm in avg_norms.items():
./encodec/balancer.py:            if self.rescale_grads:
./encodec/balancer.py:                scale = ratios[name] * self.total_norm / (self.epsilon + avg_norm)
./encodec/balancer.py:                grad = grads[name] * scale
./encodec/balancer.py:            else:
./encodec/balancer.py:                grad = self.weights[name] * grads[name]
./encodec/balancer.py:            out_grad += grad
./encodec/balancer.py:        input.backward(out_grad)
./encodec/balancer.py:def test():
./encodec/balancer.py:    from torch.nn import functional as F
./encodec/balancer.py:    x = torch.zeros(1, requires_grad=True)
./encodec/balancer.py:    one = torch.ones_like(x)
./encodec/balancer.py:    loss_1 = F.l1_loss(x, one)
./encodec/balancer.py:    loss_2 = 100 * F.l1_loss(x, -one)
./encodec/balancer.py:    losses = {'1': loss_1, '2': loss_2}
./encodec/balancer.py:    balancer = Balancer(weights={'1': 1, '2': 1}, rescale_grads=False)
./encodec/balancer.py:    balancer.backward(losses, x)
./encodec/balancer.py:    assert torch.allclose(x.grad, torch.tensor(99.)), x.grad
./encodec/balancer.py:    loss_1 = F.l1_loss(x, one)
./encodec/balancer.py:    loss_2 = 100 * F.l1_loss(x, -one)
./encodec/balancer.py:    losses = {'1': loss_1, '2': loss_2}
./encodec/balancer.py:    x.grad = None
./encodec/balancer.py:    balancer = Balancer(weights={'1': 1, '2': 1}, rescale_grads=True)
./encodec/balancer.py:    balancer.backward({'1': loss_1, '2': loss_2}, x)
./encodec/balancer.py:    assert torch.allclose(x.grad, torch.tensor(0.)), x.grad
./encodec/balancer.py:if __name__ == '__main__':
./encodec/balancer.py:    test()
./encodec/model.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/model.py:# All rights reserved.
./encodec/model.py:#
./encodec/model.py:# This source code is licensed under the license found in the
./encodec/model.py:# LICENSE file in the root directory of this source tree.
./encodec/model.py:"""EnCodec model implementation."""
./encodec/model.py:import math
./encodec/model.py:from pathlib import Path
./encodec/model.py:import typing as tp
./encodec/model.py:import numpy as np
./encodec/model.py:import torch
./encodec/model.py:from torch import nn
./encodec/model.py:from . import quantization as qt
./encodec/model.py:from . import modules as m
./encodec/model.py:from .utils import _check_checksum, _linear_overlap_add, _get_checkpoint_url
./encodec/model.py:ROOT_URL = 'https://dl.fbaipublicfiles.com/encodec/v0/'
./encodec/model.py:EncodedFrame = tp.Tuple[torch.Tensor, tp.Optional[torch.Tensor]]
./encodec/model.py:class LMModel(nn.Module):
./encodec/model.py:    """Language Model to estimate probabilities of each codebook entry.
./encodec/model.py:    We predict all codebooks in parallel for a given time step.
./encodec/model.py:    Args:
./encodec/model.py:        n_q (int): number of codebooks.
./encodec/model.py:        card (int): codebook cardinality.
./encodec/model.py:        dim (int): transformer dimension.
./encodec/model.py:        **kwargs: passed to `encodec.modules.transformer.StreamingTransformerEncoder`.
./encodec/model.py:    """
./encodec/model.py:    def __init__(self, n_q: int = 32, card: int = 1024, dim: int = 200, **kwargs):
./encodec/model.py:        super().__init__()
./encodec/model.py:        self.card = card
./encodec/model.py:        self.n_q = n_q
./encodec/model.py:        self.dim = dim
./encodec/model.py:        self.transformer = m.StreamingTransformerEncoder(dim=dim, **kwargs)
./encodec/model.py:        self.emb = nn.ModuleList([nn.Embedding(card + 1, dim) for _ in range(n_q)])
./encodec/model.py:        self.linears = nn.ModuleList([nn.Linear(dim, card) for _ in range(n_q)])
./encodec/model.py:    def forward(self, indices: torch.Tensor,
./encodec/model.py:                states: tp.Optional[tp.List[torch.Tensor]] = None, offset: int = 0):
./encodec/model.py:        """
./encodec/model.py:        Args:
./encodec/model.py:            indices (torch.Tensor): indices from the previous time step. Indices
./encodec/model.py:                should be 1 + actual index in the codebook. The value 0 is reserved for
./encodec/model.py:                when the index is missing (i.e. first time step). Shape should be
./encodec/model.py:                `[B, n_q, T]`.
./encodec/model.py:            states: state for the streaming decoding.
./encodec/model.py:            offset: offset of the current time step.
./encodec/model.py:        Returns a 3-tuple `(probabilities, new_states, new_offset)` with probabilities
./encodec/model.py:        with a shape `[B, card, n_q, T]`.
./encodec/model.py:        """
./encodec/model.py:        B, K, T = indices.shape
./encodec/model.py:        input_ = sum([self.emb[k](indices[:, k]) for k in range(K)])
./encodec/model.py:        out, states, offset = self.transformer(input_, states, offset)
./encodec/model.py:        logits = torch.stack([self.linears[k](out) for k in range(K)], dim=1).permute(0, 3, 1, 2)
./encodec/model.py:        return torch.softmax(logits, dim=1), states, offset
./encodec/model.py:class EncodecModel(nn.Module):
./encodec/model.py:    """EnCodec model operating on the raw waveform.
./encodec/model.py:    Args:
./encodec/model.py:        target_bandwidths (list of float): Target bandwidths.
./encodec/model.py:        encoder (nn.Module): Encoder network.
./encodec/model.py:        decoder (nn.Module): Decoder network.
./encodec/model.py:        sample_rate (int): Audio sample rate.
./encodec/model.py:        channels (int): Number of audio channels.
./encodec/model.py:        normalize (bool): Whether to apply audio normalization.
./encodec/model.py:        segment (float or None): segment duration in sec. when doing overlap-add.
./encodec/model.py:        overlap (float): overlap between segment, given as a fraction of the segment duration.
./encodec/model.py:        name (str): name of the model, used as metadata when compressing audio.
./encodec/model.py:    """
./encodec/model.py:    def __init__(self,
./encodec/model.py:                 encoder: m.SEANetEncoder,
./encodec/model.py:                 decoder: m.SEANetDecoder,
./encodec/model.py:                 quantizer: qt.ResidualVectorQuantizer,
./encodec/model.py:                 target_bandwidths: tp.List[float],
./encodec/model.py:                 sample_rate: int,
./encodec/model.py:                 channels: int,
./encodec/model.py:                 normalize: bool = False,
./encodec/model.py:                 segment: tp.Optional[float] = None,
./encodec/model.py:                 overlap: float = 0.01,
./encodec/model.py:                 name: str = 'unset'):
./encodec/model.py:        super().__init__()
./encodec/model.py:        self.bandwidth: tp.Optional[float] = None
./encodec/model.py:        self.target_bandwidths = target_bandwidths
./encodec/model.py:        self.encoder = encoder
./encodec/model.py:        self.quantizer = quantizer
./encodec/model.py:        self.decoder = decoder
./encodec/model.py:        self.sample_rate = sample_rate
./encodec/model.py:        self.channels = channels
./encodec/model.py:        self.normalize = normalize
./encodec/model.py:        self.segment = segment
./encodec/model.py:        self.overlap = overlap
./encodec/model.py:        self.frame_rate = math.ceil(self.sample_rate / np.prod(self.encoder.ratios))
./encodec/model.py:        self.name = name
./encodec/model.py:        self.bits_per_codebook = int(math.log2(self.quantizer.bins))
./encodec/model.py:        assert 2 ** self.bits_per_codebook == self.quantizer.bins, \
./encodec/model.py:            "quantizer bins must be a power of 2."
./encodec/model.py:    @property
./encodec/model.py:    def segment_length(self) -> tp.Optional[int]:
./encodec/model.py:        if self.segment is None:
./encodec/model.py:            return None
./encodec/model.py:        return int(self.segment * self.sample_rate)
./encodec/model.py:    @property
./encodec/model.py:    def segment_stride(self) -> tp.Optional[int]:
./encodec/model.py:        segment_length = self.segment_length
./encodec/model.py:        if segment_length is None:
./encodec/model.py:            return None
./encodec/model.py:        return max(1, int((1 - self.overlap) * segment_length))
./encodec/model.py:    def encode(self, x: torch.Tensor) -> tp.List[EncodedFrame]:
./encodec/model.py:        """Given a tensor `x`, returns a list of frames containing
./encodec/model.py:        the discrete encoded codes for `x`, along with rescaling factors
./encodec/model.py:        for each segment, when `self.normalize` is True.
./encodec/model.py:        Each frames is a tuple `(codebook, scale)`, with `codebook` of
./encodec/model.py:        shape `[B, K, T]`, with `K` the number of codebooks.
./encodec/model.py:        """
./encodec/model.py:        assert x.dim() == 3
./encodec/model.py:        _, channels, length = x.shape
./encodec/model.py:        assert channels > 0 and channels <= 2
./encodec/model.py:        segment_length = self.segment_length
./encodec/model.py:        if segment_length is None:
./encodec/model.py:            segment_length = length
./encodec/model.py:            stride = length
./encodec/model.py:        else:
./encodec/model.py:            stride = self.segment_stride  # type: ignore
./encodec/model.py:            assert stride is not None
./encodec/model.py:        encoded_frames: tp.List[EncodedFrame] = []
./encodec/model.py:        for offset in range(0, length, stride):
./encodec/model.py:            frame = x[:, :, offset: offset + segment_length]
./encodec/model.py:            encoded_frames.append(self._encode_frame(frame))
./encodec/model.py:        return encoded_frames
./encodec/model.py:    def _encode_frame(self, x: torch.Tensor) -> EncodedFrame:
./encodec/model.py:        length = x.shape[-1]
./encodec/model.py:        duration = length / self.sample_rate
./encodec/model.py:        assert self.segment is None or duration <= 1e-5 + self.segment
./encodec/model.py:        if self.normalize:
./encodec/model.py:            mono = x.mean(dim=1, keepdim=True)
./encodec/model.py:            volume = mono.pow(2).mean(dim=2, keepdim=True).sqrt()
./encodec/model.py:            scale = 1e-8 + volume
./encodec/model.py:            x = x / scale
./encodec/model.py:            scale = scale.view(-1, 1)
./encodec/model.py:        else:
./encodec/model.py:            scale = None
./encodec/model.py:        emb = self.encoder(x)
./encodec/model.py:        codes = self.quantizer.encode(emb, self.frame_rate, self.bandwidth)
./encodec/model.py:        codes = codes.transpose(0, 1)
./encodec/model.py:        # codes is [B, K, T], with T frames, K nb of codebooks.
./encodec/model.py:        return codes, scale
./encodec/model.py:    def decode(self, encoded_frames: tp.List[EncodedFrame]) -> torch.Tensor:
./encodec/model.py:        """Decode the given frames into a waveform.
./encodec/model.py:        Note that the output might be a bit bigger than the input. In that case,
./encodec/model.py:        any extra steps at the end can be trimmed.
./encodec/model.py:        """
./encodec/model.py:        segment_length = self.segment_length
./encodec/model.py:        if segment_length is None:
./encodec/model.py:            assert len(encoded_frames) == 1
./encodec/model.py:            return self._decode_frame(encoded_frames[0])
./encodec/model.py:        frames = [self._decode_frame(frame) for frame in encoded_frames]
./encodec/model.py:        return _linear_overlap_add(frames, self.segment_stride or 1)
./encodec/model.py:    def _decode_frame(self, encoded_frame: EncodedFrame) -> torch.Tensor:
./encodec/model.py:        codes, scale = encoded_frame
./encodec/model.py:        codes = codes.transpose(0, 1)
./encodec/model.py:        emb = self.quantizer.decode(codes)
./encodec/model.py:        out = self.decoder(emb)
./encodec/model.py:        if scale is not None:
./encodec/model.py:            out = out * scale.view(-1, 1, 1)
./encodec/model.py:        return out
./encodec/model.py:    def forward(self, x: torch.Tensor) -> torch.Tensor:
./encodec/model.py:        frames = self.encode(x)
./encodec/model.py:        return self.decode(frames)[:, :, :x.shape[-1]]
./encodec/model.py:    def set_target_bandwidth(self, bandwidth: float):
./encodec/model.py:        if bandwidth not in self.target_bandwidths:
./encodec/model.py:            raise ValueError(f"This model doesn't support the bandwidth {bandwidth}. "
./encodec/model.py:                             f"Select one of {self.target_bandwidths}.")
./encodec/model.py:        self.bandwidth = bandwidth
./encodec/model.py:    def get_lm_model(self) -> LMModel:
./encodec/model.py:        """Return the associated LM model to improve the compression rate.
./encodec/model.py:        """
./encodec/model.py:        device = next(self.parameters()).device
./encodec/model.py:        lm = LMModel(self.quantizer.n_q, self.quantizer.bins, num_layers=5, dim=200,
./encodec/model.py:                     past_context=int(3.5 * self.frame_rate)).to(device)
./encodec/model.py:        checkpoints = {
./encodec/model.py:            'encodec_24khz': 'encodec_lm_24khz-1608e3c0.th',
./encodec/model.py:            'encodec_48khz': 'encodec_lm_48khz-7add9fc3.th',
./encodec/model.py:        }
./encodec/model.py:        try:
./encodec/model.py:            checkpoint_name = checkpoints[self.name]
./encodec/model.py:        except KeyError:
./encodec/model.py:            raise RuntimeError("No LM pre-trained for the current Encodec model.")
./encodec/model.py:        url = _get_checkpoint_url(ROOT_URL, checkpoint_name)
./encodec/model.py:        state = torch.hub.load_state_dict_from_url(
./encodec/model.py:            url, map_location='cpu', check_hash=True)  # type: ignore
./encodec/model.py:        lm.load_state_dict(state)
./encodec/model.py:        lm.eval()
./encodec/model.py:        return lm
./encodec/model.py:    @staticmethod
./encodec/model.py:    def _get_model(target_bandwidths: tp.List[float],
./encodec/model.py:                   sample_rate: int = 24_000,
./encodec/model.py:                   channels: int = 1,
./encodec/model.py:                   causal: bool = True,
./encodec/model.py:                   model_norm: str = 'weight_norm',
./encodec/model.py:                   audio_normalize: bool = False,
./encodec/model.py:                   segment: tp.Optional[float] = None,
./encodec/model.py:                   name: str = 'unset'):
./encodec/model.py:        encoder = m.SEANetEncoder(channels=channels, norm=model_norm, causal=causal)
./encodec/model.py:        decoder = m.SEANetDecoder(channels=channels, norm=model_norm, causal=causal)
./encodec/model.py:        n_q = int(1000 * target_bandwidths[-1] // (math.ceil(sample_rate / encoder.hop_length) * 10))
./encodec/model.py:        quantizer = qt.ResidualVectorQuantizer(
./encodec/model.py:            dimension=encoder.dimension,
./encodec/model.py:            n_q=n_q,
./encodec/model.py:            bins=1024,
./encodec/model.py:        )
./encodec/model.py:        model = EncodecModel(
./encodec/model.py:            encoder,
./encodec/model.py:            decoder,
./encodec/model.py:            quantizer,
./encodec/model.py:            target_bandwidths,
./encodec/model.py:            sample_rate,
./encodec/model.py:            channels,
./encodec/model.py:            normalize=audio_normalize,
./encodec/model.py:            segment=segment,
./encodec/model.py:            name=name,
./encodec/model.py:        )
./encodec/model.py:        return model
./encodec/model.py:    @staticmethod
./encodec/model.py:    def _get_pretrained(checkpoint_name: str, repository: tp.Optional[Path] = None):
./encodec/model.py:        if repository is not None:
./encodec/model.py:            if not repository.is_dir():
./encodec/model.py:                raise ValueError(f"{repository} must exist and be a directory.")
./encodec/model.py:            file = repository / checkpoint_name
./encodec/model.py:            checksum = file.stem.split('-')[1]
./encodec/model.py:            _check_checksum(file, checksum)
./encodec/model.py:            return torch.load(file)
./encodec/model.py:        else:
./encodec/model.py:            url = _get_checkpoint_url(ROOT_URL, checkpoint_name)
./encodec/model.py:            return torch.hub.load_state_dict_from_url(url, map_location='cpu', check_hash=True)  # type:ignore
./encodec/model.py:    @staticmethod
./encodec/model.py:    def encodec_model_24khz(pretrained: bool = True, repository: tp.Optional[Path] = None):
./encodec/model.py:        """Return the pretrained causal 24khz model.
./encodec/model.py:        """
./encodec/model.py:        if repository:
./encodec/model.py:            assert pretrained
./encodec/model.py:        target_bandwidths = [1.5, 3., 6, 12., 24.]
./encodec/model.py:        checkpoint_name = 'encodec_24khz-d7cc33bc.th'
./encodec/model.py:        sample_rate = 24_000
./encodec/model.py:        channels = 1
./encodec/model.py:        model = EncodecModel._get_model(
./encodec/model.py:            target_bandwidths, sample_rate, channels,
./encodec/model.py:            causal=True, model_norm='weight_norm', audio_normalize=False,
./encodec/model.py:            name='encodec_24khz' if pretrained else 'unset')
./encodec/model.py:        if pretrained:
./encodec/model.py:            state_dict = EncodecModel._get_pretrained(checkpoint_name, repository)
./encodec/model.py:            model.load_state_dict(state_dict)
./encodec/model.py:        model.eval()
./encodec/model.py:        return model
./encodec/model.py:    @staticmethod
./encodec/model.py:    def encodec_model_48khz(pretrained: bool = True, repository: tp.Optional[Path] = None):
./encodec/model.py:        """Return the pretrained 48khz model.
./encodec/model.py:        """
./encodec/model.py:        if repository:
./encodec/model.py:            assert pretrained
./encodec/model.py:        target_bandwidths = [3., 6., 12., 24.]
./encodec/model.py:        checkpoint_name = 'encodec_48khz-7e698e3e.th'
./encodec/model.py:        sample_rate = 48_000
./encodec/model.py:        channels = 2
./encodec/model.py:        model = EncodecModel._get_model(
./encodec/model.py:            target_bandwidths, sample_rate, channels,
./encodec/model.py:            causal=False, model_norm='time_group_norm', audio_normalize=True,
./encodec/model.py:            segment=1., name='encodec_48khz' if pretrained else 'unset')
./encodec/model.py:        if pretrained:
./encodec/model.py:            state_dict = EncodecModel._get_pretrained(checkpoint_name, repository)
./encodec/model.py:            model.load_state_dict(state_dict)
./encodec/model.py:        model.eval()
./encodec/model.py:        return model
./encodec/model.py:def test():
./encodec/model.py:    from itertools import product
./encodec/model.py:    import torchaudio
./encodec/model.py:    bandwidths = [3, 6, 12, 24]
./encodec/model.py:    models = {
./encodec/model.py:        'encodec_24khz': EncodecModel.encodec_model_24khz,
./encodec/model.py:        'encodec_48khz': EncodecModel.encodec_model_48khz
./encodec/model.py:    }
./encodec/model.py:    for model_name, bw in product(models.keys(), bandwidths):
./encodec/model.py:        model = models[model_name]()
./encodec/model.py:        model.set_target_bandwidth(bw)
./encodec/model.py:        audio_suffix = model_name.split('_')[1][:3]
./encodec/model.py:        wav, sr = torchaudio.load(f"test_{audio_suffix}.wav")
./encodec/model.py:        wav = wav[:, :model.sample_rate * 2]
./encodec/model.py:        wav_in = wav.unsqueeze(0)
./encodec/model.py:        wav_dec = model(wav_in)[0]
./encodec/model.py:        assert wav.shape == wav_dec.shape, (wav.shape, wav_dec.shape)
./encodec/model.py:if __name__ == '__main__':
./encodec/model.py:    test()
./encodec/quantization/core_vq.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/quantization/core_vq.py:# All rights reserved.
./encodec/quantization/core_vq.py:#
./encodec/quantization/core_vq.py:# This source code is licensed under the license found in the
./encodec/quantization/core_vq.py:# LICENSE file in the root directory of this source tree.
./encodec/quantization/core_vq.py:#
./encodec/quantization/core_vq.py:# This implementation is inspired from
./encodec/quantization/core_vq.py:# https://github.com/lucidrains/vector-quantize-pytorch
./encodec/quantization/core_vq.py:# which is released under MIT License. Hereafter, the original license:
./encodec/quantization/core_vq.py:# MIT License
./encodec/quantization/core_vq.py:#
./encodec/quantization/core_vq.py:# Copyright (c) 2020 Phil Wang
./encodec/quantization/core_vq.py:#
./encodec/quantization/core_vq.py:# Permission is hereby granted, free of charge, to any person obtaining a copy
./encodec/quantization/core_vq.py:# of this software and associated documentation files (the "Software"), to deal
./encodec/quantization/core_vq.py:# in the Software without restriction, including without limitation the rights
./encodec/quantization/core_vq.py:# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
./encodec/quantization/core_vq.py:# copies of the Software, and to permit persons to whom the Software is
./encodec/quantization/core_vq.py:# furnished to do so, subject to the following conditions:
./encodec/quantization/core_vq.py:#
./encodec/quantization/core_vq.py:# The above copyright notice and this permission notice shall be included in all
./encodec/quantization/core_vq.py:# copies or substantial portions of the Software.
./encodec/quantization/core_vq.py:#
./encodec/quantization/core_vq.py:# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
./encodec/quantization/core_vq.py:# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
./encodec/quantization/core_vq.py:# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
./encodec/quantization/core_vq.py:# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
./encodec/quantization/core_vq.py:# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
./encodec/quantization/core_vq.py:# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
./encodec/quantization/core_vq.py:# SOFTWARE.
./encodec/quantization/core_vq.py:"""Core vector quantization implementation."""
./encodec/quantization/core_vq.py:import typing as tp
./encodec/quantization/core_vq.py:import warnings
./encodec/quantization/core_vq.py:from einops import rearrange, repeat
./encodec/quantization/core_vq.py:import torch
./encodec/quantization/core_vq.py:from torch import nn
./encodec/quantization/core_vq.py:import torch.nn.functional as F
./encodec/quantization/core_vq.py:from .. import distrib
./encodec/quantization/core_vq.py:def default(val: tp.Any, d: tp.Any) -> tp.Any:
./encodec/quantization/core_vq.py:    return val if val is not None else d
./encodec/quantization/core_vq.py:def ema_inplace(moving_avg, new, decay: float):
./encodec/quantization/core_vq.py:    moving_avg.data.mul_(decay).add_(new, alpha=(1 - decay))
./encodec/quantization/core_vq.py:def laplace_smoothing(x, n_categories: int, epsilon: float = 1e-5):
./encodec/quantization/core_vq.py:    return (x + epsilon) / (x.sum() + n_categories * epsilon)
./encodec/quantization/core_vq.py:def uniform_init(*shape: int):
./encodec/quantization/core_vq.py:    t = torch.empty(shape)
./encodec/quantization/core_vq.py:    nn.init.kaiming_uniform_(t)
./encodec/quantization/core_vq.py:    return t
./encodec/quantization/core_vq.py:def sample_vectors(samples, num: int):
./encodec/quantization/core_vq.py:    num_samples, device = samples.shape[0], samples.device
./encodec/quantization/core_vq.py:    if num_samples >= num:
./encodec/quantization/core_vq.py:        indices = torch.randperm(num_samples, device=device)[:num]
./encodec/quantization/core_vq.py:    else:
./encodec/quantization/core_vq.py:        indices = torch.randint(0, num_samples, (num,), device=device)
./encodec/quantization/core_vq.py:    return samples[indices]
./encodec/quantization/core_vq.py:def kmeans(samples, num_clusters: int, num_iters: int = 10):
./encodec/quantization/core_vq.py:    dim, dtype = samples.shape[-1], samples.dtype
./encodec/quantization/core_vq.py:    means = sample_vectors(samples, num_clusters)
./encodec/quantization/core_vq.py:    for _ in range(num_iters):
./encodec/quantization/core_vq.py:        diffs = rearrange(samples, "n d -> n () d") - rearrange(
./encodec/quantization/core_vq.py:            means, "c d -> () c d"
./encodec/quantization/core_vq.py:        )
./encodec/quantization/core_vq.py:        dists = -(diffs ** 2).sum(dim=-1)
./encodec/quantization/core_vq.py:        buckets = dists.max(dim=-1).indices
./encodec/quantization/core_vq.py:        bins = torch.bincount(buckets, minlength=num_clusters)
./encodec/quantization/core_vq.py:        zero_mask = bins == 0
./encodec/quantization/core_vq.py:        bins_min_clamped = bins.masked_fill(zero_mask, 1)
./encodec/quantization/core_vq.py:        new_means = buckets.new_zeros(num_clusters, dim, dtype=dtype)
./encodec/quantization/core_vq.py:        new_means.scatter_add_(0, repeat(buckets, "n -> n d", d=dim), samples)
./encodec/quantization/core_vq.py:        new_means = new_means / bins_min_clamped[..., None]
./encodec/quantization/core_vq.py:        means = torch.where(zero_mask[..., None], means, new_means)
./encodec/quantization/core_vq.py:    return means, bins
./encodec/quantization/core_vq.py:class EuclideanCodebook(nn.Module):
./encodec/quantization/core_vq.py:    """Codebook with Euclidean distance.
./encodec/quantization/core_vq.py:    Args:
./encodec/quantization/core_vq.py:        dim (int): Dimension.
./encodec/quantization/core_vq.py:        codebook_size (int): Codebook size.
./encodec/quantization/core_vq.py:        kmeans_init (bool): Whether to use k-means to initialize the codebooks.
./encodec/quantization/core_vq.py:            If set to true, run the k-means algorithm on the first training batch and use
./encodec/quantization/core_vq.py:            the learned centroids as initialization.
./encodec/quantization/core_vq.py:        kmeans_iters (int): Number of iterations used for k-means algorithm at initialization.
./encodec/quantization/core_vq.py:        decay (float): Decay for exponential moving average over the codebooks.
./encodec/quantization/core_vq.py:        epsilon (float): Epsilon value for numerical stability.
./encodec/quantization/core_vq.py:        threshold_ema_dead_code (int): Threshold for dead code expiration. Replace any codes
./encodec/quantization/core_vq.py:            that have an exponential moving average cluster size less than the specified threshold with
./encodec/quantization/core_vq.py:            randomly selected vector from the current batch.
./encodec/quantization/core_vq.py:    """
./encodec/quantization/core_vq.py:    def __init__(
./encodec/quantization/core_vq.py:        self,
./encodec/quantization/core_vq.py:        dim: int,
./encodec/quantization/core_vq.py:        codebook_size: int,
./encodec/quantization/core_vq.py:        kmeans_init: int = False,
./encodec/quantization/core_vq.py:        kmeans_iters: int = 10,
./encodec/quantization/core_vq.py:        decay: float = 0.99,
./encodec/quantization/core_vq.py:        epsilon: float = 1e-5,
./encodec/quantization/core_vq.py:        threshold_ema_dead_code: int = 2,
./encodec/quantization/core_vq.py:    ):
./encodec/quantization/core_vq.py:        super().__init__()
./encodec/quantization/core_vq.py:        self.decay = decay
./encodec/quantization/core_vq.py:        init_fn: tp.Union[tp.Callable[..., torch.Tensor], tp.Any] = uniform_init if not kmeans_init else torch.zeros
./encodec/quantization/core_vq.py:        embed = init_fn(codebook_size, dim)
./encodec/quantization/core_vq.py:        self.codebook_size = codebook_size
./encodec/quantization/core_vq.py:        self.kmeans_iters = kmeans_iters
./encodec/quantization/core_vq.py:        self.epsilon = epsilon
./encodec/quantization/core_vq.py:        self.threshold_ema_dead_code = threshold_ema_dead_code
./encodec/quantization/core_vq.py:        self.register_buffer("inited", torch.Tensor([not kmeans_init]))
./encodec/quantization/core_vq.py:        self.register_buffer("cluster_size", torch.zeros(codebook_size))
./encodec/quantization/core_vq.py:        self.register_buffer("embed", embed)
./encodec/quantization/core_vq.py:        self.register_buffer("embed_avg", embed.clone())
./encodec/quantization/core_vq.py:    @torch.jit.ignore
./encodec/quantization/core_vq.py:    def init_embed_(self, data):
./encodec/quantization/core_vq.py:        if self.inited:
./encodec/quantization/core_vq.py:            return
./encodec/quantization/core_vq.py:        embed, cluster_size = kmeans(data, self.codebook_size, self.kmeans_iters)
./encodec/quantization/core_vq.py:        self.embed.data.copy_(embed)
./encodec/quantization/core_vq.py:        self.embed_avg.data.copy_(embed.clone())
./encodec/quantization/core_vq.py:        self.cluster_size.data.copy_(cluster_size)
./encodec/quantization/core_vq.py:        self.inited.data.copy_(torch.Tensor([True]))
./encodec/quantization/core_vq.py:        # Make sure all buffers across workers are in sync after initialization
./encodec/quantization/core_vq.py:        distrib.broadcast_tensors(self.buffers())
./encodec/quantization/core_vq.py:    def replace_(self, samples, mask):
./encodec/quantization/core_vq.py:        modified_codebook = torch.where(
./encodec/quantization/core_vq.py:            mask[..., None], sample_vectors(samples, self.codebook_size), self.embed
./encodec/quantization/core_vq.py:        )
./encodec/quantization/core_vq.py:        self.embed.data.copy_(modified_codebook)
./encodec/quantization/core_vq.py:    def expire_codes_(self, batch_samples):
./encodec/quantization/core_vq.py:        if self.threshold_ema_dead_code == 0:
./encodec/quantization/core_vq.py:            return
./encodec/quantization/core_vq.py:        expired_codes = self.cluster_size < self.threshold_ema_dead_code
./encodec/quantization/core_vq.py:        if not torch.any(expired_codes):
./encodec/quantization/core_vq.py:            return
./encodec/quantization/core_vq.py:        batch_samples = rearrange(batch_samples, "... d -> (...) d")
./encodec/quantization/core_vq.py:        self.replace_(batch_samples, mask=expired_codes)
./encodec/quantization/core_vq.py:        distrib.broadcast_tensors(self.buffers())
./encodec/quantization/core_vq.py:    def preprocess(self, x):
./encodec/quantization/core_vq.py:        x = rearrange(x, "... d -> (...) d")
./encodec/quantization/core_vq.py:        return x
./encodec/quantization/core_vq.py:    def quantize(self, x):
./encodec/quantization/core_vq.py:        embed = self.embed.t()
./encodec/quantization/core_vq.py:        dist = -(
./encodec/quantization/core_vq.py:            x.pow(2).sum(1, keepdim=True)
./encodec/quantization/core_vq.py:            - 2 * x @ embed
./encodec/quantization/core_vq.py:            + embed.pow(2).sum(0, keepdim=True)
./encodec/quantization/core_vq.py:        )
./encodec/quantization/core_vq.py:        embed_ind = dist.max(dim=-1).indices
./encodec/quantization/core_vq.py:        return embed_ind
./encodec/quantization/core_vq.py:    def postprocess_emb(self, embed_ind, shape):
./encodec/quantization/core_vq.py:        return embed_ind.view(*shape[:-1])
./encodec/quantization/core_vq.py:    def dequantize(self, embed_ind):
./encodec/quantization/core_vq.py:        quantize = F.embedding(embed_ind, self.embed)
./encodec/quantization/core_vq.py:        return quantize
./encodec/quantization/core_vq.py:    def encode(self, x):
./encodec/quantization/core_vq.py:        shape = x.shape
./encodec/quantization/core_vq.py:        # pre-process
./encodec/quantization/core_vq.py:        x = self.preprocess(x)
./encodec/quantization/core_vq.py:        # quantize
./encodec/quantization/core_vq.py:        embed_ind = self.quantize(x)
./encodec/quantization/core_vq.py:        # post-process
./encodec/quantization/core_vq.py:        embed_ind = self.postprocess_emb(embed_ind, shape)
./encodec/quantization/core_vq.py:        return embed_ind
./encodec/quantization/core_vq.py:    def decode(self, embed_ind):
./encodec/quantization/core_vq.py:        quantize = self.dequantize(embed_ind)
./encodec/quantization/core_vq.py:        return quantize
./encodec/quantization/core_vq.py:    def forward(self, x):
./encodec/quantization/core_vq.py:        shape, dtype = x.shape, x.dtype
./encodec/quantization/core_vq.py:        x = self.preprocess(x)
./encodec/quantization/core_vq.py:        self.init_embed_(x)
./encodec/quantization/core_vq.py:        embed_ind = self.quantize(x)
./encodec/quantization/core_vq.py:        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)
./encodec/quantization/core_vq.py:        embed_ind = self.postprocess_emb(embed_ind, shape)
./encodec/quantization/core_vq.py:        quantize = self.dequantize(embed_ind)
./encodec/quantization/core_vq.py:        if self.training:
./encodec/quantization/core_vq.py:            # We do the expiry of code at that point as buffers are in sync
./encodec/quantization/core_vq.py:            # and all the workers will take the same decision.
./encodec/quantization/core_vq.py:            self.expire_codes_(x)
./encodec/quantization/core_vq.py:            ema_inplace(self.cluster_size, embed_onehot.sum(0), self.decay)
./encodec/quantization/core_vq.py:            embed_sum = x.t() @ embed_onehot
./encodec/quantization/core_vq.py:            ema_inplace(self.embed_avg, embed_sum.t(), self.decay)
./encodec/quantization/core_vq.py:            cluster_size = (
./encodec/quantization/core_vq.py:                laplace_smoothing(self.cluster_size, self.codebook_size, self.epsilon)
./encodec/quantization/core_vq.py:                * self.cluster_size.sum()
./encodec/quantization/core_vq.py:            )
./encodec/quantization/core_vq.py:            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)
./encodec/quantization/core_vq.py:            self.embed.data.copy_(embed_normalized)
./encodec/quantization/core_vq.py:        return quantize, embed_ind
./encodec/quantization/core_vq.py:class VectorQuantization(nn.Module):
./encodec/quantization/core_vq.py:    """Vector quantization implementation.
./encodec/quantization/core_vq.py:    Currently supports only euclidean distance.
./encodec/quantization/core_vq.py:    Args:
./encodec/quantization/core_vq.py:        dim (int): Dimension
./encodec/quantization/core_vq.py:        codebook_size (int): Codebook size
./encodec/quantization/core_vq.py:        codebook_dim (int): Codebook dimension. If not defined, uses the specified dimension in dim.
./encodec/quantization/core_vq.py:        decay (float): Decay for exponential moving average over the codebooks.
./encodec/quantization/core_vq.py:        epsilon (float): Epsilon value for numerical stability.
./encodec/quantization/core_vq.py:        kmeans_init (bool): Whether to use kmeans to initialize the codebooks.
./encodec/quantization/core_vq.py:        kmeans_iters (int): Number of iterations used for kmeans initialization.
./encodec/quantization/core_vq.py:        threshold_ema_dead_code (int): Threshold for dead code expiration. Replace any codes
./encodec/quantization/core_vq.py:            that have an exponential moving average cluster size less than the specified threshold with
./encodec/quantization/core_vq.py:            randomly selected vector from the current batch.
./encodec/quantization/core_vq.py:        commitment_weight (float): Weight for commitment loss.
./encodec/quantization/core_vq.py:    """
./encodec/quantization/core_vq.py:    def __init__(
./encodec/quantization/core_vq.py:        self,
./encodec/quantization/core_vq.py:        dim: int,
./encodec/quantization/core_vq.py:        codebook_size: int,
./encodec/quantization/core_vq.py:        codebook_dim: tp.Optional[int] = None,
./encodec/quantization/core_vq.py:        decay: float = 0.99,
./encodec/quantization/core_vq.py:        epsilon: float = 1e-5,
./encodec/quantization/core_vq.py:        kmeans_init: bool = True,
./encodec/quantization/core_vq.py:        kmeans_iters: int = 50,
./encodec/quantization/core_vq.py:        threshold_ema_dead_code: int = 2,
./encodec/quantization/core_vq.py:        commitment_weight: float = 1.,
./encodec/quantization/core_vq.py:    ):
./encodec/quantization/core_vq.py:        super().__init__()
./encodec/quantization/core_vq.py:        _codebook_dim: int = default(codebook_dim, dim)
./encodec/quantization/core_vq.py:        requires_projection = _codebook_dim != dim
./encodec/quantization/core_vq.py:        self.project_in = (nn.Linear(dim, _codebook_dim) if requires_projection else nn.Identity())
./encodec/quantization/core_vq.py:        self.project_out = (nn.Linear(_codebook_dim, dim) if requires_projection else nn.Identity())
./encodec/quantization/core_vq.py:        self.epsilon = epsilon
./encodec/quantization/core_vq.py:        self.commitment_weight = commitment_weight
./encodec/quantization/core_vq.py:        self._codebook = EuclideanCodebook(dim=_codebook_dim, codebook_size=codebook_size,
./encodec/quantization/core_vq.py:                                           kmeans_init=kmeans_init, kmeans_iters=kmeans_iters,
./encodec/quantization/core_vq.py:                                           decay=decay, epsilon=epsilon,
./encodec/quantization/core_vq.py:                                           threshold_ema_dead_code=threshold_ema_dead_code)
./encodec/quantization/core_vq.py:        self.codebook_size = codebook_size
./encodec/quantization/core_vq.py:    @property
./encodec/quantization/core_vq.py:    def codebook(self):
./encodec/quantization/core_vq.py:        return self._codebook.embed
./encodec/quantization/core_vq.py:    def encode(self, x):
./encodec/quantization/core_vq.py:        x = rearrange(x, "b d n -> b n d")
./encodec/quantization/core_vq.py:        x = self.project_in(x)
./encodec/quantization/core_vq.py:        embed_in = self._codebook.encode(x)
./encodec/quantization/core_vq.py:        return embed_in
./encodec/quantization/core_vq.py:    def decode(self, embed_ind):
./encodec/quantization/core_vq.py:        quantize = self._codebook.decode(embed_ind)
./encodec/quantization/core_vq.py:        quantize = self.project_out(quantize)
./encodec/quantization/core_vq.py:        quantize = rearrange(quantize, "b n d -> b d n")
./encodec/quantization/core_vq.py:        return quantize
./encodec/quantization/core_vq.py:    def forward(self, x):
./encodec/quantization/core_vq.py:        device = x.device
./encodec/quantization/core_vq.py:        x = rearrange(x, "b d n -> b n d")
./encodec/quantization/core_vq.py:        x = self.project_in(x)
./encodec/quantization/core_vq.py:        quantize, embed_ind = self._codebook(x)
./encodec/quantization/core_vq.py:        if self.training:
./encodec/quantization/core_vq.py:            quantize = x + (quantize - x).detach()
./encodec/quantization/core_vq.py:        loss = torch.tensor([0.0], device=device, requires_grad=self.training)
./encodec/quantization/core_vq.py:        if self.training:
./encodec/quantization/core_vq.py:            warnings.warn('When using RVQ in training model, first check '
./encodec/quantization/core_vq.py:                          'https://github.com/facebookresearch/encodec/issues/25 . '
./encodec/quantization/core_vq.py:                          'The bug wasn\'t fixed here for reproducibility.')
./encodec/quantization/core_vq.py:            if self.commitment_weight > 0:
./encodec/quantization/core_vq.py:                commit_loss = F.mse_loss(quantize.detach(), x)
./encodec/quantization/core_vq.py:                loss = loss + commit_loss * self.commitment_weight
./encodec/quantization/core_vq.py:        quantize = self.project_out(quantize)
./encodec/quantization/core_vq.py:        quantize = rearrange(quantize, "b n d -> b d n")
./encodec/quantization/core_vq.py:        return quantize, embed_ind, loss
./encodec/quantization/core_vq.py:class ResidualVectorQuantization(nn.Module):
./encodec/quantization/core_vq.py:    """Residual vector quantization implementation.
./encodec/quantization/core_vq.py:    Follows Algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf
./encodec/quantization/core_vq.py:    """
./encodec/quantization/core_vq.py:    def __init__(self, *, num_quantizers, **kwargs):
./encodec/quantization/core_vq.py:        super().__init__()
./encodec/quantization/core_vq.py:        self.layers = nn.ModuleList(
./encodec/quantization/core_vq.py:            [VectorQuantization(**kwargs) for _ in range(num_quantizers)]
./encodec/quantization/core_vq.py:        )
./encodec/quantization/core_vq.py:    def forward(self, x, n_q: tp.Optional[int] = None):
./encodec/quantization/core_vq.py:        quantized_out = 0.0
./encodec/quantization/core_vq.py:        residual = x
./encodec/quantization/core_vq.py:        all_losses = []
./encodec/quantization/core_vq.py:        all_indices = []
./encodec/quantization/core_vq.py:        n_q = n_q or len(self.layers)
./encodec/quantization/core_vq.py:        for layer in self.layers[:n_q]:
./encodec/quantization/core_vq.py:            quantized, indices, loss = layer(residual)
./encodec/quantization/core_vq.py:            residual = residual - quantized
./encodec/quantization/core_vq.py:            quantized_out = quantized_out + quantized
./encodec/quantization/core_vq.py:            all_indices.append(indices)
./encodec/quantization/core_vq.py:            all_losses.append(loss)
./encodec/quantization/core_vq.py:        out_losses, out_indices = map(torch.stack, (all_losses, all_indices))
./encodec/quantization/core_vq.py:        return quantized_out, out_indices, out_losses
./encodec/quantization/core_vq.py:    def encode(self, x: torch.Tensor, n_q: tp.Optional[int] = None) -> torch.Tensor:
./encodec/quantization/core_vq.py:        residual = x
./encodec/quantization/core_vq.py:        all_indices = []
./encodec/quantization/core_vq.py:        n_q = n_q or len(self.layers)
./encodec/quantization/core_vq.py:        for layer in self.layers[:n_q]:
./encodec/quantization/core_vq.py:            indices = layer.encode(residual)
./encodec/quantization/core_vq.py:            quantized = layer.decode(indices)
./encodec/quantization/core_vq.py:            residual = residual - quantized
./encodec/quantization/core_vq.py:            all_indices.append(indices)
./encodec/quantization/core_vq.py:        out_indices = torch.stack(all_indices)
./encodec/quantization/core_vq.py:        return out_indices
./encodec/quantization/core_vq.py:    def decode(self, q_indices: torch.Tensor) -> torch.Tensor:
./encodec/quantization/core_vq.py:        quantized_out = torch.tensor(0.0, device=q_indices.device)
./encodec/quantization/core_vq.py:        for i, indices in enumerate(q_indices):
./encodec/quantization/core_vq.py:            layer = self.layers[i]
./encodec/quantization/core_vq.py:            quantized = layer.decode(indices)
./encodec/quantization/core_vq.py:            quantized_out = quantized_out + quantized
./encodec/quantization/core_vq.py:        return quantized_out
./encodec/quantization/__init__.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/quantization/__init__.py:# All rights reserved.
./encodec/quantization/__init__.py:#
./encodec/quantization/__init__.py:# This source code is licensed under the license found in the
./encodec/quantization/__init__.py:# LICENSE file in the root directory of this source tree.
./encodec/quantization/__init__.py:# flake8: noqa
./encodec/quantization/__init__.py:from .vq import QuantizedResult, ResidualVectorQuantizer
./encodec/quantization/vq.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/quantization/vq.py:# All rights reserved.
./encodec/quantization/vq.py:#
./encodec/quantization/vq.py:# This source code is licensed under the license found in the
./encodec/quantization/vq.py:# LICENSE file in the root directory of this source tree.
./encodec/quantization/vq.py:"""Residual vector quantizer implementation."""
./encodec/quantization/vq.py:from dataclasses import dataclass, field
./encodec/quantization/vq.py:import math
./encodec/quantization/vq.py:import typing as tp
./encodec/quantization/vq.py:import torch
./encodec/quantization/vq.py:from torch import nn
./encodec/quantization/vq.py:from .core_vq import ResidualVectorQuantization
./encodec/quantization/vq.py:@dataclass
./encodec/quantization/vq.py:class QuantizedResult:
./encodec/quantization/vq.py:    quantized: torch.Tensor
./encodec/quantization/vq.py:    codes: torch.Tensor
./encodec/quantization/vq.py:    bandwidth: torch.Tensor  # bandwidth in kb/s used, per batch item.
./encodec/quantization/vq.py:    penalty: tp.Optional[torch.Tensor] = None
./encodec/quantization/vq.py:    metrics: dict = field(default_factory=dict)
./encodec/quantization/vq.py:class ResidualVectorQuantizer(nn.Module):
./encodec/quantization/vq.py:    """Residual Vector Quantizer.
./encodec/quantization/vq.py:    Args:
./encodec/quantization/vq.py:        dimension (int): Dimension of the codebooks.
./encodec/quantization/vq.py:        n_q (int): Number of residual vector quantizers used.
./encodec/quantization/vq.py:        bins (int): Codebook size.
./encodec/quantization/vq.py:        decay (float): Decay for exponential moving average over the codebooks.
./encodec/quantization/vq.py:        kmeans_init (bool): Whether to use kmeans to initialize the codebooks.
./encodec/quantization/vq.py:        kmeans_iters (int): Number of iterations used for kmeans initialization.
./encodec/quantization/vq.py:        threshold_ema_dead_code (int): Threshold for dead code expiration. Replace any codes
./encodec/quantization/vq.py:            that have an exponential moving average cluster size less than the specified threshold with
./encodec/quantization/vq.py:            randomly selected vector from the current batch.
./encodec/quantization/vq.py:    """
./encodec/quantization/vq.py:    def __init__(
./encodec/quantization/vq.py:        self,
./encodec/quantization/vq.py:        dimension: int = 256,
./encodec/quantization/vq.py:        n_q: int = 8,
./encodec/quantization/vq.py:        bins: int = 1024,
./encodec/quantization/vq.py:        decay: float = 0.99,
./encodec/quantization/vq.py:        kmeans_init: bool = True,
./encodec/quantization/vq.py:        kmeans_iters: int = 50,
./encodec/quantization/vq.py:        threshold_ema_dead_code: int = 2,
./encodec/quantization/vq.py:    ):
./encodec/quantization/vq.py:        super().__init__()
./encodec/quantization/vq.py:        self.n_q = n_q
./encodec/quantization/vq.py:        self.dimension = dimension
./encodec/quantization/vq.py:        self.bins = bins
./encodec/quantization/vq.py:        self.decay = decay
./encodec/quantization/vq.py:        self.kmeans_init = kmeans_init
./encodec/quantization/vq.py:        self.kmeans_iters = kmeans_iters
./encodec/quantization/vq.py:        self.threshold_ema_dead_code = threshold_ema_dead_code
./encodec/quantization/vq.py:        self.vq = ResidualVectorQuantization(
./encodec/quantization/vq.py:            dim=self.dimension,
./encodec/quantization/vq.py:            codebook_size=self.bins,
./encodec/quantization/vq.py:            num_quantizers=self.n_q,
./encodec/quantization/vq.py:            decay=self.decay,
./encodec/quantization/vq.py:            kmeans_init=self.kmeans_init,
./encodec/quantization/vq.py:            kmeans_iters=self.kmeans_iters,
./encodec/quantization/vq.py:            threshold_ema_dead_code=self.threshold_ema_dead_code,
./encodec/quantization/vq.py:        )
./encodec/quantization/vq.py:    def forward(self, x: torch.Tensor, frame_rate: int, bandwidth: tp.Optional[float] = None) -> QuantizedResult:
./encodec/quantization/vq.py:        """Residual vector quantization on the given input tensor.
./encodec/quantization/vq.py:        Args:
./encodec/quantization/vq.py:            x (torch.Tensor): Input tensor.
./encodec/quantization/vq.py:            frame_rate (int): Sample rate of the input tensor.
./encodec/quantization/vq.py:            bandwidth (float): Target bandwidth.
./encodec/quantization/vq.py:        Returns:
./encodec/quantization/vq.py:            QuantizedResult:
./encodec/quantization/vq.py:                The quantized (or approximately quantized) representation with
./encodec/quantization/vq.py:                the associated bandwidth and any penalty term for the loss.
./encodec/quantization/vq.py:        """
./encodec/quantization/vq.py:        bw_per_q = self.get_bandwidth_per_quantizer(frame_rate)
./encodec/quantization/vq.py:        n_q = self.get_num_quantizers_for_bandwidth(frame_rate, bandwidth)
./encodec/quantization/vq.py:        quantized, codes, commit_loss = self.vq(x, n_q=n_q)
./encodec/quantization/vq.py:        bw = torch.tensor(n_q * bw_per_q).to(x)
./encodec/quantization/vq.py:        return QuantizedResult(quantized, codes, bw, penalty=torch.mean(commit_loss))
./encodec/quantization/vq.py:    def get_num_quantizers_for_bandwidth(self, frame_rate: int, bandwidth: tp.Optional[float] = None) -> int:
./encodec/quantization/vq.py:        """Return n_q based on specified target bandwidth.
./encodec/quantization/vq.py:        """
./encodec/quantization/vq.py:        bw_per_q = self.get_bandwidth_per_quantizer(frame_rate)
./encodec/quantization/vq.py:        n_q = self.n_q
./encodec/quantization/vq.py:        if bandwidth and bandwidth > 0.:
./encodec/quantization/vq.py:            # bandwidth is represented as a thousandth of what it is, e.g. 6kbps bandwidth is represented as
./encodec/quantization/vq.py:            # bandwidth == 6.0
./encodec/quantization/vq.py:            n_q = int(max(1, math.floor(bandwidth * 1000 / bw_per_q)))
./encodec/quantization/vq.py:        return n_q
./encodec/quantization/vq.py:    def get_bandwidth_per_quantizer(self, frame_rate: int):
./encodec/quantization/vq.py:        """Return bandwidth per quantizer for a given input frame rate.
./encodec/quantization/vq.py:        Each quantizer encodes a frame with lg(bins) bits.
./encodec/quantization/vq.py:        """
./encodec/quantization/vq.py:        return math.log2(self.bins) * frame_rate
./encodec/quantization/vq.py:    def encode(self, x: torch.Tensor, frame_rate: int, bandwidth: tp.Optional[float] = None) -> torch.Tensor:
./encodec/quantization/vq.py:        """Encode a given input tensor with the specified frame rate at the given bandwidth.
./encodec/quantization/vq.py:        The RVQ encode method sets the appropriate number of quantizers to use
./encodec/quantization/vq.py:        and returns indices for each quantizer.
./encodec/quantization/vq.py:        """
./encodec/quantization/vq.py:        n_q = self.get_num_quantizers_for_bandwidth(frame_rate, bandwidth)
./encodec/quantization/vq.py:        codes = self.vq.encode(x, n_q=n_q)
./encodec/quantization/vq.py:        return codes
./encodec/quantization/vq.py:    def decode(self, codes: torch.Tensor) -> torch.Tensor:
./encodec/quantization/vq.py:        """Decode the given codes to the quantized representation.
./encodec/quantization/vq.py:        """
./encodec/quantization/vq.py:        quantized = self.vq.decode(codes)
./encodec/quantization/vq.py:        return quantized
./encodec/quantization/ac.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/quantization/ac.py:# All rights reserved.
./encodec/quantization/ac.py:#
./encodec/quantization/ac.py:# This source code is licensed under the license found in the
./encodec/quantization/ac.py:# LICENSE file in the root directory of this source tree.
./encodec/quantization/ac.py:"""Arithmetic coder."""
./encodec/quantization/ac.py:import io
./encodec/quantization/ac.py:import math
./encodec/quantization/ac.py:import random
./encodec/quantization/ac.py:import typing as tp
./encodec/quantization/ac.py:import torch
./encodec/quantization/ac.py:from ..binary import BitPacker, BitUnpacker
./encodec/quantization/ac.py:def build_stable_quantized_cdf(pdf: torch.Tensor, total_range_bits: int,
./encodec/quantization/ac.py:                               roundoff: float = 1e-8, min_range: int = 2,
./encodec/quantization/ac.py:                               check: bool = True) -> torch.Tensor:
./encodec/quantization/ac.py:    """Turn the given PDF into a quantized CDF that splits
./encodec/quantization/ac.py:    [0, 2 ** self.total_range_bits - 1] into chunks of size roughly proportional
./encodec/quantization/ac.py:    to the PDF.
./encodec/quantization/ac.py:    Args:
./encodec/quantization/ac.py:        pdf (torch.Tensor): probability distribution, shape should be `[N]`.
./encodec/quantization/ac.py:        total_range_bits (int): see `ArithmeticCoder`, the typical range we expect
./encodec/quantization/ac.py:            during the coding process is `[0, 2 ** total_range_bits - 1]`.
./encodec/quantization/ac.py:        roundoff (float): will round the pdf up to that level to remove difference coming
./encodec/quantization/ac.py:        from e.g. evaluating the Language Model on different architectures.
./encodec/quantization/ac.py:        min_range (int): minimum range width. Should always be at least 2 for numerical
./encodec/quantization/ac.py:            stability. Use this to avoid pathological behavior is a value
./encodec/quantization/ac.py:            that is expected to be rare actually happens in real life.
./encodec/quantization/ac.py:        check (bool): if True, checks that nothing bad happened, can be deactivated for speed.
./encodec/quantization/ac.py:    """
./encodec/quantization/ac.py:    pdf = pdf.detach()
./encodec/quantization/ac.py:    if roundoff:
./encodec/quantization/ac.py:        pdf = (pdf / roundoff).floor() * roundoff
./encodec/quantization/ac.py:    # interpolate with uniform distribution to achieve desired minimum probability.
./encodec/quantization/ac.py:    total_range = 2 ** total_range_bits
./encodec/quantization/ac.py:    cardinality = len(pdf)
./encodec/quantization/ac.py:    alpha = min_range * cardinality / total_range
./encodec/quantization/ac.py:    assert alpha <= 1, "you must reduce min_range"
./encodec/quantization/ac.py:    ranges = (((1 - alpha) * total_range) * pdf).floor().long()
./encodec/quantization/ac.py:    ranges += min_range
./encodec/quantization/ac.py:    quantized_cdf = torch.cumsum(ranges, dim=-1)
./encodec/quantization/ac.py:    if min_range < 2:
./encodec/quantization/ac.py:        raise ValueError("min_range must be at least 2.")
./encodec/quantization/ac.py:    if check:
./encodec/quantization/ac.py:        assert quantized_cdf[-1] <= 2 ** total_range_bits, quantized_cdf[-1]
./encodec/quantization/ac.py:        if ((quantized_cdf[1:] - quantized_cdf[:-1]) < min_range).any() or quantized_cdf[0] < min_range:
./encodec/quantization/ac.py:            raise ValueError("You must increase your total_range_bits.")
./encodec/quantization/ac.py:    return quantized_cdf
./encodec/quantization/ac.py:class ArithmeticCoder:
./encodec/quantization/ac.py:    """ArithmeticCoder,
./encodec/quantization/ac.py:    Let us take a distribution `p` over `N` symbols, and assume we have a stream
./encodec/quantization/ac.py:    of random variables `s_t` sampled from `p`. Let us assume that we have a budget
./encodec/quantization/ac.py:    of `B` bits that we can afford to write on device. There are `2**B` possible numbers,
./encodec/quantization/ac.py:    corresponding to the range `[0, 2 ** B - 1]`. We can map each of those number to a single
./encodec/quantization/ac.py:    sequence `(s_t)` by doing the following:
./encodec/quantization/ac.py:    1) Initialize the current range to` [0 ** 2 B - 1]`.
./encodec/quantization/ac.py:    2) For each time step t, split the current range into contiguous chunks,
./encodec/quantization/ac.py:        one for each possible outcome, with size roughly proportional to `p`.
./encodec/quantization/ac.py:        For instance, if `p = [0.75, 0.25]`, and the range is `[0, 3]`, the chunks
./encodec/quantization/ac.py:        would be `{[0, 2], [3, 3]}`.
./encodec/quantization/ac.py:    3) Select the chunk corresponding to `s_t`, and replace the current range with this.
./encodec/quantization/ac.py:    4) When done encoding all the values, just select any value remaining in the range.
./encodec/quantization/ac.py:    You will notice that this procedure can fail: for instance if at any point in time
./encodec/quantization/ac.py:    the range is smaller than `N`, then we can no longer assign a non-empty chunk to each
./encodec/quantization/ac.py:    possible outcome. Intuitively, the more likely a value is, the less the range width
./encodec/quantization/ac.py:    will reduce, and the longer we can go on encoding values. This makes sense: for any efficient
./encodec/quantization/ac.py:    coding scheme, likely outcomes would take less bits, and more of them can be coded
./encodec/quantization/ac.py:    with a fixed budget.
./encodec/quantization/ac.py:    In practice, we do not know `B` ahead of time, but we have a way to inject new bits
./encodec/quantization/ac.py:    when the current range decreases below a given limit (given by `total_range_bits`), without
./encodec/quantization/ac.py:    having to redo all the computations. If we encode mostly likely values, we will seldom
./encodec/quantization/ac.py:    need to inject new bits, but a single rare value can deplete our stock of entropy!
./encodec/quantization/ac.py:    In this explanation, we assumed that the distribution `p` was constant. In fact, the present
./encodec/quantization/ac.py:    code works for any sequence `(p_t)` possibly different for each timestep.
./encodec/quantization/ac.py:    We also assume that `s_t ~ p_t`, but that doesn't need to be true, although the smaller
./encodec/quantization/ac.py:    the KL between the true distribution and `p_t`, the most efficient the coding will be.
./encodec/quantization/ac.py:    Args:
./encodec/quantization/ac.py:        fo (IO[bytes]): file-like object to which the bytes will be written to.
./encodec/quantization/ac.py:        total_range_bits (int): the range `M` described above is `2 ** total_range_bits.
./encodec/quantization/ac.py:            Any time the current range width fall under this limit, new bits will
./encodec/quantization/ac.py:            be injected to rescale the initial range.
./encodec/quantization/ac.py:    """
./encodec/quantization/ac.py:    def __init__(self, fo: tp.IO[bytes], total_range_bits: int = 24):
./encodec/quantization/ac.py:        assert total_range_bits <= 30
./encodec/quantization/ac.py:        self.total_range_bits = total_range_bits
./encodec/quantization/ac.py:        self.packer = BitPacker(bits=1, fo=fo)  # we push single bits at a time.
./encodec/quantization/ac.py:        self.low: int = 0
./encodec/quantization/ac.py:        self.high: int = 0
./encodec/quantization/ac.py:        self.max_bit: int = -1
./encodec/quantization/ac.py:        self._dbg: tp.List[tp.Any] = []
./encodec/quantization/ac.py:        self._dbg2: tp.List[tp.Any] = []
./encodec/quantization/ac.py:    @property
./encodec/quantization/ac.py:    def delta(self) -> int:
./encodec/quantization/ac.py:        """Return the current range width."""
./encodec/quantization/ac.py:        return self.high - self.low + 1
./encodec/quantization/ac.py:    def _flush_common_prefix(self):
./encodec/quantization/ac.py:        # If self.low and self.high start with the sames bits,
./encodec/quantization/ac.py:        # those won't change anymore as we always just increase the range
./encodec/quantization/ac.py:        # by powers of 2, and we can flush them out to the bit stream.
./encodec/quantization/ac.py:        assert self.high >= self.low, (self.low, self.high)
./encodec/quantization/ac.py:        assert self.high < 2 ** (self.max_bit + 1)
./encodec/quantization/ac.py:        while self.max_bit >= 0:
./encodec/quantization/ac.py:            b1 = self.low >> self.max_bit
./encodec/quantization/ac.py:            b2 = self.high >> self.max_bit
./encodec/quantization/ac.py:            if b1 == b2:
./encodec/quantization/ac.py:                self.low -= (b1 << self.max_bit)
./encodec/quantization/ac.py:                self.high -= (b1 << self.max_bit)
./encodec/quantization/ac.py:                assert self.high >= self.low, (self.high, self.low, self.max_bit)
./encodec/quantization/ac.py:                assert self.low >= 0
./encodec/quantization/ac.py:                self.max_bit -= 1
./encodec/quantization/ac.py:                self.packer.push(b1)
./encodec/quantization/ac.py:            else:
./encodec/quantization/ac.py:                break
./encodec/quantization/ac.py:    def push(self, symbol: int, quantized_cdf: torch.Tensor):
./encodec/quantization/ac.py:        """Push the given symbol on the stream, flushing out bits
./encodec/quantization/ac.py:        if possible.
./encodec/quantization/ac.py:        Args:
./encodec/quantization/ac.py:            symbol (int): symbol to encode with the AC.
./encodec/quantization/ac.py:            quantized_cdf (torch.Tensor): use `build_stable_quantized_cdf`
./encodec/quantization/ac.py:                to build this from your pdf estimate.
./encodec/quantization/ac.py:        """
./encodec/quantization/ac.py:        while self.delta < 2 ** self.total_range_bits:
./encodec/quantization/ac.py:            self.low *= 2
./encodec/quantization/ac.py:            self.high = self.high * 2 + 1
./encodec/quantization/ac.py:            self.max_bit += 1
./encodec/quantization/ac.py:        range_low = 0 if symbol == 0 else quantized_cdf[symbol - 1].item()
./encodec/quantization/ac.py:        range_high = quantized_cdf[symbol].item() - 1
./encodec/quantization/ac.py:        effective_low = int(math.ceil(range_low * (self.delta / (2 ** self.total_range_bits))))
./encodec/quantization/ac.py:        effective_high = int(math.floor(range_high * (self.delta / (2 ** self.total_range_bits))))
./encodec/quantization/ac.py:        assert self.low <= self.high
./encodec/quantization/ac.py:        self.high = self.low + effective_high
./encodec/quantization/ac.py:        self.low = self.low + effective_low
./encodec/quantization/ac.py:        assert self.low <= self.high, (effective_low, effective_high, range_low, range_high)
./encodec/quantization/ac.py:        self._dbg.append((self.low, self.high))
./encodec/quantization/ac.py:        self._dbg2.append((self.low, self.high))
./encodec/quantization/ac.py:        outs = self._flush_common_prefix()
./encodec/quantization/ac.py:        assert self.low <= self.high
./encodec/quantization/ac.py:        assert self.max_bit >= -1
./encodec/quantization/ac.py:        assert self.max_bit <= 61, self.max_bit
./encodec/quantization/ac.py:        return outs
./encodec/quantization/ac.py:    def flush(self):
./encodec/quantization/ac.py:        """Flush the remaining information to the stream.
./encodec/quantization/ac.py:        """
./encodec/quantization/ac.py:        while self.max_bit >= 0:
./encodec/quantization/ac.py:            b1 = (self.low >> self.max_bit) & 1
./encodec/quantization/ac.py:            self.packer.push(b1)
./encodec/quantization/ac.py:            self.max_bit -= 1
./encodec/quantization/ac.py:        self.packer.flush()
./encodec/quantization/ac.py:class ArithmeticDecoder:
./encodec/quantization/ac.py:    """ArithmeticDecoder, see `ArithmeticCoder` for a detailed explanation.
./encodec/quantization/ac.py:    Note that this must be called with **exactly** the same parameters and sequence
./encodec/quantization/ac.py:    of quantized cdf as the arithmetic encoder or the wrong values will be decoded.
./encodec/quantization/ac.py:    If the AC encoder current range is [L, H], with `L` and `H` having the some common
./encodec/quantization/ac.py:    prefix (i.e. the same most significant bits), then this prefix will be flushed to the stream.
./encodec/quantization/ac.py:    For instances, having read 3 bits `b1 b2 b3`, we know that `[L, H]` is contained inside
./encodec/quantization/ac.py:    `[b1 b2 b3 0 ... 0 b1 b3 b3 1 ... 1]`. Now this specific sub-range can only be obtained
./encodec/quantization/ac.py:    for a specific sequence of symbols and a binary-search allows us to decode those symbols.
./encodec/quantization/ac.py:    At some point, the prefix `b1 b2 b3` will no longer be sufficient to decode new symbols,
./encodec/quantization/ac.py:    and we will need to read new bits from the stream and repeat the process.
./encodec/quantization/ac.py:    """
./encodec/quantization/ac.py:    def __init__(self, fo: tp.IO[bytes], total_range_bits: int = 24):
./encodec/quantization/ac.py:        self.total_range_bits = total_range_bits
./encodec/quantization/ac.py:        self.low: int = 0
./encodec/quantization/ac.py:        self.high: int = 0
./encodec/quantization/ac.py:        self.current: int = 0
./encodec/quantization/ac.py:        self.max_bit: int = -1
./encodec/quantization/ac.py:        self.unpacker = BitUnpacker(bits=1, fo=fo)  # we pull single bits at a time.
./encodec/quantization/ac.py:        # Following is for debugging
./encodec/quantization/ac.py:        self._dbg: tp.List[tp.Any] = []
./encodec/quantization/ac.py:        self._dbg2: tp.List[tp.Any] = []
./encodec/quantization/ac.py:        self._last: tp.Any = None
./encodec/quantization/ac.py:    @property
./encodec/quantization/ac.py:    def delta(self) -> int:
./encodec/quantization/ac.py:        return self.high - self.low + 1
./encodec/quantization/ac.py:    def _flush_common_prefix(self):
./encodec/quantization/ac.py:        # Given the current range [L, H], if both have a common prefix,
./encodec/quantization/ac.py:        # we know we can remove it from our representation to avoid handling large numbers.
./encodec/quantization/ac.py:        while self.max_bit >= 0:
./encodec/quantization/ac.py:            b1 = self.low >> self.max_bit
./encodec/quantization/ac.py:            b2 = self.high >> self.max_bit
./encodec/quantization/ac.py:            if b1 == b2:
./encodec/quantization/ac.py:                self.low -= (b1 << self.max_bit)
./encodec/quantization/ac.py:                self.high -= (b1 << self.max_bit)
./encodec/quantization/ac.py:                self.current -= (b1 << self.max_bit)
./encodec/quantization/ac.py:                assert self.high >= self.low
./encodec/quantization/ac.py:                assert self.low >= 0
./encodec/quantization/ac.py:                self.max_bit -= 1
./encodec/quantization/ac.py:            else:
./encodec/quantization/ac.py:                break
./encodec/quantization/ac.py:    def pull(self, quantized_cdf: torch.Tensor) -> tp.Optional[int]:
./encodec/quantization/ac.py:        """Pull a symbol, reading as many bits from the stream as required.
./encodec/quantization/ac.py:        This returns `None` when the stream has been exhausted.
./encodec/quantization/ac.py:        Args:
./encodec/quantization/ac.py:            quantized_cdf (torch.Tensor): use `build_stable_quantized_cdf`
./encodec/quantization/ac.py:                to build this from your pdf estimate. This must be **exatly**
./encodec/quantization/ac.py:                the same cdf as the one used at encoding time.
./encodec/quantization/ac.py:        """
./encodec/quantization/ac.py:        while self.delta < 2 ** self.total_range_bits:
./encodec/quantization/ac.py:            bit = self.unpacker.pull()
./encodec/quantization/ac.py:            if bit is None:
./encodec/quantization/ac.py:                return None
./encodec/quantization/ac.py:            self.low *= 2
./encodec/quantization/ac.py:            self.high = self.high * 2 + 1
./encodec/quantization/ac.py:            self.current = self.current * 2 + bit
./encodec/quantization/ac.py:            self.max_bit += 1
./encodec/quantization/ac.py:        def bin_search(low_idx: int, high_idx: int):
./encodec/quantization/ac.py:            # Binary search is not just for coding interviews :)
./encodec/quantization/ac.py:            if high_idx < low_idx:
./encodec/quantization/ac.py:                raise RuntimeError("Binary search failed")
./encodec/quantization/ac.py:            mid = (low_idx + high_idx) // 2
./encodec/quantization/ac.py:            range_low = quantized_cdf[mid - 1].item() if mid > 0 else 0
./encodec/quantization/ac.py:            range_high = quantized_cdf[mid].item() - 1
./encodec/quantization/ac.py:            effective_low = int(math.ceil(range_low * (self.delta / (2 ** self.total_range_bits))))
./encodec/quantization/ac.py:            effective_high = int(math.floor(range_high * (self.delta / (2 ** self.total_range_bits))))
./encodec/quantization/ac.py:            low = effective_low + self.low
./encodec/quantization/ac.py:            high = effective_high + self.low
./encodec/quantization/ac.py:            if self.current >= low:
./encodec/quantization/ac.py:                if self.current <= high:
./encodec/quantization/ac.py:                    return (mid, low, high, self.current)
./encodec/quantization/ac.py:                else:
./encodec/quantization/ac.py:                    return bin_search(mid + 1, high_idx)
./encodec/quantization/ac.py:            else:
./encodec/quantization/ac.py:                return bin_search(low_idx, mid - 1)
./encodec/quantization/ac.py:        self._last = (self.low, self.high, self.current, self.max_bit)
./encodec/quantization/ac.py:        sym, self.low, self.high, self.current = bin_search(0, len(quantized_cdf) - 1)
./encodec/quantization/ac.py:        self._dbg.append((self.low, self.high, self.current))
./encodec/quantization/ac.py:        self._flush_common_prefix()
./encodec/quantization/ac.py:        self._dbg2.append((self.low, self.high, self.current))
./encodec/quantization/ac.py:        return sym
./encodec/quantization/ac.py:def test():
./encodec/quantization/ac.py:    torch.manual_seed(1234)
./encodec/quantization/ac.py:    random.seed(1234)
./encodec/quantization/ac.py:    for _ in range(4):
./encodec/quantization/ac.py:        pdfs = []
./encodec/quantization/ac.py:        cardinality = random.randrange(4000)
./encodec/quantization/ac.py:        steps = random.randrange(100, 500)
./encodec/quantization/ac.py:        fo = io.BytesIO()
./encodec/quantization/ac.py:        encoder = ArithmeticCoder(fo)
./encodec/quantization/ac.py:        symbols = []
./encodec/quantization/ac.py:        for step in range(steps):
./encodec/quantization/ac.py:            pdf = torch.softmax(torch.randn(cardinality), dim=0)
./encodec/quantization/ac.py:            pdfs.append(pdf)
./encodec/quantization/ac.py:            q_cdf = build_stable_quantized_cdf(pdf, encoder.total_range_bits)
./encodec/quantization/ac.py:            symbol = torch.multinomial(pdf, 1).item()
./encodec/quantization/ac.py:            symbols.append(symbol)
./encodec/quantization/ac.py:            encoder.push(symbol, q_cdf)
./encodec/quantization/ac.py:        encoder.flush()
./encodec/quantization/ac.py:        fo.seek(0)
./encodec/quantization/ac.py:        decoder = ArithmeticDecoder(fo)
./encodec/quantization/ac.py:        for idx, (pdf, symbol) in enumerate(zip(pdfs, symbols)):
./encodec/quantization/ac.py:            q_cdf = build_stable_quantized_cdf(pdf, encoder.total_range_bits)
./encodec/quantization/ac.py:            decoded_symbol = decoder.pull(q_cdf)
./encodec/quantization/ac.py:            assert decoded_symbol == symbol, idx
./encodec/quantization/ac.py:        assert decoder.pull(torch.zeros(1)) is None
./encodec/quantization/ac.py:if __name__ == "__main__":
./encodec/quantization/ac.py:    test()
./encodec/binary.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/binary.py:# All rights reserved.
./encodec/binary.py:#
./encodec/binary.py:# This source code is licensed under the license found in the
./encodec/binary.py:# LICENSE file in the root directory of this source tree.
./encodec/binary.py:"""Raw binary format for Encodec compressed audio. Actual compression API is in `encodec.compress`."""
./encodec/binary.py:import io
./encodec/binary.py:import json
./encodec/binary.py:import struct
./encodec/binary.py:import typing as tp
./encodec/binary.py:# format is `ECDC` magic code, followed by the header size as uint32.
./encodec/binary.py:# Then an uint8 indicates the protocol version (0.)
./encodec/binary.py:# The header is then provided as json and should contain all required
./encodec/binary.py:# informations for decoding. A raw stream of bytes is then provided
./encodec/binary.py:# and should be interpretable using the json header.
./encodec/binary.py:_encodec_header_struct = struct.Struct('!4sBI')
./encodec/binary.py:_ENCODEC_MAGIC = b'ECDC'
./encodec/binary.py:def write_ecdc_header(fo: tp.IO[bytes], metadata: tp.Any):
./encodec/binary.py:    meta_dumped = json.dumps(metadata).encode('utf-8')
./encodec/binary.py:    version = 0
./encodec/binary.py:    header = _encodec_header_struct.pack(_ENCODEC_MAGIC, version, len(meta_dumped))
./encodec/binary.py:    fo.write(header)
./encodec/binary.py:    fo.write(meta_dumped)
./encodec/binary.py:    fo.flush()
./encodec/binary.py:def _read_exactly(fo: tp.IO[bytes], size: int) -> bytes:
./encodec/binary.py:    buf = b""
./encodec/binary.py:    while len(buf) < size:
./encodec/binary.py:        new_buf = fo.read(size)
./encodec/binary.py:        if not new_buf:
./encodec/binary.py:            raise EOFError("Impossible to read enough data from the stream, "
./encodec/binary.py:                           f"{size} bytes remaining.")
./encodec/binary.py:        buf += new_buf
./encodec/binary.py:        size -= len(new_buf)
./encodec/binary.py:    return buf
./encodec/binary.py:def read_ecdc_header(fo: tp.IO[bytes]):
./encodec/binary.py:    header_bytes = _read_exactly(fo, _encodec_header_struct.size)
./encodec/binary.py:    magic, version, meta_size = _encodec_header_struct.unpack(header_bytes)
./encodec/binary.py:    if magic != _ENCODEC_MAGIC:
./encodec/binary.py:        raise ValueError("File is not in ECDC format.")
./encodec/binary.py:    if version != 0:
./encodec/binary.py:        raise ValueError("Version not supported.")
./encodec/binary.py:    meta_bytes = _read_exactly(fo, meta_size)
./encodec/binary.py:    return json.loads(meta_bytes.decode('utf-8'))
./encodec/binary.py:class BitPacker:
./encodec/binary.py:    """Simple bit packer to handle ints with a non standard width, e.g. 10 bits.
./encodec/binary.py:    Note that for some bandwidth (1.5, 3), the codebook representation
./encodec/binary.py:    will not cover an integer number of bytes.
./encodec/binary.py:    Args:
./encodec/binary.py:        bits (int): number of bits per value that will be pushed.
./encodec/binary.py:        fo (IO[bytes]): file-object to push the bytes to.
./encodec/binary.py:    """
./encodec/binary.py:    def __init__(self, bits: int, fo: tp.IO[bytes]):
./encodec/binary.py:        self._current_value = 0
./encodec/binary.py:        self._current_bits = 0
./encodec/binary.py:        self.bits = bits
./encodec/binary.py:        self.fo = fo
./encodec/binary.py:    def push(self, value: int):
./encodec/binary.py:        """Push a new value to the stream. This will immediately
./encodec/binary.py:        write as many uint8 as possible to the underlying file-object."""
./encodec/binary.py:        self._current_value += (value << self._current_bits)
./encodec/binary.py:        self._current_bits += self.bits
./encodec/binary.py:        while self._current_bits >= 8:
./encodec/binary.py:            lower_8bits = self._current_value & 0xff
./encodec/binary.py:            self._current_bits -= 8
./encodec/binary.py:            self._current_value >>= 8
./encodec/binary.py:            self.fo.write(bytes([lower_8bits]))
./encodec/binary.py:    def flush(self):
./encodec/binary.py:        """Flushes the remaining partial uint8, call this at the end
./encodec/binary.py:        of the stream to encode."""
./encodec/binary.py:        if self._current_bits:
./encodec/binary.py:            self.fo.write(bytes([self._current_value]))
./encodec/binary.py:            self._current_value = 0
./encodec/binary.py:            self._current_bits = 0
./encodec/binary.py:        self.fo.flush()
./encodec/binary.py:class BitUnpacker:
./encodec/binary.py:    """BitUnpacker does the opposite of `BitPacker`.
./encodec/binary.py:    Args:
./encodec/binary.py:        bits (int): number of bits of the values to decode.
./encodec/binary.py:        fo (IO[bytes]): file-object to push the bytes to.
./encodec/binary.py:        """
./encodec/binary.py:    def __init__(self, bits: int, fo: tp.IO[bytes]):
./encodec/binary.py:        self.bits = bits
./encodec/binary.py:        self.fo = fo
./encodec/binary.py:        self._mask = (1 << bits) - 1
./encodec/binary.py:        self._current_value = 0
./encodec/binary.py:        self._current_bits = 0
./encodec/binary.py:    def pull(self) -> tp.Optional[int]:
./encodec/binary.py:        """
./encodec/binary.py:        Pull a single value from the stream, potentially reading some
./encodec/binary.py:        extra bytes from the underlying file-object.
./encodec/binary.py:        Returns `None` when reaching the end of the stream.
./encodec/binary.py:        """
./encodec/binary.py:        while self._current_bits < self.bits:
./encodec/binary.py:            buf = self.fo.read(1)
./encodec/binary.py:            if not buf:
./encodec/binary.py:                return None
./encodec/binary.py:            character = buf[0]
./encodec/binary.py:            self._current_value += character << self._current_bits
./encodec/binary.py:            self._current_bits += 8
./encodec/binary.py:        out = self._current_value & self._mask
./encodec/binary.py:        self._current_value >>= self.bits
./encodec/binary.py:        self._current_bits -= self.bits
./encodec/binary.py:        return out
./encodec/binary.py:def test():
./encodec/binary.py:    import torch
./encodec/binary.py:    torch.manual_seed(1234)
./encodec/binary.py:    for rep in range(4):
./encodec/binary.py:        length: int = torch.randint(10, 2_000, (1,)).item()
./encodec/binary.py:        bits: int = torch.randint(1, 16, (1,)).item()
./encodec/binary.py:        tokens: tp.List[int] = torch.randint(2 ** bits, (length,)).tolist()
./encodec/binary.py:        rebuilt: tp.List[int] = []
./encodec/binary.py:        buf = io.BytesIO()
./encodec/binary.py:        packer = BitPacker(bits, buf)
./encodec/binary.py:        for token in tokens:
./encodec/binary.py:            packer.push(token)
./encodec/binary.py:        packer.flush()
./encodec/binary.py:        buf.seek(0)
./encodec/binary.py:        unpacker = BitUnpacker(bits, buf)
./encodec/binary.py:        while True:
./encodec/binary.py:            value = unpacker.pull()
./encodec/binary.py:            if value is None:
./encodec/binary.py:                break
./encodec/binary.py:            rebuilt.append(value)
./encodec/binary.py:        assert len(rebuilt) >= len(tokens), (len(rebuilt), len(tokens))
./encodec/binary.py:        # The flushing mechanism might lead to "ghost" values at the end of the stream.
./encodec/binary.py:        assert len(rebuilt) <= len(tokens) + 8 // bits, (len(rebuilt), len(tokens), bits)
./encodec/binary.py:        for idx, (a, b) in enumerate(zip(tokens, rebuilt)):
./encodec/binary.py:            assert a == b, (idx, a, b)
./encodec/binary.py:if __name__ == '__main__':
./encodec/binary.py:    test()
./encodec/utils.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/utils.py:# All rights reserved.
./encodec/utils.py:#
./encodec/utils.py:# This source code is licensed under the license found in the
./encodec/utils.py:# LICENSE file in the root directory of this source tree.
./encodec/utils.py:"""Various utilities."""
./encodec/utils.py:from hashlib import sha256
./encodec/utils.py:from pathlib import Path
./encodec/utils.py:import typing as tp
./encodec/utils.py:import torch
./encodec/utils.py:import torchaudio
./encodec/utils.py:def _linear_overlap_add(frames: tp.List[torch.Tensor], stride: int):
./encodec/utils.py:    # Generic overlap add, with linear fade-in/fade-out, supporting complex scenario
./encodec/utils.py:    # e.g., more than 2 frames per position.
./encodec/utils.py:    # The core idea is to use a weight function that is a triangle,
./encodec/utils.py:    # with a maximum value at the middle of the segment.
./encodec/utils.py:    # We use this weighting when summing the frames, and divide by the sum of weights
./encodec/utils.py:    # for each positions at the end. Thus:
./encodec/utils.py:    #   - if a frame is the only one to cover a position, the weighting is a no-op.
./encodec/utils.py:    #   - if 2 frames cover a position:
./encodec/utils.py:    #          ...  ...
./encodec/utils.py:    #         /   \/   \
./encodec/utils.py:    #        /    /\    \
./encodec/utils.py:    #            S  T       , i.e. S offset of second frame starts, T end of first frame.
./encodec/utils.py:    # Then the weight function for each one is: (t - S), (T - t), with `t` a given offset.
./encodec/utils.py:    # After the final normalization, the weight of the second frame at position `t` is
./encodec/utils.py:    # (t - S) / (t - S + (T - t)) = (t - S) / (T - S), which is exactly what we want.
./encodec/utils.py:    #
./encodec/utils.py:    #   - if more than 2 frames overlap at a given point, we hope that by induction
./encodec/utils.py:    #      something sensible happens.
./encodec/utils.py:    assert len(frames)
./encodec/utils.py:    device = frames[0].device
./encodec/utils.py:    dtype = frames[0].dtype
./encodec/utils.py:    shape = frames[0].shape[:-1]
./encodec/utils.py:    total_size = stride * (len(frames) - 1) + frames[-1].shape[-1]
./encodec/utils.py:    frame_length = frames[0].shape[-1]
./encodec/utils.py:    t = torch.linspace(0, 1, frame_length + 2, device=device, dtype=dtype)[1: -1]
./encodec/utils.py:    weight = 0.5 - (t - 0.5).abs()
./encodec/utils.py:    sum_weight = torch.zeros(total_size, device=device, dtype=dtype)
./encodec/utils.py:    out = torch.zeros(*shape, total_size, device=device, dtype=dtype)
./encodec/utils.py:    offset: int = 0
./encodec/utils.py:    for frame in frames:
./encodec/utils.py:        frame_length = frame.shape[-1]
./encodec/utils.py:        out[..., offset:offset + frame_length] += weight[:frame_length] * frame
./encodec/utils.py:        sum_weight[offset:offset + frame_length] += weight[:frame_length]
./encodec/utils.py:        offset += stride
./encodec/utils.py:    assert sum_weight.min() > 0
./encodec/utils.py:    return out / sum_weight
./encodec/utils.py:def _get_checkpoint_url(root_url: str, checkpoint: str):
./encodec/utils.py:    if not root_url.endswith('/'):
./encodec/utils.py:        root_url += '/'
./encodec/utils.py:    return root_url + checkpoint
./encodec/utils.py:def _check_checksum(path: Path, checksum: str):
./encodec/utils.py:    sha = sha256()
./encodec/utils.py:    with open(path, 'rb') as file:
./encodec/utils.py:        while True:
./encodec/utils.py:            buf = file.read(2**20)
./encodec/utils.py:            if not buf:
./encodec/utils.py:                break
./encodec/utils.py:            sha.update(buf)
./encodec/utils.py:    actual_checksum = sha.hexdigest()[:len(checksum)]
./encodec/utils.py:    if actual_checksum != checksum:
./encodec/utils.py:        raise RuntimeError(f'Invalid checksum for file {path}, '
./encodec/utils.py:                           f'expected {checksum} but got {actual_checksum}')
./encodec/utils.py:def convert_audio(wav: torch.Tensor, sr: int, target_sr: int, target_channels: int):
./encodec/utils.py:    assert wav.dim() >= 2, "Audio tensor must have at least 2 dimensions"
./encodec/utils.py:    assert wav.shape[-2] in [1, 2], "Audio must be mono or stereo."
./encodec/utils.py:    *shape, channels, length = wav.shape
./encodec/utils.py:    if target_channels == 1:
./encodec/utils.py:        wav = wav.mean(-2, keepdim=True)
./encodec/utils.py:    elif target_channels == 2:
./encodec/utils.py:        wav = wav.expand(*shape, target_channels, length)
./encodec/utils.py:    elif channels == 1:
./encodec/utils.py:        wav = wav.expand(target_channels, -1)
./encodec/utils.py:    else:
./encodec/utils.py:        raise RuntimeError(f"Impossible to convert from {channels} to {target_channels}")
./encodec/utils.py:    wav = torchaudio.transforms.Resample(sr, target_sr)(wav)
./encodec/utils.py:    return wav
./encodec/utils.py:def save_audio(wav: torch.Tensor, path: tp.Union[Path, str],
./encodec/utils.py:               sample_rate: int, rescale: bool = False):
./encodec/utils.py:    limit = 0.99
./encodec/utils.py:    mx = wav.abs().max()
./encodec/utils.py:    if rescale:
./encodec/utils.py:        wav = wav * min(limit / mx, 1)
./encodec/utils.py:    else:
./encodec/utils.py:        wav = wav.clamp(-limit, limit)
./encodec/utils.py:    torchaudio.save(str(path), wav, sample_rate=sample_rate, encoding='PCM_S', bits_per_sample=16)
./encodec/msstftd.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/msstftd.py:# All rights reserved.
./encodec/msstftd.py:#
./encodec/msstftd.py:# This source code is licensed under the license found in the
./encodec/msstftd.py:# LICENSE file in the root directory of this source tree.
./encodec/msstftd.py:"""MS-STFT discriminator, provided here for reference."""
./encodec/msstftd.py:import typing as tp
./encodec/msstftd.py:import torchaudio
./encodec/msstftd.py:import torch
./encodec/msstftd.py:from torch import nn
./encodec/msstftd.py:from einops import rearrange
./encodec/msstftd.py:from .modules import NormConv2d
./encodec/msstftd.py:FeatureMapType = tp.List[torch.Tensor]
./encodec/msstftd.py:LogitsType = torch.Tensor
./encodec/msstftd.py:DiscriminatorOutput = tp.Tuple[tp.List[LogitsType], tp.List[FeatureMapType]]
./encodec/msstftd.py:def get_2d_padding(kernel_size: tp.Tuple[int, int], dilation: tp.Tuple[int, int] = (1, 1)):
./encodec/msstftd.py:    return (((kernel_size[0] - 1) * dilation[0]) // 2, ((kernel_size[1] - 1) * dilation[1]) // 2)
./encodec/msstftd.py:class DiscriminatorSTFT(nn.Module):
./encodec/msstftd.py:    """STFT sub-discriminator.
./encodec/msstftd.py:    Args:
./encodec/msstftd.py:        filters (int): Number of filters in convolutions
./encodec/msstftd.py:        in_channels (int): Number of input channels. Default: 1
./encodec/msstftd.py:        out_channels (int): Number of output channels. Default: 1
./encodec/msstftd.py:        n_fft (int): Size of FFT for each scale. Default: 1024
./encodec/msstftd.py:        hop_length (int): Length of hop between STFT windows for each scale. Default: 256
./encodec/msstftd.py:        kernel_size (tuple of int): Inner Conv2d kernel sizes. Default: ``(3, 9)``
./encodec/msstftd.py:        stride (tuple of int): Inner Conv2d strides. Default: ``(1, 2)``
./encodec/msstftd.py:        dilations (list of int): Inner Conv2d dilation on the time dimension. Default: ``[1, 2, 4]``
./encodec/msstftd.py:        win_length (int): Window size for each scale. Default: 1024
./encodec/msstftd.py:        normalized (bool): Whether to normalize by magnitude after stft. Default: True
./encodec/msstftd.py:        norm (str): Normalization method. Default: `'weight_norm'`
./encodec/msstftd.py:        activation (str): Activation function. Default: `'LeakyReLU'`
./encodec/msstftd.py:        activation_params (dict): Parameters to provide to the activation function.
./encodec/msstftd.py:        growth (int): Growth factor for the filters. Default: 1
./encodec/msstftd.py:    """
./encodec/msstftd.py:    def __init__(self, filters: int, in_channels: int = 1, out_channels: int = 1,
./encodec/msstftd.py:                 n_fft: int = 1024, hop_length: int = 256, win_length: int = 1024, max_filters: int = 1024,
./encodec/msstftd.py:                 filters_scale: int = 1, kernel_size: tp.Tuple[int, int] = (3, 9), dilations: tp.List = [1, 2, 4],
./encodec/msstftd.py:                 stride: tp.Tuple[int, int] = (1, 2), normalized: bool = True, norm: str = 'weight_norm',
./encodec/msstftd.py:                 activation: str = 'LeakyReLU', activation_params: dict = {'negative_slope': 0.2}):
./encodec/msstftd.py:        super().__init__()
./encodec/msstftd.py:        assert len(kernel_size) == 2
./encodec/msstftd.py:        assert len(stride) == 2
./encodec/msstftd.py:        self.filters = filters
./encodec/msstftd.py:        self.in_channels = in_channels
./encodec/msstftd.py:        self.out_channels = out_channels
./encodec/msstftd.py:        self.n_fft = n_fft
./encodec/msstftd.py:        self.hop_length = hop_length
./encodec/msstftd.py:        self.win_length = win_length
./encodec/msstftd.py:        self.normalized = normalized
./encodec/msstftd.py:        self.activation = getattr(torch.nn, activation)(**activation_params)
./encodec/msstftd.py:        self.spec_transform = torchaudio.transforms.Spectrogram(
./encodec/msstftd.py:            n_fft=self.n_fft, hop_length=self.hop_length, win_length=self.win_length, window_fn=torch.hann_window,
./encodec/msstftd.py:            normalized=self.normalized, center=False, pad_mode=None, power=None)
./encodec/msstftd.py:        spec_channels = 2 * self.in_channels
./encodec/msstftd.py:        self.convs = nn.ModuleList()
./encodec/msstftd.py:        self.convs.append(
./encodec/msstftd.py:            NormConv2d(spec_channels, self.filters, kernel_size=kernel_size, padding=get_2d_padding(kernel_size))
./encodec/msstftd.py:        )
./encodec/msstftd.py:        in_chs = min(filters_scale * self.filters, max_filters)
./encodec/msstftd.py:        for i, dilation in enumerate(dilations):
./encodec/msstftd.py:            out_chs = min((filters_scale ** (i + 1)) * self.filters, max_filters)
./encodec/msstftd.py:            self.convs.append(NormConv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride,
./encodec/msstftd.py:                                         dilation=(dilation, 1), padding=get_2d_padding(kernel_size, (dilation, 1)),
./encodec/msstftd.py:                                         norm=norm))
./encodec/msstftd.py:            in_chs = out_chs
./encodec/msstftd.py:        out_chs = min((filters_scale ** (len(dilations) + 1)) * self.filters, max_filters)
./encodec/msstftd.py:        self.convs.append(NormConv2d(in_chs, out_chs, kernel_size=(kernel_size[0], kernel_size[0]),
./encodec/msstftd.py:                                     padding=get_2d_padding((kernel_size[0], kernel_size[0])),
./encodec/msstftd.py:                                     norm=norm))
./encodec/msstftd.py:        self.conv_post = NormConv2d(out_chs, self.out_channels,
./encodec/msstftd.py:                                    kernel_size=(kernel_size[0], kernel_size[0]),
./encodec/msstftd.py:                                    padding=get_2d_padding((kernel_size[0], kernel_size[0])),
./encodec/msstftd.py:                                    norm=norm)
./encodec/msstftd.py:    def forward(self, x: torch.Tensor):
./encodec/msstftd.py:        fmap = []
./encodec/msstftd.py:        z = self.spec_transform(x)  # [B, 2, Freq, Frames, 2]
./encodec/msstftd.py:        z = torch.cat([z.real, z.imag], dim=1)
./encodec/msstftd.py:        z = rearrange(z, 'b c w t -> b c t w')
./encodec/msstftd.py:        for i, layer in enumerate(self.convs):
./encodec/msstftd.py:            z = layer(z)
./encodec/msstftd.py:            z = self.activation(z)
./encodec/msstftd.py:            fmap.append(z)
./encodec/msstftd.py:        z = self.conv_post(z)
./encodec/msstftd.py:        return z, fmap
./encodec/msstftd.py:class MultiScaleSTFTDiscriminator(nn.Module):
./encodec/msstftd.py:    """Multi-Scale STFT (MS-STFT) discriminator.
./encodec/msstftd.py:    Args:
./encodec/msstftd.py:        filters (int): Number of filters in convolutions
./encodec/msstftd.py:        in_channels (int): Number of input channels. Default: 1
./encodec/msstftd.py:        out_channels (int): Number of output channels. Default: 1
./encodec/msstftd.py:        n_ffts (Sequence[int]): Size of FFT for each scale
./encodec/msstftd.py:        hop_lengths (Sequence[int]): Length of hop between STFT windows for each scale
./encodec/msstftd.py:        win_lengths (Sequence[int]): Window size for each scale
./encodec/msstftd.py:        **kwargs: additional args for STFTDiscriminator
./encodec/msstftd.py:    """
./encodec/msstftd.py:    def __init__(self, filters: int, in_channels: int = 1, out_channels: int = 1,
./encodec/msstftd.py:                 n_ffts: tp.List[int] = [1024, 2048, 512], hop_lengths: tp.List[int] = [256, 512, 128],
./encodec/msstftd.py:                 win_lengths: tp.List[int] = [1024, 2048, 512], **kwargs):
./encodec/msstftd.py:        super().__init__()
./encodec/msstftd.py:        assert len(n_ffts) == len(hop_lengths) == len(win_lengths)
./encodec/msstftd.py:        self.discriminators = nn.ModuleList([
./encodec/msstftd.py:            DiscriminatorSTFT(filters, in_channels=in_channels, out_channels=out_channels,
./encodec/msstftd.py:                              n_fft=n_ffts[i], win_length=win_lengths[i], hop_length=hop_lengths[i], **kwargs)
./encodec/msstftd.py:            for i in range(len(n_ffts))
./encodec/msstftd.py:        ])
./encodec/msstftd.py:        self.num_discriminators = len(self.discriminators)
./encodec/msstftd.py:    def forward(self, x: torch.Tensor) -> DiscriminatorOutput:
./encodec/msstftd.py:        logits = []
./encodec/msstftd.py:        fmaps = []
./encodec/msstftd.py:        for disc in self.discriminators:
./encodec/msstftd.py:            logit, fmap = disc(x)
./encodec/msstftd.py:            logits.append(logit)
./encodec/msstftd.py:            fmaps.append(fmap)
./encodec/msstftd.py:        return logits, fmaps
./encodec/msstftd.py:def test():
./encodec/msstftd.py:    disc = MultiScaleSTFTDiscriminator(filters=32)
./encodec/msstftd.py:    y = torch.randn(1, 1, 24000)
./encodec/msstftd.py:    y_hat = torch.randn(1, 1, 24000)
./encodec/msstftd.py:    y_disc_r, fmap_r = disc(y)
./encodec/msstftd.py:    y_disc_gen, fmap_gen = disc(y_hat)
./encodec/msstftd.py:    assert len(y_disc_r) == len(y_disc_gen) == len(fmap_r) == len(fmap_gen) == disc.num_discriminators
./encodec/msstftd.py:    assert all([len(fm) == 5 for fm in fmap_r + fmap_gen])
./encodec/msstftd.py:    assert all([list(f.shape)[:2] == [1, 32] for fm in fmap_r + fmap_gen for f in fm])
./encodec/msstftd.py:    assert all([len(logits.shape) == 4 for logits in y_disc_r + y_disc_gen])
./encodec/msstftd.py:if __name__ == '__main__':
./encodec/msstftd.py:    test()
./encodec/__main__.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/__main__.py:# All rights reserved.
./encodec/__main__.py:#
./encodec/__main__.py:# This source code is licensed under the license found in the
./encodec/__main__.py:# LICENSE file in the root directory of this source tree.
./encodec/__main__.py:"""Command-line for audio compression."""
./encodec/__main__.py:import argparse
./encodec/__main__.py:from pathlib import Path
./encodec/__main__.py:import sys
./encodec/__main__.py:import torchaudio
./encodec/__main__.py:from .compress import compress, decompress, MODELS
./encodec/__main__.py:from .utils import save_audio, convert_audio
./encodec/__main__.py:SUFFIX = '.ecdc'
./encodec/__main__.py:def get_parser():
./encodec/__main__.py:    parser = argparse.ArgumentParser(
./encodec/__main__.py:        'encodec',
./encodec/__main__.py:        description='High fidelity neural audio codec. '
./encodec/__main__.py:                    'If input is a .ecdc, decompresses it. '
./encodec/__main__.py:                    'If input is .wav, compresses it. If output is also wav, '
./encodec/__main__.py:                    'do a compression/decompression cycle.')
./encodec/__main__.py:    parser.add_argument(
./encodec/__main__.py:        'input', type=Path,
./encodec/__main__.py:        help='Input file, whatever is supported by torchaudio on your system.')
./encodec/__main__.py:    parser.add_argument(
./encodec/__main__.py:        'output', type=Path, nargs='?',
./encodec/__main__.py:        help='Output file, otherwise inferred from input file.')
./encodec/__main__.py:    parser.add_argument(
./encodec/__main__.py:        '-b', '--bandwidth', type=float, default=6, choices=[1.5, 3., 6., 12., 24.],
./encodec/__main__.py:        help='Target bandwidth (1.5, 3, 6, 12 or 24). 1.5 is not supported with --hq.')
./encodec/__main__.py:    parser.add_argument(
./encodec/__main__.py:        '-q', '--hq', action='store_true',
./encodec/__main__.py:        help='Use HQ stereo model operating on 48 kHz sampled audio.')
./encodec/__main__.py:    parser.add_argument(
./encodec/__main__.py:        '-l', '--lm', action='store_true',
./encodec/__main__.py:        help='Use a language model to reduce the model size (5x slower though).')
./encodec/__main__.py:    parser.add_argument(
./encodec/__main__.py:        '-f', '--force', action='store_true',
./encodec/__main__.py:        help='Overwrite output file if it exists.')
./encodec/__main__.py:    parser.add_argument(
./encodec/__main__.py:        '-s', '--decompress_suffix', type=str, default='_decompressed',
./encodec/__main__.py:        help='Suffix for the decompressed output file (if no output path specified)')
./encodec/__main__.py:    parser.add_argument(
./encodec/__main__.py:        '-r', '--rescale', action='store_true',
./encodec/__main__.py:        help='Automatically rescale the output to avoid clipping.')
./encodec/__main__.py:    return parser
./encodec/__main__.py:def fatal(*args):
./encodec/__main__.py:    print(*args, file=sys.stderr)
./encodec/__main__.py:    sys.exit(1)
./encodec/__main__.py:def check_output_exists(args):
./encodec/__main__.py:    if not args.output.parent.exists():
./encodec/__main__.py:        fatal(f"Output folder for {args.output} does not exist.")
./encodec/__main__.py:    if args.output.exists() and not args.force:
./encodec/__main__.py:        fatal(f"Output file {args.output} exist. Use -f / --force to overwrite.")
./encodec/__main__.py:def check_clipping(wav, args):
./encodec/__main__.py:    if args.rescale:
./encodec/__main__.py:        return
./encodec/__main__.py:    mx = wav.abs().max()
./encodec/__main__.py:    limit = 0.99
./encodec/__main__.py:    if mx > limit:
./encodec/__main__.py:        print(
./encodec/__main__.py:            f"Clipping!! max scale {mx}, limit is {limit}. "
./encodec/__main__.py:            "To avoid clipping, use the `-r` option to rescale the output.",
./encodec/__main__.py:            file=sys.stderr)
./encodec/__main__.py:def main():
./encodec/__main__.py:    args = get_parser().parse_args()
./encodec/__main__.py:    if not args.input.exists():
./encodec/__main__.py:        fatal(f"Input file {args.input} does not exist.")
./encodec/__main__.py:    if args.input.suffix.lower() == SUFFIX:
./encodec/__main__.py:        # Decompression
./encodec/__main__.py:        if args.output is None:
./encodec/__main__.py:            args.output = args.input.with_name(args.input.stem + args.decompress_suffix).with_suffix('.wav')
./encodec/__main__.py:        elif args.output.suffix.lower() != '.wav':
./encodec/__main__.py:            fatal("Output extension must be .wav")
./encodec/__main__.py:        check_output_exists(args)
./encodec/__main__.py:        out, out_sample_rate = decompress(args.input.read_bytes())
./encodec/__main__.py:        check_clipping(out, args)
./encodec/__main__.py:        save_audio(out, args.output, out_sample_rate, rescale=args.rescale)
./encodec/__main__.py:    else:
./encodec/__main__.py:        # Compression
./encodec/__main__.py:        if args.output is None:
./encodec/__main__.py:            args.output = args.input.with_suffix(SUFFIX)
./encodec/__main__.py:        elif args.output.suffix.lower() not in [SUFFIX, '.wav']:
./encodec/__main__.py:            fatal(f"Output extension must be .wav or {SUFFIX}")
./encodec/__main__.py:        check_output_exists(args)
./encodec/__main__.py:        model_name = 'encodec_48khz' if args.hq else 'encodec_24khz'
./encodec/__main__.py:        model = MODELS[model_name]()
./encodec/__main__.py:        if args.bandwidth not in model.target_bandwidths:
./encodec/__main__.py:            fatal(f"Bandwidth {args.bandwidth} is not supported by the model {model_name}")
./encodec/__main__.py:        model.set_target_bandwidth(args.bandwidth)
./encodec/__main__.py:        wav, sr = torchaudio.load(args.input)
./encodec/__main__.py:        wav = convert_audio(wav, sr, model.sample_rate, model.channels)
./encodec/__main__.py:        compressed = compress(model, wav, use_lm=args.lm)
./encodec/__main__.py:        if args.output.suffix.lower() == SUFFIX:
./encodec/__main__.py:            args.output.write_bytes(compressed)
./encodec/__main__.py:        else:
./encodec/__main__.py:            # Directly run decompression stage
./encodec/__main__.py:            assert args.output.suffix.lower() == '.wav'
./encodec/__main__.py:            out, out_sample_rate = decompress(compressed)
./encodec/__main__.py:            check_clipping(out, args)
./encodec/__main__.py:            save_audio(out, args.output, out_sample_rate, rescale=args.rescale)
./encodec/__main__.py:if __name__ == '__main__':
./encodec/__main__.py:    main()
./encodec/compress.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/compress.py:# All rights reserved.
./encodec/compress.py:#
./encodec/compress.py:# This source code is licensed under the license found in the
./encodec/compress.py:# LICENSE file in the root directory of this source tree.
./encodec/compress.py:"""API to compress/decompress audio to bytestreams."""
./encodec/compress.py:import io
./encodec/compress.py:import math
./encodec/compress.py:import struct
./encodec/compress.py:import time
./encodec/compress.py:import typing as tp
./encodec/compress.py:import torch
./encodec/compress.py:from . import binary
./encodec/compress.py:from .quantization.ac import ArithmeticCoder, ArithmeticDecoder, build_stable_quantized_cdf
./encodec/compress.py:from .model import EncodecModel, EncodedFrame
./encodec/compress.py:MODELS = {
./encodec/compress.py:    'encodec_24khz': EncodecModel.encodec_model_24khz,
./encodec/compress.py:    'encodec_48khz': EncodecModel.encodec_model_48khz,
./encodec/compress.py:}
./encodec/compress.py:def compress_to_file(model: EncodecModel, wav: torch.Tensor, fo: tp.IO[bytes],
./encodec/compress.py:                     use_lm: bool = True):
./encodec/compress.py:    """Compress a waveform to a file-object using the given model.
./encodec/compress.py:    Args:
./encodec/compress.py:        model (EncodecModel): a pre-trained EncodecModel to use to compress the audio.
./encodec/compress.py:        wav (torch.Tensor): waveform to compress, should have a shape `[C, T]`, with `C`
./encodec/compress.py:            matching `model.channels`, and the proper sample rate (e.g. `model.sample_rate`).
./encodec/compress.py:            Use `utils.convert_audio` if this is not the case.
./encodec/compress.py:        fo (IO[bytes]): file-object to which the compressed bits will be written.
./encodec/compress.py:            See `compress` if you want obtain a `bytes` object instead.
./encodec/compress.py:        use_lm (bool): if True, use a pre-trained language model to further
./encodec/compress.py:            compress the stream using Entropy Coding. This will slow down compression
./encodec/compress.py:            quite a bit, expect between 20 to 30% of size reduction.
./encodec/compress.py:    """
./encodec/compress.py:    assert wav.dim() == 2, "Only single waveform can be encoded."
./encodec/compress.py:    if model.name not in MODELS:
./encodec/compress.py:        raise ValueError(f"The provided model {model.name} is not supported.")
./encodec/compress.py:    if use_lm:
./encodec/compress.py:        lm = model.get_lm_model()
./encodec/compress.py:    with torch.no_grad():
./encodec/compress.py:        frames = model.encode(wav[None])
./encodec/compress.py:    metadata = {
./encodec/compress.py:        'm': model.name,                 # model name
./encodec/compress.py:        'al': wav.shape[-1],             # audio_length
./encodec/compress.py:        'nc': frames[0][0].shape[1],     # num_codebooks
./encodec/compress.py:        'lm': use_lm,                    # use lm?
./encodec/compress.py:    }
./encodec/compress.py:    binary.write_ecdc_header(fo, metadata)
./encodec/compress.py:    for (frame, scale) in frames:
./encodec/compress.py:        if scale is not None:
./encodec/compress.py:            fo.write(struct.pack('!f', scale.cpu().item()))
./encodec/compress.py:        _, K, T = frame.shape
./encodec/compress.py:        if use_lm:
./encodec/compress.py:            coder = ArithmeticCoder(fo)
./encodec/compress.py:            states: tp.Any = None
./encodec/compress.py:            offset = 0
./encodec/compress.py:            input_ = torch.zeros(1, K, 1, dtype=torch.long, device=wav.device)
./encodec/compress.py:        else:
./encodec/compress.py:            packer = binary.BitPacker(model.bits_per_codebook, fo)
./encodec/compress.py:        for t in range(T):
./encodec/compress.py:            if use_lm:
./encodec/compress.py:                with torch.no_grad():
./encodec/compress.py:                    probas, states, offset = lm(input_, states, offset)
./encodec/compress.py:                # We emulate a streaming scenario even though we do not provide an API for it.
./encodec/compress.py:                # This gives us a more accurate benchmark.
./encodec/compress.py:                input_ = 1 + frame[:, :, t: t + 1]
./encodec/compress.py:            for k, value in enumerate(frame[0, :, t].tolist()):
./encodec/compress.py:                if use_lm:
./encodec/compress.py:                    q_cdf = build_stable_quantized_cdf(
./encodec/compress.py:                        probas[0, :, k, 0], coder.total_range_bits, check=False)
./encodec/compress.py:                    coder.push(value, q_cdf)
./encodec/compress.py:                else:
./encodec/compress.py:                    packer.push(value)
./encodec/compress.py:        if use_lm:
./encodec/compress.py:            coder.flush()
./encodec/compress.py:        else:
./encodec/compress.py:            packer.flush()
./encodec/compress.py:def decompress_from_file(fo: tp.IO[bytes], device='cpu') -> tp.Tuple[torch.Tensor, int]:
./encodec/compress.py:    """Decompress from a file-object.
./encodec/compress.py:    Returns a tuple `(wav, sample_rate)`.
./encodec/compress.py:    Args:
./encodec/compress.py:        fo (IO[bytes]): file-object from which to read. If you want to decompress
./encodec/compress.py:            from `bytes` instead, see `decompress`.
./encodec/compress.py:        device: device to use to perform the computations.
./encodec/compress.py:    """
./encodec/compress.py:    metadata = binary.read_ecdc_header(fo)
./encodec/compress.py:    model_name = metadata['m']
./encodec/compress.py:    audio_length = metadata['al']
./encodec/compress.py:    num_codebooks = metadata['nc']
./encodec/compress.py:    use_lm = metadata['lm']
./encodec/compress.py:    assert isinstance(audio_length, int)
./encodec/compress.py:    assert isinstance(num_codebooks, int)
./encodec/compress.py:    if model_name not in MODELS:
./encodec/compress.py:        raise ValueError(f"The audio was compressed with an unsupported model {model_name}.")
./encodec/compress.py:    model = MODELS[model_name]().to(device)
./encodec/compress.py:    if use_lm:
./encodec/compress.py:        lm = model.get_lm_model()
./encodec/compress.py:    frames: tp.List[EncodedFrame] = []
./encodec/compress.py:    segment_length = model.segment_length or audio_length
./encodec/compress.py:    segment_stride = model.segment_stride or audio_length
./encodec/compress.py:    for offset in range(0, audio_length, segment_stride):
./encodec/compress.py:        this_segment_length = min(audio_length - offset, segment_length)
./encodec/compress.py:        frame_length = int(math.ceil(this_segment_length * model.frame_rate / model.sample_rate))
./encodec/compress.py:        if model.normalize:
./encodec/compress.py:            scale_f, = struct.unpack('!f', binary._read_exactly(fo, struct.calcsize('!f')))
./encodec/compress.py:            scale = torch.tensor(scale_f, device=device).view(1)
./encodec/compress.py:        else:
./encodec/compress.py:            scale = None
./encodec/compress.py:        if use_lm:
./encodec/compress.py:            decoder = ArithmeticDecoder(fo)
./encodec/compress.py:            states: tp.Any = None
./encodec/compress.py:            offset = 0
./encodec/compress.py:            input_ = torch.zeros(1, num_codebooks, 1, dtype=torch.long, device=device)
./encodec/compress.py:        else:
./encodec/compress.py:            unpacker = binary.BitUnpacker(model.bits_per_codebook, fo)
./encodec/compress.py:        frame = torch.zeros(1, num_codebooks, frame_length, dtype=torch.long, device=device)
./encodec/compress.py:        for t in range(frame_length):
./encodec/compress.py:            if use_lm:
./encodec/compress.py:                with torch.no_grad():
./encodec/compress.py:                    probas, states, offset = lm(input_, states, offset)
./encodec/compress.py:            code_list: tp.List[int] = []
./encodec/compress.py:            for k in range(num_codebooks):
./encodec/compress.py:                if use_lm:
./encodec/compress.py:                    q_cdf = build_stable_quantized_cdf(
./encodec/compress.py:                        probas[0, :, k, 0], decoder.total_range_bits, check=False)
./encodec/compress.py:                    code = decoder.pull(q_cdf)
./encodec/compress.py:                else:
./encodec/compress.py:                    code = unpacker.pull()
./encodec/compress.py:                if code is None:
./encodec/compress.py:                    raise EOFError("The stream ended sooner than expected.")
./encodec/compress.py:                code_list.append(code)
./encodec/compress.py:            codes = torch.tensor(code_list, dtype=torch.long, device=device)
./encodec/compress.py:            frame[0, :, t] = codes
./encodec/compress.py:            if use_lm:
./encodec/compress.py:                input_ = 1 + frame[:, :, t: t + 1]
./encodec/compress.py:        frames.append((frame, scale))
./encodec/compress.py:    with torch.no_grad():
./encodec/compress.py:        wav = model.decode(frames)
./encodec/compress.py:    return wav[0, :, :audio_length], model.sample_rate
./encodec/compress.py:def compress(model: EncodecModel, wav: torch.Tensor, use_lm: bool = False) -> bytes:
./encodec/compress.py:    """Compress a waveform using the given model. Returns the compressed bytes.
./encodec/compress.py:    Args:
./encodec/compress.py:        model (EncodecModel): a pre-trained EncodecModel to use to compress the audio.
./encodec/compress.py:        wav (torch.Tensor): waveform to compress, should have a shape `[C, T]`, with `C`
./encodec/compress.py:            matching `model.channels`, and the proper sample rate (e.g. `model.sample_rate`).
./encodec/compress.py:            Use `utils.convert_audio` if this is not the case.
./encodec/compress.py:        use_lm (bool): if True, use a pre-trained language model to further
./encodec/compress.py:            compress the stream using Entropy Coding. This will slow down compression
./encodec/compress.py:            quite a bit, expect between 20 to 30% of size reduction.
./encodec/compress.py:    """
./encodec/compress.py:    fo = io.BytesIO()
./encodec/compress.py:    compress_to_file(model, wav, fo, use_lm=use_lm)
./encodec/compress.py:    return fo.getvalue()
./encodec/compress.py:def decompress(compressed: bytes, device='cpu') -> tp.Tuple[torch.Tensor, int]:
./encodec/compress.py:    """Decompress from a file-object.
./encodec/compress.py:    Returns a tuple `(wav, sample_rate)`.
./encodec/compress.py:    Args:
./encodec/compress.py:        compressed (bytes): compressed bytes.
./encodec/compress.py:        device: device to use to perform the computations.
./encodec/compress.py:    """
./encodec/compress.py:    fo = io.BytesIO(compressed)
./encodec/compress.py:    return decompress_from_file(fo, device=device)
./encodec/compress.py:def test():
./encodec/compress.py:    import torchaudio
./encodec/compress.py:    torch.set_num_threads(1)
./encodec/compress.py:    for name in MODELS.keys():
./encodec/compress.py:        model = MODELS[name]()
./encodec/compress.py:        sr = model.sample_rate // 1000
./encodec/compress.py:        x, _ = torchaudio.load(f'test_{sr}k.wav')
./encodec/compress.py:        x = x[:, :model.sample_rate * 5]
./encodec/compress.py:        model.set_target_bandwidth(12)
./encodec/compress.py:        for use_lm in [False, True]:
./encodec/compress.py:            print(f"Doing {name}, use_lm={use_lm}")
./encodec/compress.py:            begin = time.time()
./encodec/compress.py:            res = compress(model, x, use_lm=use_lm)
./encodec/compress.py:            t_comp = time.time() - begin
./encodec/compress.py:            x_dec, _ = decompress(res)
./encodec/compress.py:            t_decomp = time.time() - begin - t_comp
./encodec/compress.py:            kbps = 8 * len(res) / 1000 / (x.shape[-1] / model.sample_rate)
./encodec/compress.py:            print(f"kbps: {kbps:.1f}, time comp: {t_comp:.1f} sec. "
./encodec/compress.py:                  f"time decomp:{t_decomp:.1f}.")
./encodec/compress.py:            assert x_dec.shape == x.shape
./encodec/compress.py:if __name__ == '__main__':
./encodec/compress.py:    test()
./encodec/modules/transformer.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/modules/transformer.py:# All rights reserved.
./encodec/modules/transformer.py:#
./encodec/modules/transformer.py:# This source code is licensed under the license found in the
./encodec/modules/transformer.py:# LICENSE file in the root directory of this source tree.
./encodec/modules/transformer.py:"""A streamable transformer."""
./encodec/modules/transformer.py:import typing as tp
./encodec/modules/transformer.py:import torch
./encodec/modules/transformer.py:import torch.nn as nn
./encodec/modules/transformer.py:import torch.nn.functional as F
./encodec/modules/transformer.py:def create_sin_embedding(positions: torch.Tensor, dim: int, max_period: float = 10000):
./encodec/modules/transformer.py:    """Create time embedding for the given positions, target dimension `dim`.
./encodec/modules/transformer.py:    """
./encodec/modules/transformer.py:    # We aim for BTC format
./encodec/modules/transformer.py:    assert dim % 2 == 0
./encodec/modules/transformer.py:    half_dim = dim // 2
./encodec/modules/transformer.py:    adim = torch.arange(half_dim, device=positions.device).view(1, 1, -1)
./encodec/modules/transformer.py:    phase = positions / (max_period ** (adim / (half_dim - 1)))
./encodec/modules/transformer.py:    return torch.cat([
./encodec/modules/transformer.py:        torch.cos(phase),
./encodec/modules/transformer.py:        torch.sin(phase),
./encodec/modules/transformer.py:    ], dim=-1)
./encodec/modules/transformer.py:class StreamingTransformerEncoderLayer(nn.TransformerEncoderLayer):
./encodec/modules/transformer.py:    def forward(self, x: torch.Tensor, x_past: torch.Tensor, past_context: int):  # type: ignore
./encodec/modules/transformer.py:        if self.norm_first:
./encodec/modules/transformer.py:            sa_input = self.norm1(x)
./encodec/modules/transformer.py:            x = x + self._sa_block(sa_input, x_past, past_context)
./encodec/modules/transformer.py:            x = x + self._ff_block(self.norm2(x))
./encodec/modules/transformer.py:        else:
./encodec/modules/transformer.py:            sa_input = x
./encodec/modules/transformer.py:            x = self.norm1(x + self._sa_block(sa_input, x_past, past_context))
./encodec/modules/transformer.py:            x = self.norm2(x + self._ff_block(x))
./encodec/modules/transformer.py:        return x, sa_input
./encodec/modules/transformer.py:    # self-attention block
./encodec/modules/transformer.py:    def _sa_block(self, x: torch.Tensor, x_past: torch.Tensor, past_context: int):  # type: ignore
./encodec/modules/transformer.py:        _, T, _ = x.shape
./encodec/modules/transformer.py:        _, H, _ = x_past.shape
./encodec/modules/transformer.py:        queries = x
./encodec/modules/transformer.py:        keys = torch.cat([x_past, x], dim=1)
./encodec/modules/transformer.py:        values = keys
./encodec/modules/transformer.py:        queries_pos = torch.arange(H, T + H, device=x.device).view(-1, 1)
./encodec/modules/transformer.py:        keys_pos = torch.arange(T + H, device=x.device).view(1, -1)
./encodec/modules/transformer.py:        delta = queries_pos - keys_pos
./encodec/modules/transformer.py:        valid_access = (delta >= 0) & (delta <= past_context)
./encodec/modules/transformer.py:        x = self.self_attn(queries, keys, values,
./encodec/modules/transformer.py:                           attn_mask=~valid_access,
./encodec/modules/transformer.py:                           need_weights=False)[0]
./encodec/modules/transformer.py:        return self.dropout1(x)
./encodec/modules/transformer.py:class StreamingTransformerEncoder(nn.Module):
./encodec/modules/transformer.py:    """TransformerEncoder with streaming support.
./encodec/modules/transformer.py:    Args:
./encodec/modules/transformer.py:        dim (int): dimension of the data.
./encodec/modules/transformer.py:        hidden_scale (int): intermediate dimension of FF module is this times the dimension.
./encodec/modules/transformer.py:        num_heads (int): number of heads.
./encodec/modules/transformer.py:        num_layers (int): number of layers.
./encodec/modules/transformer.py:        max_period (float): maxium period of cosines in the positional embedding.
./encodec/modules/transformer.py:        past_context (int or None): receptive field for the causal mask, infinite if None.
./encodec/modules/transformer.py:        gelu (bool): if true uses GeLUs, otherwise use ReLUs.
./encodec/modules/transformer.py:        norm_in (bool): normalize the input.
./encodec/modules/transformer.py:        dropout (float): dropout probability.
./encodec/modules/transformer.py:        **kwargs: See `nn.TransformerEncoderLayer`.
./encodec/modules/transformer.py:    """
./encodec/modules/transformer.py:    def __init__(self, dim, hidden_scale: float = 4., num_heads: int = 8, num_layers: int = 5,
./encodec/modules/transformer.py:                 max_period: float = 10000, past_context: int = 1000, gelu: bool = True,
./encodec/modules/transformer.py:                 norm_in: bool = True, dropout: float = 0., **kwargs):
./encodec/modules/transformer.py:        super().__init__()
./encodec/modules/transformer.py:        assert dim % num_heads == 0
./encodec/modules/transformer.py:        hidden_dim = int(dim * hidden_scale)
./encodec/modules/transformer.py:        self.max_period = max_period
./encodec/modules/transformer.py:        self.past_context = past_context
./encodec/modules/transformer.py:        activation: tp.Any = F.gelu if gelu else F.relu
./encodec/modules/transformer.py:        self.norm_in: nn.Module
./encodec/modules/transformer.py:        if norm_in:
./encodec/modules/transformer.py:            self.norm_in = nn.LayerNorm(dim)
./encodec/modules/transformer.py:        else:
./encodec/modules/transformer.py:            self.norm_in = nn.Identity()
./encodec/modules/transformer.py:        self.layers = nn.ModuleList()
./encodec/modules/transformer.py:        for idx in range(num_layers):
./encodec/modules/transformer.py:            self.layers.append(
./encodec/modules/transformer.py:                StreamingTransformerEncoderLayer(
./encodec/modules/transformer.py:                    dim, num_heads, hidden_dim,
./encodec/modules/transformer.py:                    activation=activation, batch_first=True, dropout=dropout, **kwargs))
./encodec/modules/transformer.py:    def forward(self, x: torch.Tensor,
./encodec/modules/transformer.py:                states: tp.Optional[tp.List[torch.Tensor]] = None,
./encodec/modules/transformer.py:                offset: tp.Union[int, torch.Tensor] = 0):
./encodec/modules/transformer.py:        B, T, C = x.shape
./encodec/modules/transformer.py:        if states is None:
./encodec/modules/transformer.py:            states = [torch.zeros_like(x[:, :1]) for _ in range(1 + len(self.layers))]
./encodec/modules/transformer.py:        positions = torch.arange(T, device=x.device).view(1, -1, 1) + offset
./encodec/modules/transformer.py:        pos_emb = create_sin_embedding(positions, C, max_period=self.max_period)
./encodec/modules/transformer.py:        new_state: tp.List[torch.Tensor] = []
./encodec/modules/transformer.py:        x = self.norm_in(x)
./encodec/modules/transformer.py:        x = x + pos_emb
./encodec/modules/transformer.py:        for layer_state, layer in zip(states, self.layers):
./encodec/modules/transformer.py:            x, new_layer_state = layer(x, layer_state, self.past_context)
./encodec/modules/transformer.py:            new_layer_state = torch.cat([layer_state, new_layer_state], dim=1)
./encodec/modules/transformer.py:            new_state.append(new_layer_state[:, -self.past_context:, :])
./encodec/modules/transformer.py:        return x, new_state, offset + T
./encodec/modules/__init__.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/modules/__init__.py:# All rights reserved.
./encodec/modules/__init__.py:#
./encodec/modules/__init__.py:# This source code is licensed under the license found in the
./encodec/modules/__init__.py:# LICENSE file in the root directory of this source tree.
./encodec/modules/__init__.py:"""Torch modules."""
./encodec/modules/__init__.py:# flake8: noqa
./encodec/modules/__init__.py:from .conv import (
./encodec/modules/__init__.py:    pad1d,
./encodec/modules/__init__.py:    unpad1d,
./encodec/modules/__init__.py:    NormConv1d,
./encodec/modules/__init__.py:    NormConvTranspose1d,
./encodec/modules/__init__.py:    NormConv2d,
./encodec/modules/__init__.py:    NormConvTranspose2d,
./encodec/modules/__init__.py:    SConv1d,
./encodec/modules/__init__.py:    SConvTranspose1d,
./encodec/modules/__init__.py:)
./encodec/modules/__init__.py:from .lstm import SLSTM
./encodec/modules/__init__.py:from .seanet import SEANetEncoder, SEANetDecoder
./encodec/modules/__init__.py:from .transformer import StreamingTransformerEncoder
./encodec/modules/norm.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/modules/norm.py:# All rights reserved.
./encodec/modules/norm.py:#
./encodec/modules/norm.py:# This source code is licensed under the license found in the
./encodec/modules/norm.py:# LICENSE file in the root directory of this source tree.
./encodec/modules/norm.py:"""Normalization modules."""
./encodec/modules/norm.py:import typing as tp
./encodec/modules/norm.py:import einops
./encodec/modules/norm.py:import torch
./encodec/modules/norm.py:from torch import nn
./encodec/modules/norm.py:class ConvLayerNorm(nn.LayerNorm):
./encodec/modules/norm.py:    """
./encodec/modules/norm.py:    Convolution-friendly LayerNorm that moves channels to last dimensions
./encodec/modules/norm.py:    before running the normalization and moves them back to original position right after.
./encodec/modules/norm.py:    """
./encodec/modules/norm.py:    def __init__(self, normalized_shape: tp.Union[int, tp.List[int], torch.Size], **kwargs):
./encodec/modules/norm.py:        super().__init__(normalized_shape, **kwargs)
./encodec/modules/norm.py:    def forward(self, x):
./encodec/modules/norm.py:        x = einops.rearrange(x, 'b ... t -> b t ...')
./encodec/modules/norm.py:        x = super().forward(x)
./encodec/modules/norm.py:        x = einops.rearrange(x, 'b t ... -> b ... t')
./encodec/modules/norm.py:        return
./encodec/modules/lstm.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/modules/lstm.py:# All rights reserved.
./encodec/modules/lstm.py:#
./encodec/modules/lstm.py:# This source code is licensed under the license found in the
./encodec/modules/lstm.py:# LICENSE file in the root directory of this source tree.
./encodec/modules/lstm.py:"""LSTM layers module."""
./encodec/modules/lstm.py:from torch import nn
./encodec/modules/lstm.py:class SLSTM(nn.Module):
./encodec/modules/lstm.py:    """
./encodec/modules/lstm.py:    LSTM without worrying about the hidden state, nor the layout of the data.
./encodec/modules/lstm.py:    Expects input as convolutional layout.
./encodec/modules/lstm.py:    """
./encodec/modules/lstm.py:    def __init__(self, dimension: int, num_layers: int = 2, skip: bool = True):
./encodec/modules/lstm.py:        super().__init__()
./encodec/modules/lstm.py:        self.skip = skip
./encodec/modules/lstm.py:        self.lstm = nn.LSTM(dimension, dimension, num_layers)
./encodec/modules/lstm.py:    def forward(self, x):
./encodec/modules/lstm.py:        x = x.permute(2, 0, 1)
./encodec/modules/lstm.py:        y, _ = self.lstm(x)
./encodec/modules/lstm.py:        if self.skip:
./encodec/modules/lstm.py:            y = y + x
./encodec/modules/lstm.py:        y = y.permute(1, 2, 0)
./encodec/modules/lstm.py:        return y
./encodec/modules/seanet.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/modules/seanet.py:# All rights reserved.
./encodec/modules/seanet.py:#
./encodec/modules/seanet.py:# This source code is licensed under the license found in the
./encodec/modules/seanet.py:# LICENSE file in the root directory of this source tree.
./encodec/modules/seanet.py:"""Encodec SEANet-based encoder and decoder implementation."""
./encodec/modules/seanet.py:import typing as tp
./encodec/modules/seanet.py:import numpy as np
./encodec/modules/seanet.py:import torch.nn as nn
./encodec/modules/seanet.py:from . import (
./encodec/modules/seanet.py:    SConv1d,
./encodec/modules/seanet.py:    SConvTranspose1d,
./encodec/modules/seanet.py:    SLSTM
./encodec/modules/seanet.py:)
./encodec/modules/seanet.py:class SEANetResnetBlock(nn.Module):
./encodec/modules/seanet.py:    """Residual block from SEANet model.
./encodec/modules/seanet.py:    Args:
./encodec/modules/seanet.py:        dim (int): Dimension of the input/output
./encodec/modules/seanet.py:        kernel_sizes (list): List of kernel sizes for the convolutions.
./encodec/modules/seanet.py:        dilations (list): List of dilations for the convolutions.
./encodec/modules/seanet.py:        activation (str): Activation function.
./encodec/modules/seanet.py:        activation_params (dict): Parameters to provide to the activation function
./encodec/modules/seanet.py:        norm (str): Normalization method.
./encodec/modules/seanet.py:        norm_params (dict): Parameters to provide to the underlying normalization used along with the convolution.
./encodec/modules/seanet.py:        causal (bool): Whether to use fully causal convolution.
./encodec/modules/seanet.py:        pad_mode (str): Padding mode for the convolutions.
./encodec/modules/seanet.py:        compress (int): Reduced dimensionality in residual branches (from Demucs v3)
./encodec/modules/seanet.py:        true_skip (bool): Whether to use true skip connection or a simple convolution as the skip connection.
./encodec/modules/seanet.py:    """
./encodec/modules/seanet.py:    def __init__(self, dim: int, kernel_sizes: tp.List[int] = [3, 1], dilations: tp.List[int] = [1, 1],
./encodec/modules/seanet.py:                 activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},
./encodec/modules/seanet.py:                 norm: str = 'weight_norm', norm_params: tp.Dict[str, tp.Any] = {}, causal: bool = False,
./encodec/modules/seanet.py:                 pad_mode: str = 'reflect', compress: int = 2, true_skip: bool = True):
./encodec/modules/seanet.py:        super().__init__()
./encodec/modules/seanet.py:        assert len(kernel_sizes) == len(dilations), 'Number of kernel sizes should match number of dilations'
./encodec/modules/seanet.py:        act = getattr(nn, activation)
./encodec/modules/seanet.py:        hidden = dim // compress
./encodec/modules/seanet.py:        block = []
./encodec/modules/seanet.py:        for i, (kernel_size, dilation) in enumerate(zip(kernel_sizes, dilations)):
./encodec/modules/seanet.py:            in_chs = dim if i == 0 else hidden
./encodec/modules/seanet.py:            out_chs = dim if i == len(kernel_sizes) - 1 else hidden
./encodec/modules/seanet.py:            block += [
./encodec/modules/seanet.py:                act(**activation_params),
./encodec/modules/seanet.py:                SConv1d(in_chs, out_chs, kernel_size=kernel_size, dilation=dilation,
./encodec/modules/seanet.py:                        norm=norm, norm_kwargs=norm_params,
./encodec/modules/seanet.py:                        causal=causal, pad_mode=pad_mode),
./encodec/modules/seanet.py:            ]
./encodec/modules/seanet.py:        self.block = nn.Sequential(*block)
./encodec/modules/seanet.py:        self.shortcut: nn.Module
./encodec/modules/seanet.py:        if true_skip:
./encodec/modules/seanet.py:            self.shortcut = nn.Identity()
./encodec/modules/seanet.py:        else:
./encodec/modules/seanet.py:            self.shortcut = SConv1d(dim, dim, kernel_size=1, norm=norm, norm_kwargs=norm_params,
./encodec/modules/seanet.py:                                    causal=causal, pad_mode=pad_mode)
./encodec/modules/seanet.py:    def forward(self, x):
./encodec/modules/seanet.py:        return self.shortcut(x) + self.block(x)
./encodec/modules/seanet.py:class SEANetEncoder(nn.Module):
./encodec/modules/seanet.py:    """SEANet encoder.
./encodec/modules/seanet.py:    Args:
./encodec/modules/seanet.py:        channels (int): Audio channels.
./encodec/modules/seanet.py:        dimension (int): Intermediate representation dimension.
./encodec/modules/seanet.py:        n_filters (int): Base width for the model.
./encodec/modules/seanet.py:        n_residual_layers (int): nb of residual layers.
./encodec/modules/seanet.py:        ratios (Sequence[int]): kernel size and stride ratios. The encoder uses downsampling ratios instead of
./encodec/modules/seanet.py:            upsampling ratios, hence it will use the ratios in the reverse order to the ones specified here
./encodec/modules/seanet.py:            that must match the decoder order
./encodec/modules/seanet.py:        activation (str): Activation function.
./encodec/modules/seanet.py:        activation_params (dict): Parameters to provide to the activation function
./encodec/modules/seanet.py:        norm (str): Normalization method.
./encodec/modules/seanet.py:        norm_params (dict): Parameters to provide to the underlying normalization used along with the convolution.
./encodec/modules/seanet.py:        kernel_size (int): Kernel size for the initial convolution.
./encodec/modules/seanet.py:        last_kernel_size (int): Kernel size for the initial convolution.
./encodec/modules/seanet.py:        residual_kernel_size (int): Kernel size for the residual layers.
./encodec/modules/seanet.py:        dilation_base (int): How much to increase the dilation with each layer.
./encodec/modules/seanet.py:        causal (bool): Whether to use fully causal convolution.
./encodec/modules/seanet.py:        pad_mode (str): Padding mode for the convolutions.
./encodec/modules/seanet.py:        true_skip (bool): Whether to use true skip connection or a simple
./encodec/modules/seanet.py:            (streamable) convolution as the skip connection in the residual network blocks.
./encodec/modules/seanet.py:        compress (int): Reduced dimensionality in residual branches (from Demucs v3).
./encodec/modules/seanet.py:        lstm (int): Number of LSTM layers at the end of the encoder.
./encodec/modules/seanet.py:    """
./encodec/modules/seanet.py:    def __init__(self, channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 1,
./encodec/modules/seanet.py:                 ratios: tp.List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},
./encodec/modules/seanet.py:                 norm: str = 'weight_norm', norm_params: tp.Dict[str, tp.Any] = {}, kernel_size: int = 7,
./encodec/modules/seanet.py:                 last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False,
./encodec/modules/seanet.py:                 pad_mode: str = 'reflect', true_skip: bool = False, compress: int = 2, lstm: int = 2):
./encodec/modules/seanet.py:        super().__init__()
./encodec/modules/seanet.py:        self.channels = channels
./encodec/modules/seanet.py:        self.dimension = dimension
./encodec/modules/seanet.py:        self.n_filters = n_filters
./encodec/modules/seanet.py:        self.ratios = list(reversed(ratios))
./encodec/modules/seanet.py:        del ratios
./encodec/modules/seanet.py:        self.n_residual_layers = n_residual_layers
./encodec/modules/seanet.py:        self.hop_length = np.prod(self.ratios)
./encodec/modules/seanet.py:        act = getattr(nn, activation)
./encodec/modules/seanet.py:        mult = 1
./encodec/modules/seanet.py:        model: tp.List[nn.Module] = [
./encodec/modules/seanet.py:            SConv1d(channels, mult * n_filters, kernel_size, norm=norm, norm_kwargs=norm_params,
./encodec/modules/seanet.py:                    causal=causal, pad_mode=pad_mode)
./encodec/modules/seanet.py:        ]
./encodec/modules/seanet.py:        # Downsample to raw audio scale
./encodec/modules/seanet.py:        for i, ratio in enumerate(self.ratios):
./encodec/modules/seanet.py:            # Add residual layers
./encodec/modules/seanet.py:            for j in range(n_residual_layers):
./encodec/modules/seanet.py:                model += [
./encodec/modules/seanet.py:                    SEANetResnetBlock(mult * n_filters, kernel_sizes=[residual_kernel_size, 1],
./encodec/modules/seanet.py:                                      dilations=[dilation_base ** j, 1],
./encodec/modules/seanet.py:                                      norm=norm, norm_params=norm_params,
./encodec/modules/seanet.py:                                      activation=activation, activation_params=activation_params,
./encodec/modules/seanet.py:                                      causal=causal, pad_mode=pad_mode, compress=compress, true_skip=true_skip)]
./encodec/modules/seanet.py:            # Add downsampling layers
./encodec/modules/seanet.py:            model += [
./encodec/modules/seanet.py:                act(**activation_params),
./encodec/modules/seanet.py:                SConv1d(mult * n_filters, mult * n_filters * 2,
./encodec/modules/seanet.py:                        kernel_size=ratio * 2, stride=ratio,
./encodec/modules/seanet.py:                        norm=norm, norm_kwargs=norm_params,
./encodec/modules/seanet.py:                        causal=causal, pad_mode=pad_mode),
./encodec/modules/seanet.py:            ]
./encodec/modules/seanet.py:            mult *= 2
./encodec/modules/seanet.py:        if lstm:
./encodec/modules/seanet.py:            model += [SLSTM(mult * n_filters, num_layers=lstm)]
./encodec/modules/seanet.py:        model += [
./encodec/modules/seanet.py:            act(**activation_params),
./encodec/modules/seanet.py:            SConv1d(mult * n_filters, dimension, last_kernel_size, norm=norm, norm_kwargs=norm_params,
./encodec/modules/seanet.py:                    causal=causal, pad_mode=pad_mode)
./encodec/modules/seanet.py:        ]
./encodec/modules/seanet.py:        self.model = nn.Sequential(*model)
./encodec/modules/seanet.py:    def forward(self, x):
./encodec/modules/seanet.py:        return self.model(x)
./encodec/modules/seanet.py:class SEANetDecoder(nn.Module):
./encodec/modules/seanet.py:    """SEANet decoder.
./encodec/modules/seanet.py:    Args:
./encodec/modules/seanet.py:        channels (int): Audio channels.
./encodec/modules/seanet.py:        dimension (int): Intermediate representation dimension.
./encodec/modules/seanet.py:        n_filters (int): Base width for the model.
./encodec/modules/seanet.py:        n_residual_layers (int): nb of residual layers.
./encodec/modules/seanet.py:        ratios (Sequence[int]): kernel size and stride ratios
./encodec/modules/seanet.py:        activation (str): Activation function.
./encodec/modules/seanet.py:        activation_params (dict): Parameters to provide to the activation function
./encodec/modules/seanet.py:        final_activation (str): Final activation function after all convolutions.
./encodec/modules/seanet.py:        final_activation_params (dict): Parameters to provide to the activation function
./encodec/modules/seanet.py:        norm (str): Normalization method.
./encodec/modules/seanet.py:        norm_params (dict): Parameters to provide to the underlying normalization used along with the convolution.
./encodec/modules/seanet.py:        kernel_size (int): Kernel size for the initial convolution.
./encodec/modules/seanet.py:        last_kernel_size (int): Kernel size for the initial convolution.
./encodec/modules/seanet.py:        residual_kernel_size (int): Kernel size for the residual layers.
./encodec/modules/seanet.py:        dilation_base (int): How much to increase the dilation with each layer.
./encodec/modules/seanet.py:        causal (bool): Whether to use fully causal convolution.
./encodec/modules/seanet.py:        pad_mode (str): Padding mode for the convolutions.
./encodec/modules/seanet.py:        true_skip (bool): Whether to use true skip connection or a simple
./encodec/modules/seanet.py:            (streamable) convolution as the skip connection in the residual network blocks.
./encodec/modules/seanet.py:        compress (int): Reduced dimensionality in residual branches (from Demucs v3).
./encodec/modules/seanet.py:        lstm (int): Number of LSTM layers at the end of the encoder.
./encodec/modules/seanet.py:        trim_right_ratio (float): Ratio for trimming at the right of the transposed convolution under the causal setup.
./encodec/modules/seanet.py:            If equal to 1.0, it means that all the trimming is done at the right.
./encodec/modules/seanet.py:    """
./encodec/modules/seanet.py:    def __init__(self, channels: int = 1, dimension: int = 128, n_filters: int = 32, n_residual_layers: int = 1,
./encodec/modules/seanet.py:                 ratios: tp.List[int] = [8, 5, 4, 2], activation: str = 'ELU', activation_params: dict = {'alpha': 1.0},
./encodec/modules/seanet.py:                 final_activation: tp.Optional[str] = None, final_activation_params: tp.Optional[dict] = None,
./encodec/modules/seanet.py:                 norm: str = 'weight_norm', norm_params: tp.Dict[str, tp.Any] = {}, kernel_size: int = 7,
./encodec/modules/seanet.py:                 last_kernel_size: int = 7, residual_kernel_size: int = 3, dilation_base: int = 2, causal: bool = False,
./encodec/modules/seanet.py:                 pad_mode: str = 'reflect', true_skip: bool = False, compress: int = 2, lstm: int = 2,
./encodec/modules/seanet.py:                 trim_right_ratio: float = 1.0):
./encodec/modules/seanet.py:        super().__init__()
./encodec/modules/seanet.py:        self.dimension = dimension
./encodec/modules/seanet.py:        self.channels = channels
./encodec/modules/seanet.py:        self.n_filters = n_filters
./encodec/modules/seanet.py:        self.ratios = ratios
./encodec/modules/seanet.py:        del ratios
./encodec/modules/seanet.py:        self.n_residual_layers = n_residual_layers
./encodec/modules/seanet.py:        self.hop_length = np.prod(self.ratios)
./encodec/modules/seanet.py:        act = getattr(nn, activation)
./encodec/modules/seanet.py:        mult = int(2 ** len(self.ratios))
./encodec/modules/seanet.py:        model: tp.List[nn.Module] = [
./encodec/modules/seanet.py:            SConv1d(dimension, mult * n_filters, kernel_size, norm=norm, norm_kwargs=norm_params,
./encodec/modules/seanet.py:                    causal=causal, pad_mode=pad_mode)
./encodec/modules/seanet.py:        ]
./encodec/modules/seanet.py:        if lstm:
./encodec/modules/seanet.py:            model += [SLSTM(mult * n_filters, num_layers=lstm)]
./encodec/modules/seanet.py:        # Upsample to raw audio scale
./encodec/modules/seanet.py:        for i, ratio in enumerate(self.ratios):
./encodec/modules/seanet.py:            # Add upsampling layers
./encodec/modules/seanet.py:            model += [
./encodec/modules/seanet.py:                act(**activation_params),
./encodec/modules/seanet.py:                SConvTranspose1d(mult * n_filters, mult * n_filters // 2,
./encodec/modules/seanet.py:                                 kernel_size=ratio * 2, stride=ratio,
./encodec/modules/seanet.py:                                 norm=norm, norm_kwargs=norm_params,
./encodec/modules/seanet.py:                                 causal=causal, trim_right_ratio=trim_right_ratio),
./encodec/modules/seanet.py:            ]
./encodec/modules/seanet.py:            # Add residual layers
./encodec/modules/seanet.py:            for j in range(n_residual_layers):
./encodec/modules/seanet.py:                model += [
./encodec/modules/seanet.py:                    SEANetResnetBlock(mult * n_filters // 2, kernel_sizes=[residual_kernel_size, 1],
./encodec/modules/seanet.py:                                      dilations=[dilation_base ** j, 1],
./encodec/modules/seanet.py:                                      activation=activation, activation_params=activation_params,
./encodec/modules/seanet.py:                                      norm=norm, norm_params=norm_params, causal=causal,
./encodec/modules/seanet.py:                                      pad_mode=pad_mode, compress=compress, true_skip=true_skip)]
./encodec/modules/seanet.py:            mult //= 2
./encodec/modules/seanet.py:        # Add final layers
./encodec/modules/seanet.py:        model += [
./encodec/modules/seanet.py:            act(**activation_params),
./encodec/modules/seanet.py:            SConv1d(n_filters, channels, last_kernel_size, norm=norm, norm_kwargs=norm_params,
./encodec/modules/seanet.py:                    causal=causal, pad_mode=pad_mode)
./encodec/modules/seanet.py:        ]
./encodec/modules/seanet.py:        # Add optional final activation to decoder (eg. tanh)
./encodec/modules/seanet.py:        if final_activation is not None:
./encodec/modules/seanet.py:            final_act = getattr(nn, final_activation)
./encodec/modules/seanet.py:            final_activation_params = final_activation_params or {}
./encodec/modules/seanet.py:            model += [
./encodec/modules/seanet.py:                final_act(**final_activation_params)
./encodec/modules/seanet.py:            ]
./encodec/modules/seanet.py:        self.model = nn.Sequential(*model)
./encodec/modules/seanet.py:    def forward(self, z):
./encodec/modules/seanet.py:        y = self.model(z)
./encodec/modules/seanet.py:        return y
./encodec/modules/seanet.py:def test():
./encodec/modules/seanet.py:    import torch
./encodec/modules/seanet.py:    encoder = SEANetEncoder()
./encodec/modules/seanet.py:    decoder = SEANetDecoder()
./encodec/modules/seanet.py:    x = torch.randn(1, 1, 24000)
./encodec/modules/seanet.py:    z = encoder(x)
./encodec/modules/seanet.py:    assert list(z.shape) == [1, 128, 75], z.shape
./encodec/modules/seanet.py:    y = decoder(z)
./encodec/modules/seanet.py:    assert y.shape == x.shape, (x.shape, y.shape)
./encodec/modules/seanet.py:if __name__ == '__main__':
./encodec/modules/seanet.py:    test()
./encodec/modules/conv.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./encodec/modules/conv.py:# All rights reserved.
./encodec/modules/conv.py:#
./encodec/modules/conv.py:# This source code is licensed under the license found in the
./encodec/modules/conv.py:# LICENSE file in the root directory of this source tree.
./encodec/modules/conv.py:"""Convolutional layers wrappers and utilities."""
./encodec/modules/conv.py:import math
./encodec/modules/conv.py:import typing as tp
./encodec/modules/conv.py:import warnings
./encodec/modules/conv.py:import torch
./encodec/modules/conv.py:from torch import nn
./encodec/modules/conv.py:from torch.nn import functional as F
./encodec/modules/conv.py:from torch.nn.utils import spectral_norm, weight_norm
./encodec/modules/conv.py:from .norm import ConvLayerNorm
./encodec/modules/conv.py:CONV_NORMALIZATIONS = frozenset(['none', 'weight_norm', 'spectral_norm',
./encodec/modules/conv.py:                                 'time_layer_norm', 'layer_norm', 'time_group_norm'])
./encodec/modules/conv.py:def apply_parametrization_norm(module: nn.Module, norm: str = 'none') -> nn.Module:
./encodec/modules/conv.py:    assert norm in CONV_NORMALIZATIONS
./encodec/modules/conv.py:    if norm == 'weight_norm':
./encodec/modules/conv.py:        return weight_norm(module)
./encodec/modules/conv.py:    elif norm == 'spectral_norm':
./encodec/modules/conv.py:        return spectral_norm(module)
./encodec/modules/conv.py:    else:
./encodec/modules/conv.py:        # We already check was in CONV_NORMALIZATION, so any other choice
./encodec/modules/conv.py:        # doesn't need reparametrization.
./encodec/modules/conv.py:        return module
./encodec/modules/conv.py:def get_norm_module(module: nn.Module, causal: bool = False, norm: str = 'none', **norm_kwargs) -> nn.Module:
./encodec/modules/conv.py:    """Return the proper normalization module. If causal is True, this will ensure the returned
./encodec/modules/conv.py:    module is causal, or return an error if the normalization doesn't support causal evaluation.
./encodec/modules/conv.py:    """
./encodec/modules/conv.py:    assert norm in CONV_NORMALIZATIONS
./encodec/modules/conv.py:    if norm == 'layer_norm':
./encodec/modules/conv.py:        assert isinstance(module, nn.modules.conv._ConvNd)
./encodec/modules/conv.py:        return ConvLayerNorm(module.out_channels, **norm_kwargs)
./encodec/modules/conv.py:    elif norm == 'time_group_norm':
./encodec/modules/conv.py:        if causal:
./encodec/modules/conv.py:            raise ValueError("GroupNorm doesn't support causal evaluation.")
./encodec/modules/conv.py:        assert isinstance(module, nn.modules.conv._ConvNd)
./encodec/modules/conv.py:        return nn.GroupNorm(1, module.out_channels, **norm_kwargs)
./encodec/modules/conv.py:    else:
./encodec/modules/conv.py:        return nn.Identity()
./encodec/modules/conv.py:def get_extra_padding_for_conv1d(x: torch.Tensor, kernel_size: int, stride: int,
./encodec/modules/conv.py:                                 padding_total: int = 0) -> int:
./encodec/modules/conv.py:    """See `pad_for_conv1d`.
./encodec/modules/conv.py:    """
./encodec/modules/conv.py:    length = x.shape[-1]
./encodec/modules/conv.py:    n_frames = (length - kernel_size + padding_total) / stride + 1
./encodec/modules/conv.py:    ideal_length = (math.ceil(n_frames) - 1) * stride + (kernel_size - padding_total)
./encodec/modules/conv.py:    return ideal_length - length
./encodec/modules/conv.py:def pad_for_conv1d(x: torch.Tensor, kernel_size: int, stride: int, padding_total: int = 0):
./encodec/modules/conv.py:    """Pad for a convolution to make sure that the last window is full.
./encodec/modules/conv.py:    Extra padding is added at the end. This is required to ensure that we can rebuild
./encodec/modules/conv.py:    an output of the same length, as otherwise, even with padding, some time steps
./encodec/modules/conv.py:    might get removed.
./encodec/modules/conv.py:    For instance, with total padding = 4, kernel size = 4, stride = 2:
./encodec/modules/conv.py:        0 0 1 2 3 4 5 0 0   # (0s are padding)
./encodec/modules/conv.py:        1   2   3           # (output frames of a convolution, last 0 is never used)
./encodec/modules/conv.py:        0 0 1 2 3 4 5 0     # (output of tr. conv., but pos. 5 is going to get removed as padding)
./encodec/modules/conv.py:            1 2 3 4         # once you removed padding, we are missing one time step !
./encodec/modules/conv.py:    """
./encodec/modules/conv.py:    extra_padding = get_extra_padding_for_conv1d(x, kernel_size, stride, padding_total)
./encodec/modules/conv.py:    return F.pad(x, (0, extra_padding))
./encodec/modules/conv.py:def pad1d(x: torch.Tensor, paddings: tp.Tuple[int, int], mode: str = 'zero', value: float = 0.):
./encodec/modules/conv.py:    """Tiny wrapper around F.pad, just to allow for reflect padding on small input.
./encodec/modules/conv.py:    If this is the case, we insert extra 0 padding to the right before the reflection happen.
./encodec/modules/conv.py:    """
./encodec/modules/conv.py:    length = x.shape[-1]
./encodec/modules/conv.py:    padding_left, padding_right = paddings
./encodec/modules/conv.py:    assert padding_left >= 0 and padding_right >= 0, (padding_left, padding_right)
./encodec/modules/conv.py:    if mode == 'reflect':
./encodec/modules/conv.py:        max_pad = max(padding_left, padding_right)
./encodec/modules/conv.py:        extra_pad = 0
./encodec/modules/conv.py:        if length <= max_pad:
./encodec/modules/conv.py:            extra_pad = max_pad - length + 1
./encodec/modules/conv.py:            x = F.pad(x, (0, extra_pad))
./encodec/modules/conv.py:        padded = F.pad(x, paddings, mode, value)
./encodec/modules/conv.py:        end = padded.shape[-1] - extra_pad
./encodec/modules/conv.py:        return padded[..., :end]
./encodec/modules/conv.py:    else:
./encodec/modules/conv.py:        return F.pad(x, paddings, mode, value)
./encodec/modules/conv.py:def unpad1d(x: torch.Tensor, paddings: tp.Tuple[int, int]):
./encodec/modules/conv.py:    """Remove padding from x, handling properly zero padding. Only for 1d!"""
./encodec/modules/conv.py:    padding_left, padding_right = paddings
./encodec/modules/conv.py:    assert padding_left >= 0 and padding_right >= 0, (padding_left, padding_right)
./encodec/modules/conv.py:    assert (padding_left + padding_right) <= x.shape[-1]
./encodec/modules/conv.py:    end = x.shape[-1] - padding_right
./encodec/modules/conv.py:    return x[..., padding_left: end]
./encodec/modules/conv.py:class NormConv1d(nn.Module):
./encodec/modules/conv.py:    """Wrapper around Conv1d and normalization applied to this conv
./encodec/modules/conv.py:    to provide a uniform interface across normalization approaches.
./encodec/modules/conv.py:    """
./encodec/modules/conv.py:    def __init__(self, *args, causal: bool = False, norm: str = 'none',
./encodec/modules/conv.py:                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
./encodec/modules/conv.py:        super().__init__()
./encodec/modules/conv.py:        self.conv = apply_parametrization_norm(nn.Conv1d(*args, **kwargs), norm)
./encodec/modules/conv.py:        self.norm = get_norm_module(self.conv, causal, norm, **norm_kwargs)
./encodec/modules/conv.py:        self.norm_type = norm
./encodec/modules/conv.py:    def forward(self, x):
./encodec/modules/conv.py:        x = self.conv(x)
./encodec/modules/conv.py:        x = self.norm(x)
./encodec/modules/conv.py:        return x
./encodec/modules/conv.py:class NormConv2d(nn.Module):
./encodec/modules/conv.py:    """Wrapper around Conv2d and normalization applied to this conv
./encodec/modules/conv.py:    to provide a uniform interface across normalization approaches.
./encodec/modules/conv.py:    """
./encodec/modules/conv.py:    def __init__(self, *args, norm: str = 'none',
./encodec/modules/conv.py:                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
./encodec/modules/conv.py:        super().__init__()
./encodec/modules/conv.py:        self.conv = apply_parametrization_norm(nn.Conv2d(*args, **kwargs), norm)
./encodec/modules/conv.py:        self.norm = get_norm_module(self.conv, causal=False, norm=norm, **norm_kwargs)
./encodec/modules/conv.py:        self.norm_type = norm
./encodec/modules/conv.py:    def forward(self, x):
./encodec/modules/conv.py:        x = self.conv(x)
./encodec/modules/conv.py:        x = self.norm(x)
./encodec/modules/conv.py:        return x
./encodec/modules/conv.py:class NormConvTranspose1d(nn.Module):
./encodec/modules/conv.py:    """Wrapper around ConvTranspose1d and normalization applied to this conv
./encodec/modules/conv.py:    to provide a uniform interface across normalization approaches.
./encodec/modules/conv.py:    """
./encodec/modules/conv.py:    def __init__(self, *args, causal: bool = False, norm: str = 'none',
./encodec/modules/conv.py:                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
./encodec/modules/conv.py:        super().__init__()
./encodec/modules/conv.py:        self.convtr = apply_parametrization_norm(nn.ConvTranspose1d(*args, **kwargs), norm)
./encodec/modules/conv.py:        self.norm = get_norm_module(self.convtr, causal, norm, **norm_kwargs)
./encodec/modules/conv.py:        self.norm_type = norm
./encodec/modules/conv.py:    def forward(self, x):
./encodec/modules/conv.py:        x = self.convtr(x)
./encodec/modules/conv.py:        x = self.norm(x)
./encodec/modules/conv.py:        return x
./encodec/modules/conv.py:class NormConvTranspose2d(nn.Module):
./encodec/modules/conv.py:    """Wrapper around ConvTranspose2d and normalization applied to this conv
./encodec/modules/conv.py:    to provide a uniform interface across normalization approaches.
./encodec/modules/conv.py:    """
./encodec/modules/conv.py:    def __init__(self, *args, norm: str = 'none',
./encodec/modules/conv.py:                 norm_kwargs: tp.Dict[str, tp.Any] = {}, **kwargs):
./encodec/modules/conv.py:        super().__init__()
./encodec/modules/conv.py:        self.convtr = apply_parametrization_norm(nn.ConvTranspose2d(*args, **kwargs), norm)
./encodec/modules/conv.py:        self.norm = get_norm_module(self.convtr, causal=False, norm=norm, **norm_kwargs)
./encodec/modules/conv.py:    def forward(self, x):
./encodec/modules/conv.py:        x = self.convtr(x)
./encodec/modules/conv.py:        x = self.norm(x)
./encodec/modules/conv.py:        return x
./encodec/modules/conv.py:class SConv1d(nn.Module):
./encodec/modules/conv.py:    """Conv1d with some builtin handling of asymmetric or causal padding
./encodec/modules/conv.py:    and normalization.
./encodec/modules/conv.py:    """
./encodec/modules/conv.py:    def __init__(self, in_channels: int, out_channels: int,
./encodec/modules/conv.py:                 kernel_size: int, stride: int = 1, dilation: int = 1,
./encodec/modules/conv.py:                 groups: int = 1, bias: bool = True, causal: bool = False,
./encodec/modules/conv.py:                 norm: str = 'none', norm_kwargs: tp.Dict[str, tp.Any] = {},
./encodec/modules/conv.py:                 pad_mode: str = 'reflect'):
./encodec/modules/conv.py:        super().__init__()
./encodec/modules/conv.py:        # warn user on unusual setup between dilation and stride
./encodec/modules/conv.py:        if stride > 1 and dilation > 1:
./encodec/modules/conv.py:            warnings.warn('SConv1d has been initialized with stride > 1 and dilation > 1'
./encodec/modules/conv.py:                          f' (kernel_size={kernel_size} stride={stride}, dilation={dilation}).')
./encodec/modules/conv.py:        self.conv = NormConv1d(in_channels, out_channels, kernel_size, stride,
./encodec/modules/conv.py:                               dilation=dilation, groups=groups, bias=bias, causal=causal,
./encodec/modules/conv.py:                               norm=norm, norm_kwargs=norm_kwargs)
./encodec/modules/conv.py:        self.causal = causal
./encodec/modules/conv.py:        self.pad_mode = pad_mode
./encodec/modules/conv.py:    def forward(self, x):
./encodec/modules/conv.py:        B, C, T = x.shape
./encodec/modules/conv.py:        kernel_size = self.conv.conv.kernel_size[0]
./encodec/modules/conv.py:        stride = self.conv.conv.stride[0]
./encodec/modules/conv.py:        dilation = self.conv.conv.dilation[0]
./encodec/modules/conv.py:        kernel_size = (kernel_size - 1) * dilation + 1  # effective kernel size with dilations
./encodec/modules/conv.py:        padding_total = kernel_size - stride
./encodec/modules/conv.py:        extra_padding = get_extra_padding_for_conv1d(x, kernel_size, stride, padding_total)
./encodec/modules/conv.py:        if self.causal:
./encodec/modules/conv.py:            # Left padding for causal
./encodec/modules/conv.py:            x = pad1d(x, (padding_total, extra_padding), mode=self.pad_mode)
./encodec/modules/conv.py:        else:
./encodec/modules/conv.py:            # Asymmetric padding required for odd strides
./encodec/modules/conv.py:            padding_right = padding_total // 2
./encodec/modules/conv.py:            padding_left = padding_total - padding_right
./encodec/modules/conv.py:            x = pad1d(x, (padding_left, padding_right + extra_padding), mode=self.pad_mode)
./encodec/modules/conv.py:        return self.conv(x)
./encodec/modules/conv.py:class SConvTranspose1d(nn.Module):
./encodec/modules/conv.py:    """ConvTranspose1d with some builtin handling of asymmetric or causal padding
./encodec/modules/conv.py:    and normalization.
./encodec/modules/conv.py:    """
./encodec/modules/conv.py:    def __init__(self, in_channels: int, out_channels: int,
./encodec/modules/conv.py:                 kernel_size: int, stride: int = 1, causal: bool = False,
./encodec/modules/conv.py:                 norm: str = 'none', trim_right_ratio: float = 1.,
./encodec/modules/conv.py:                 norm_kwargs: tp.Dict[str, tp.Any] = {}):
./encodec/modules/conv.py:        super().__init__()
./encodec/modules/conv.py:        self.convtr = NormConvTranspose1d(in_channels, out_channels, kernel_size, stride,
./encodec/modules/conv.py:                                          causal=causal, norm=norm, norm_kwargs=norm_kwargs)
./encodec/modules/conv.py:        self.causal = causal
./encodec/modules/conv.py:        self.trim_right_ratio = trim_right_ratio
./encodec/modules/conv.py:        assert self.causal or self.trim_right_ratio == 1., \
./encodec/modules/conv.py:            "`trim_right_ratio` != 1.0 only makes sense for causal convolutions"
./encodec/modules/conv.py:        assert self.trim_right_ratio >= 0. and self.trim_right_ratio <= 1.
./encodec/modules/conv.py:    def forward(self, x):
./encodec/modules/conv.py:        kernel_size = self.convtr.convtr.kernel_size[0]
./encodec/modules/conv.py:        stride = self.convtr.convtr.stride[0]
./encodec/modules/conv.py:        padding_total = kernel_size - stride
./encodec/modules/conv.py:        y = self.convtr(x)
./encodec/modules/conv.py:        # We will only trim fixed padding. Extra padding from `pad_for_conv1d` would be
./encodec/modules/conv.py:        # removed at the very end, when keeping only the right length for the output,
./encodec/modules/conv.py:        # as removing it here would require also passing the length at the matching layer
./encodec/modules/conv.py:        # in the encoder.
./encodec/modules/conv.py:        if self.causal:
./encodec/modules/conv.py:            # Trim the padding on the right according to the specified ratio
./encodec/modules/conv.py:            # if trim_right_ratio = 1.0, trim everything from right
./encodec/modules/conv.py:            padding_right = math.ceil(padding_total * self.trim_right_ratio)
./encodec/modules/conv.py:            padding_left = padding_total - padding_right
./encodec/modules/conv.py:            y = unpad1d(y, (padding_left, padding_right))
./encodec/modules/conv.py:        else:
./encodec/modules/conv.py:            # Asymmetric padding required for odd strides
./encodec/modules/conv.py:            padding_right = padding_total // 2
./encodec/modules/conv.py:            padding_left = padding_total - padding_right
./encodec/modules/conv.py:            y = unpad1d(y, (padding_left, padding_right))
./encodec/modules/conv.py:        return y
./benchmark.py:# Copyright (c) Meta Platforms, Inc. and affiliates.
./benchmark.py:# All rights reserved.
./benchmark.py:#
./benchmark.py:# This source code is licensed under the license found in the
./benchmark.py:# LICENSE file in the root directory of this source tree.
./benchmark.py:"""Benchmark script."""
./benchmark.py:import io
./benchmark.py:import time
./benchmark.py:import torch
./benchmark.py:import torchaudio
./benchmark.py:from encodec.model import EncodecModel
./benchmark.py:from encodec.quantization.ac import ArithmeticCoder, ArithmeticDecoder, build_stable_quantized_cdf
./benchmark.py:def _timer():
./benchmark.py:    last = time.perf_counter()
./benchmark.py:    def _measure():
./benchmark.py:        nonlocal last
./benchmark.py:        result = time.perf_counter() - last
./benchmark.py:        last += result
./benchmark.py:        return result
./benchmark.py:    return _measure
./benchmark.py:def main():
./benchmark.py:    torch.set_num_threads(1)
./benchmark.py:    model_lq = EncodecModel.encodec_model_24khz()
./benchmark.py:    model_hq = EncodecModel.encodec_model_48khz()
./benchmark.py:    for model in [model_lq, model_hq]:
./benchmark.py:        sr = model.sample_rate // 1000
./benchmark.py:        x, _ = torchaudio.load(f'test_{sr}k.wav')
./benchmark.py:        x = x[None, :, :model.sample_rate * 10]
./benchmark.py:        model.set_target_bandwidth(12)
./benchmark.py:        lm = model.get_lm_model()
./benchmark.py:        timer = _timer()
./benchmark.py:        with torch.no_grad():
./benchmark.py:            frames = model.encode(x)
./benchmark.py:        print("Time to encode: ", timer())
./benchmark.py:        codes = torch.cat([f for f, _ in frames], dim=-1)
./benchmark.py:        _, K, T = codes.shape
./benchmark.py:        offset = 0
./benchmark.py:        input_ = torch.zeros(1, K, 1, dtype=torch.long, device=x.device)
./benchmark.py:        probas = torch.zeros(1, lm.card, K, T)
./benchmark.py:        offset = 0
./benchmark.py:        states = None
./benchmark.py:        for t in range(T):
./benchmark.py:            with torch.no_grad():
./benchmark.py:                probas[:, :, :, t: t + 1], states, offset = lm(input_, states, offset)
./benchmark.py:            input_ = codes[:, :, t: t + 1] + 1
./benchmark.py:        print("Time to eval LM:", timer())
./benchmark.py:        fo = io.BytesIO()
./benchmark.py:        coder = ArithmeticCoder(fo)
./benchmark.py:        for t in range(T):
./benchmark.py:            for k, value in enumerate(codes[0, :, t].tolist()):
./benchmark.py:                q_cdf = build_stable_quantized_cdf(
./benchmark.py:                    probas[0, :, k, t], coder.total_range_bits, check=False)
./benchmark.py:                coder.push(value, q_cdf)
./benchmark.py:        print("Time to AC enc.:", timer())
./benchmark.py:        decoder = ArithmeticDecoder(fo)
./benchmark.py:        for t in range(T):
./benchmark.py:            for k, value in enumerate(codes[0, :, t].tolist()):
./benchmark.py:                q_cdf = build_stable_quantized_cdf(
./benchmark.py:                    probas[0, :, k, t], coder.total_range_bits, check=False)
./benchmark.py:                decoder.pull(q_cdf)
./benchmark.py:        print("Time to AC dec.:", timer())
./benchmark.py:        with torch.no_grad():
./benchmark.py:            _ = model.decode(frames)
./benchmark.py:        print("Time to decode:", timer())
./benchmark.py:if __name__ == '__main__':
./benchmark.py:    main()
